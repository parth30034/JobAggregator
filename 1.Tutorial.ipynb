{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56c5058-d67d-4d2b-94c2-c029491cf5b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc74708a-6681-43d1-b8c1-eabfcf367f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# workspace.default.big_mart_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "242d063a-0bc8-438c-ad29-67982585fde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Start with SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86081da2-a1f5-48ef-a64d-590d8494be4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Understand What a DataFrame Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d3e115-77a4-49da-9b0f-d3fa61e8edf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74aa2710-5591-480d-a9da-ec2e2b9f0fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Bronze Layer _ Ingesting Data to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dedb1fc7-3dd5-4312-88e4-0528d6cfaea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\n✅Data Written to Bronze Layer\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JobETL_Bronze\").getOrCreate()\n",
    "\n",
    "# Setup catalog/schema (run this once)\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS main\")\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "# Your existing code for API calls and DataFrame creation\n",
    "API_KEY = \"c79216ba9f9d5e98d3c3339cab4ed84ec83b15b662810fd5d4bf7ba6b544fd98\"\n",
    "roles = [\"Data Engineer\", \"Python Developer\", \"ETL Developer\", \"Spark Engineer\", \"Data Analyst\",\"AI Engineer\",\"AI/ML Engineer\"]\n",
    "location = \"India\"\n",
    "all_jobs = []\n",
    "\n",
    "for role in roles:\n",
    "    params = {\n",
    "        \"engine\": \"google_jobs\",\n",
    "        \"q\": role,\n",
    "        \"location\": location,\n",
    "        \"api_key\": API_KEY\n",
    "    }\n",
    "    res = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "    print(\"✅Reading \uD83D\uDD34LIVE Data from Google Jobs API\")\n",
    "    jobs = res.json().get(\"jobs_results\", [])\n",
    "    \n",
    "    for job in jobs:\n",
    "        job[\"search_role\"] = role\n",
    "    all_jobs.extend(jobs)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"company_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"share_link\", StringType(), True),\n",
    "    StructField(\"search_role\", StringType(), True)\n",
    "])\n",
    "\n",
    "bronze_df = spark.createDataFrame(all_jobs, schema=schema)\n",
    "\n",
    "# Write to your bronze schema\n",
    "bronze_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"main.bronze.jobs_raw\")\n",
    "\n",
    "print(\"✅Data Written to Bronze Layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103b0a05-6628-4550-9c34-2e802b0b8eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>company_name</th><th>location</th><th>description</th><th>share_link</th><th>search_role</th></tr></thead><tbody><tr><td>Data Engineer- Associate</td><td>Federal Express Corporation AMEA</td><td>Anywhere</td><td>Responsible for developing, optimize, and maintaining business intelligence and data warehouse systems, ensuring secure, efficient data storage and retrieval, enabling self-service data exploration, and supporting stakeholders with insightful reporting and analysis.\n",
       "\n",
       "1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n",
       "2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n",
       "3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n",
       "4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n",
       "5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n",
       "6. Design and implement data models to organize and structure data for analytical purposes.\n",
       "7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n",
       "8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n",
       "9. Assist in training and support to users on business intelligence tools and applications.\n",
       "10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n",
       "\n",
       "Education: Bachelor's degree or equivalent in Computer Science, MIS, Mathematics, Statistics, or similar discipline. Master's degree or PhD preferred.\n",
       "\n",
       "Relevant work experience in data engineering based on the following number of years:\n",
       "Standard I: Two (2) years\n",
       "Standard II: Three (3) years\n",
       "Senior I: Four (4) years\n",
       "Senior II: Five (5) years\n",
       "\n",
       "Knowledge, Skills and Abilities\n",
       "• Fluency in English\n",
       "• Analytical Skills\n",
       "• Accuracy & Attention to Detail\n",
       "• Numerical Skills\n",
       "• Planning & Organizing Skills\n",
       "• Presentation Skills\n",
       "• Data Modeling and Database Design\n",
       "• ETL (Extract, Transform, Load) Skills\n",
       "• Programming Skills\n",
       "\n",
       "FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n",
       "\n",
       "All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\n",
       "Our Company\n",
       "\n",
       "FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\n",
       "Our Philosophy\n",
       "\n",
       "The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\n",
       "Our Culture\n",
       "\n",
       "Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=maoME4iLxcapPLHYAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNuw7CMAwAxdpPYPKMSoOQWGBAFZSXxMZeOa2VBgU7ijOUr-IXaZebTnfFb1GYM2aEhp1norSGWlU6j5lgDQ-xoISpG0AYriIu0PIw5Bx1b4xqqJxmzL6rOvkYYbIymrdYndHqgIlimFLtdrcZq8hudbxQTwkDNGNMpAonSVHS1JgG9bOpwTPcvrNjsS_hRQHZIWMJd-49_gFTfTOCsgAAAA&shmds=v1_AdeF8Ki1EEWj9FdGES3eD1oVB_vceQKrOD0l6VHbac6JR12j_Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=maoME4iLxcapPLHYAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Senior Data Engineer</td><td>Mastercard</td><td>Pune, Maharashtra, India</td><td>Job Title:\n",
       "\n",
       "Senior Data Engineer\n",
       "\n",
       "Overview:\n",
       "\n",
       "Position Overview:\n",
       "\n",
       "The Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n",
       "\n",
       "This role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n",
       "\n",
       "The ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n",
       "\n",
       "1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n",
       "2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n",
       "3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n",
       "\n",
       "Role:\n",
       "\n",
       "• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n",
       "• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n",
       "• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n",
       "• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n",
       "• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n",
       "• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n",
       "• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n",
       "• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n",
       "\n",
       "All About You:\n",
       "\n",
       "• Strong understanding of Windows and Linux server.\n",
       "• Good understanding of SQL Server or Oracle DB.\n",
       "• Solid understanding of Essbase technology – understand how this technology works, for both BSO\n",
       "and ASO cubes.\n",
       "• Develop BSO and ASO cubes with a strong eye for performance.\n",
       "• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n",
       "• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCQAwAUFz7CU6ZHLT2RHDRVRGFguAHlPQa7k5qUpIIfodfrC5vetVnVq3uxEUUjugIJ06FiRTWcJUejFBjBmE4i6SR5ofsPtk-BLOxSeboJTZRnkGYenmHh_T2p7OMStOITt12t3k3E6flokVz0og6QGG4vZhqaPE30bIr1nDhoeAXlqvX1JUAAAA&shmds=v1_AdeF8Kjilf5bxnv3Bc7iM6HGmbYmIZhgjXWK3XVsMCSyUs2b_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>AWS Data Engineer</td><td>The Value Maximizer</td><td>Madhavaram, Telangana, India</td><td>Job Title: AWS Data Engineer Location: Gurgaon / Hyderabad Experience: 1.6 - 6 yearsAbout the Role: We are seeking a skilled AWS Data Engineer with strong expertise in data engineering tools and cloud technologies. The ideal candidate should have hands-on experience in Snowflake, Python, and SQL, with exposure to AWS Glue. Candidates with experience in DBT, Redshift, and Lambda will have an added advantage.Key Responsibilities:\n",
       "• Design, develop, and maintain scalable data pipelines and ETL workflows on AWS\n",
       "• Work extensively with Snowflake, Python, and SQL to process, transform, and analyze data\n",
       "• Optimize and maintain existing data architectures to ensure high performance and cost efficiency\n",
       "• Integrate data from multiple sources into a unified data warehouse\n",
       "• Collaborate with data analysts, data scientists, and business stakeholders to meet data requirements\n",
       "• Implement data governance, quality checks, and best practices in data engineering\n",
       "• Leverage AWS services such as Redshift and Lambda for data processing tasks\n",
       "• Use DBT (if applicable) for data modeling and transformation in the ELT framework\n",
       "• Troubleshoot data pipeline issues and perform root cause analysis\n",
       "\n",
       "Required Skills (Mandatory):\n",
       "• Snowflake - Data warehouse development, performance tuning, and query optimization\n",
       "• Python - Scripting for automation and data processing\n",
       "• SQL - Advanced query writing and optimization skills\n",
       "• AWS Glue\n",
       "\n",
       "Preferred / Optional Skills:\n",
       "• DBT - Data transformation and modeling\n",
       "• AWS Redshift - Data warehousing and analytics\n",
       "• AWS Lambda - Serverless data processing\n",
       "\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field\n",
       "• 1.6 - 6 years of hands-on experience in data engineering roles\n",
       "• Strong problem-solving skills and ability to work in fast-paced environments\n",
       "• Good communication and collaboration skills</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=yiNOHEMeOuiOnDinAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBCAYVz7CE43itRGFBedBEUUnCw6lmt7JJH0ruSiFJ_Fh7UuP3zTn30n2Wz_uMEBE8KRrWeiCAu4SA1KGBsHwnASsYGmO5dSr1tjVENhNWHyTdFIZ4SplsE8pdZ_KnUYqQ-YqFptlkPRs52vS0dwx_AiuOLgO_8ZR55HtA7fGLHLoaSAbJExhzO3Hn_YDysOnwAAAA&shmds=v1_AdeF8KjzDld6mLRFYpwvRbl80Cf9scknY3E5o_gVqfWYnAcvXA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=yiNOHEMeOuiOnDinAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Senior Data Engineer - Azure</td><td>EPAM Systems</td><td>Chennai, Tamil Nadu, India</td><td>EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n",
       "\n",
       "We are seeking a skilled and detail-oriented Azure Data Engineer to join our data engineering team. The ideal candidate will have hands-on experience with Azure Data Factory, Azure Synapse or Microsoft Fabric, and strong programming skills in Python or PySpark. Proficiency in SQL and experience in designing and maintaining modern data pipelines and data lakes is essential.\n",
       "\n",
       "RESPONSIBILITIES\n",
       "• Design, build, and manage scalable data pipelines using Azure Data Factory (ADF)\n",
       "• Develop and optimize data transformations using Azure Synapse Analytics or Microsoft Fabric\n",
       "• Write efficient and reusable code in Python or PySpark for data wrangling and processing\n",
       "• Create, maintain, and optimize complex SQL queries and stored procedures\n",
       "• Collaborate with data scientists, analysts, and business stakeholders to understand data needs\n",
       "• Ensure data accuracy, consistency, and security across pipelines and storage layers\n",
       "• Monitor pipeline performance and troubleshoot data issues in production environments\n",
       "• Participate in architectural discussions and propose scalable, secure data solutions on Azure\n",
       "\n",
       "REQUIREMENTS\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field\n",
       "• 3-7 years of experience in data engineering or related roles\n",
       "• Strong experience with Azure Data Factory (ADF) for ETL/ELT workflows\n",
       "• Proficiency in Azure Synapse Analytics and/or Microsoft Fabric\n",
       "• Solid coding skills in Python or PySpark for data processing and automation\n",
       "• Advanced knowledge of SQL – including writing, optimizing, and troubleshooting queries\n",
       "• Experience with data modeling, data lakes, and data warehouse concepts\n",
       "• Good communication skills and the ability to work in a collaborative, agile team environment\n",
       "• Ability to communicate effectively in both written and spoken English (B2 level and higher)\n",
       "\n",
       "NICE TO HAVE\n",
       "• Azure certifications (e.g., Azure Data Engineer Associate or Azure Fundamentals)\n",
       "• Familiarity with DevOps practices and CI/CD pipelines in data projects\n",
       "• Experience with Azure DevOps, Git, or other version control tools\n",
       "• Understanding of data governance, security, and compliance within Azure\n",
       "\n",
       "WE OFFER\n",
       "• Opportunity to work on technical challenges that may impact across geographies\n",
       "• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n",
       "• Opportunity to share your ideas on international platforms\n",
       "• Sponsored Tech Talks & Hackathons\n",
       "• Unlimited access to LinkedIn learning solutions\n",
       "• Possibility to relocate to any EPAM office for short and long-term projects\n",
       "• Focused individual development\n",
       "• Benefit package\n",
       "• Health benefits\n",
       "• Retirement benefits\n",
       "• Paid time off\n",
       "• Flexible benefits\n",
       "• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ONHPfHeGGN8tjaXVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFwQqCQBAAULr6CZ3mLOZG0aVOUhIFRWB3GXVwN9YZ2dnA-pi-tbq8l3xmyboidhLggBGh5N4xUYAFFO9noN9naUAJQ2tBGI4ivaf5zsY46tYYVZ_3GjG6Nm9lMMLUyGQe0uifWi0GGj1Gqleb5ZSP3KdpeSsuUL000qDgGPaWmNFlcMfBebhi98zgxJ3DL2PuWH-hAAAA&shmds=v1_AdeF8KhzU6NKnB_yH5r4Hg-94JiVes164BbSavVIIbnamHK6SA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ONHPfHeGGN8tjaXVAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Data Engineer - Periscope</td><td>McKinsey & Company</td><td>Gurugram, Haryana, India (+1 other)</td><td>Your Growth Driving lasting impact and building long-term capabilities with our clients is not easy work. You are the kind of person who thrives in a high performance/high reward culture - doing hard things, picking yourself up when you stumble, and having the resilience to try another way forward.\n",
       "\n",
       "In return for your drive, determination, and curiosity, we'll provide the resources, mentorship, and opportunities you need to become a stronger leader faster than you ever thought possible. Your colleagues—at all levels—will invest deeply in your development, just as much as they invest in delivering exceptional results for clients. Every day, you'll receive apprenticeship, coaching, and exposure that will accelerate your growth in ways you won’t find anywhere else.\n",
       "\n",
       "When you join us, you will have:\n",
       "• Continuous learning: Our learning and apprenticeship culture, backed by structured programs, is all about helping you grow while creating an environment where feedback is clear, actionable, and focused on your development. The real magic happens when you take the input from others to heart and embrace the fast-paced learning experience, owning your journey.\n",
       "• A voice that matters: From day one, we value your ideas and contributions. You’ll make a tangible impact by offering innovative ideas and practical solutions. We not only encourage diverse perspectives, but they are critical in driving us toward the best possible outcomes.\n",
       "• Global community: With colleagues across 65+ countries and over 100 different nationalities, our firm’s diversity fuels creativity and helps us come up with the best solutions for our clients. Plus, you’ll have the opportunity to learn from exceptional colleagues with diverse backgrounds and experiences.\n",
       "• World-class benefits: On top of a competitive salary (based on your location, experience, and skills), we provide a comprehensive benefits package to enable holistic well-being for you and your family.\n",
       "\n",
       "Your ImpactYou will be part of the data delivery team and will have the opportunity to develop a deep understanding of the domain/function.\n",
       "\n",
       "You will design and drive the work plan for the optimization/automation and standardization of the processes incorporating best practices to achieve efficiency gains.\n",
       "\n",
       "You will run data engineering pipelines, link raw client data with data model, conduct data assessment, perform data quality checks, and transform data using ETL tools.\n",
       "\n",
       "You will perform data transformations, modeling, and validation activities, as well as configure applications to the client context. You will also develop scripts to validate, transform, and load raw data using programming languages such as Python/PySpark/SparkSQL. You’ll also be required to analyze and visualize data using Power BI to provide insights to support business decisions.\n",
       "\n",
       "In this role, you will determine database structural requirements by analyzing client operations, applications, and programming.\n",
       "\n",
       "Given the pace of this role, you will develop cross-site relationships to enhance idea generation, and manage stakeholders. Lastly, you will collaborate with the team to support ongoing business processes by delivering high-quality end products on-time and perform quality checks wherever required.\n",
       "\n",
       "You’ll work with our Periscope team in our Gurgaon and Bangalore, India office. Periscope is the technology backbone of McKinsey’s Growth, Marketing & Sales Practice. Founded in 2007, it combines world-leading Intellectual Property, prescriptive analytics, and cloud-based tools, with expert support and training. This unique combination drives revenue growth – now, and in the future. The platform offers a suite of Growth, Marketing & Sales solutions that accelerate and sustain commercial transformation for businesses.\n",
       "\n",
       "The Growth, Marketing & Sales Practice strives to help clients in both consumer and business-to-business environments on a wide variety of marketing and sales topics. Our clients benefit from our experience in core areas of sales and marketing topics such as sales and channel management, branding, customer insights, marketing ROI, digital marketing, CLM, and pricing. Our Practice offers an exceptional opportunity to work at the intersection of sales, marketing, and consulting. Focusing on issues like redefining sales and marketing operations and commercial transformation, our people help clients build capabilities and transform how companies go to market-moving them to customer-centric organizations.\n",
       "\n",
       "Periscope leverages its world-leading IP (largely from McKinsey but also other partners) and best-in-class technology to enable transparency into Big Data, create actionable insights, and new ways of working that drive lasting performance improvement, and typically sustain a 2-7% increase in return on sales (ROS). With a truly global reach, the portfolio of solutions is comprised of: Marketing Solutions, Customer Experience Solutions, Category Solutions, B2C Pricing Solutions, B2B Pricing Solutions, and Sales Solutions. These are complemented by ongoing client service and custom capability building programs.\n",
       "\n",
       "Periscope has a presence in 27 locations across 16 countries with a team of 800+ IT and business professionals and a network of 300+ experts. To learn more about how Periscope’s solutions and experts are helping businesses continually drive better performance, visit www.mckinsey.com/periscope\n",
       "\n",
       "Your qualifications and skills\n",
       "• Bachelor’s degree/master's degree with high rankings, with 3+ years of professional work experience\n",
       "• Ability to design and drive the work plan for the optimization/automation and standardization of the processes incorporating best practices to achieve efficiency gains.\n",
       "• Expertise to run data engineering pipelines, link raw client data with data model, conduct data assessment, perform data quality checks, and transform data using ETL tools.\n",
       "• Perform data transformations, modeling, and validation activities, as well as configure applications to the client context. Develop scripts to validate, transform, and load raw data using programming languages such as Python/PySpark/SparkSQL. Analyze and visualize data using Power BI to provide insights to support business decisions.\n",
       "• Determine database structural requirements by analyzing client operations, applications, and programming..\n",
       "• Collaborate with the team to support ongoing business processes by delivering high-quality end products on-time and perform quality checks wherever required.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Tj0hmhPFvTY9OVviAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMTQrCMBBAYdz2CK5m5UJqUgQ3ulTxD8EblGkc0kg7EzIptKfxqtbN41u94rsoqhNmhDP7wEQJNvCiFNRJpNl3aUAJk2tBGC4ivqPloc056t5a1c54zZiDM056K0yNjPYjjf5Ta4uJYoeZ6u2uGk1kvzZP9wisNMEKjtJH5AnCvB7S4BP2JVwxTchYwo3fAX_kGvoWogAAAA&shmds=v1_AdeF8Kh_qZOoWsCGwx3jd10Ic3QMbDA4ou9RxXvWCM0D8cP6mQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Tj0hmhPFvTY9OVviAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Engineering Manager - Data Engineer</td><td>Motive</td><td>Anywhere</td><td>Who we are:\n",
       "\n",
       "Motive empowers the people who run physical operations with tools to make their work safer, more productive, and more profitable. For the first time ever, safety, operations and finance teams can manage their drivers, vehicles, equipment, and fleet related spend in a single system. Combined with industry leading AI, the Motive platform gives you complete visibility and control, and significantly reduces manual workloads by automating and simplifying tasks.\n",
       "\n",
       "Motive serves more than 100,000 customers – from Fortune 500 enterprises to small businesses – across a wide range of industries, including transportation and logistics, construction, energy, field service, manufacturing, agriculture, food and beverage, retail, and the public sector.\n",
       "\n",
       "Visit gomotive.com to learn more.\n",
       "\n",
       "About the Role:\n",
       "\n",
       "As a Data Engineering Manager you will be part of the core data team building out world class data products that will be in front of our largest customers. You will be working on the intersection of all data streams in the company from our IOT data consuming 100s of thousands of data points per minute to our user data. You will partner closely with both the Product, Engineering, as well as the Strategic Analytics teams. We are seeking strong team players who thrive on innovation and continuous improvement. We pride ourselves on our culture, and ability to work effectively across a highly diversified team.\n",
       "What You'll Do:\n",
       "• Build data roadmap for the data engineering team building both internal and external data products.\n",
       "• Drive innovation in our data products with the application of AI in both the processes we work on to the products we deliver.\n",
       "• Build teams to scale out our maturing set of analytics products.\n",
       "• Be deeply technical and continue to code and drive technical standards within the team.\n",
       "• Architect and design data models in collaboration with data and product teams\n",
       "• Communicate effectively and drive strategy across multiple teams and projects.\n",
       "What We're Looking For:\n",
       "• Bachelor's degree or higher in a quantitative field, e.g. Computer Science, Math, Economics, or Statistics\n",
       "• 10+ years experience in Data Engineering, and 3+ years managing a team\n",
       "• Expertise with data engineering stack, dBt, Snowflake, airflow, data observability tools, AWS, pyspark.\n",
       "• Expertise in SQL and Python\n",
       "• Great planning and roadmapping skills\n",
       "• Excellent communication, collaboration, and people skills\n",
       "\n",
       "Creating a diverse and inclusive workplace is one of Motive's core values. We are an equal opportunity employer and welcome people of different backgrounds, experiences, abilities and perspectives.\n",
       "\n",
       "Please review our Candidate Privacy Notice here .\n",
       "\n",
       "UK Candidate Privacy Notice here.\n",
       "\n",
       "The applicant must be authorized to receive and access those commodities and technologies controlled under U.S. Export Administration Regulations. It is Motive's policy to require that employees be authorized to receive access to Motive products and technology.\n",
       "\n",
       "#LI-Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Zx0eSLYQ3BIkxjrfAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zXJsQoCMQyAYVxvcXfKLNiK4HKuiijcMxxpDW3lLilNkHsGn1odXH74-Lv3qusvnAoTtcIJBmRM1GAHZzSE__r6LgGUsMUMwnAVSRNtTtmsau-96uSSGlqJLsrshSnI4p8S9JdRMzaqExqNh-N-cZXTdj2IlRdBYbjxo-AHsH6x5Y0AAAA&shmds=v1_AdeF8KjHMtnogMcO6BGQQWTzxgKuOWuTRWGvKC3hl5EHa29uXA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Zx0eSLYQ3BIkxjrfAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Sr. Data Engineer - Tietoevry create (m/f/d)</td><td>Tietoevry</td><td>Pune, Maharashtra, India</td><td>Job Description\n",
       "\n",
       "As an Data Engineer, you'll be responsible for designing, implementing, and managing data solutions on the Azure platform. Here are some common use cases and examples of tasks performed by Azure Data Engineers:\n",
       "\n",
       "Data Ingestion & Integration: Use tools like Azure Data Factory to extract, transform, and load (ETL) data from sources such as databases, files, APIs, and streams into Azure.\n",
       "\n",
       "Data Transformation & Processing: Prepare data for analysis using platforms like Azure Databricks or Synapse Analytics with Apache Spark.\n",
       "\n",
       "Data Storage & Management: Choose appropriate storage solutions—Azure SQL Database or Cosmos DB for structured data, Data Lake Storage for big data, and Blob Storage for files.\n",
       "\n",
       "Data Warehousing: Implement scalable analytics with Azure Synapse Analytics for large datasets.\n",
       "\n",
       "Data Modeling & Analysis: Collaborate on data models using Azure Analysis Services or Databricks to support analytics.\n",
       "\n",
       "Real-time Processing: Use Azure Stream Analytics to process and analyze live data from sources like IoT or social media.\n",
       "\n",
       "Governance & Security: Ensure data security and compliance through access controls, encryption, monitoring, and retention policies.\n",
       "\n",
       "DevOps & Automation: Automate pipelines and deployments using Azure DevOps, Monitor, and Automation tools.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "At Tietoevry, we believe in the power of diversity, equity, and inclusion. We encourage applicants of all backgrounds, genders (m/f/d), and walks of life to join our team, as we believe that this fosters an inspiring workplace and fuels innovation. Our commitment to openness, trust, and diversity is at the heart of our mission to create digital futures that benefit businesses, societies, and humanity.\n",
       "\n",
       "Diversity, equity and inclusion (tietoevry.com)</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=biY8iWwr_GPeA2AFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WNMQrCQBBFsc0RrKYRVGJWBBstVURBELSXyWbcXUlmws4q8Uje0ljZ_OY93s8-g2xziQVsMSHs2AUmijCDa6Ak9IpvsJEwEYwbczfVpEdHKUEJo_UgDHsRV9Nw7VNqdWWMal04TZiCLaw0RphK6cxDSv3NTT1Gaus-eVss513RspuO_m-B4fxkyuGEvYjqU8QcDlwF_ALoF33trAAAAA&shmds=v1_AdeF8Kif_rKEpiYN9VgkSQxKz-DeMLFr-tyhftLA4r1zLF3LoA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=biY8iWwr_GPeA2AFAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>BI & Data Warehouse Data Engineer</td><td>Astellas Pharma</td><td>Bengaluru, Karnataka, India</td><td>As part of the Astellas commitment to delivering value for our patients, our organization is currently undergoing transformation to achieve this critical goal. This is an opportunity to work on digital transformation and make a real impact within a company dedicated to improving lives.\n",
       "\n",
       "DigitalX our new information technology function is spearheading this value driven transformation across Astellas. We are looking for people who excel in embracing change, manage technical challenges and have exceptional communication skills.\n",
       "\n",
       "Purpose and Scope:\n",
       "\n",
       "As a Junior Data Engineer, you will play a crucial role in assisting in the design, build, and maintenance of our data infrastructure focusing on BI and DWH capabilities. Working with the Senior Data Engineer, your foundational expertise in BI, Databricks, PySpark, SQL, Talend and other related technologies, will be instrumental in driving data-driven decision-making across the organization. You will play a pivotal role in building maintaining and enhancing our systems across the organization. This is a fantastic global opportunity to use your proven agile delivery skills across a diverse range of initiatives, utilize your development skills, and contribute to the continuous improvement/delivery of critical IT solutions.\n",
       "\n",
       "This position is based in Bangalore, India. We recognize the importance of work/life balance & believe in optimizing the most productive work environment for all employees to succeed and deliver.\n",
       "\n",
       "Essential Job Responsibilities:\n",
       "• Collaborate with FoundationX Engineers to design and maintain scalable data systems.\n",
       "• Assist in building robust infrastructure using technologies like PowerBI, Qlik or alternative, Databricks, PySpark, and SQL.\n",
       "• Contribute to ensuring system reliability by incorporating accurate business-driving data.\n",
       "• Gain experience in BI engineering through hands-on projects.\n",
       "\n",
       "Data Modelling and Integration:\n",
       "• Collaborate with cross-functional teams to analyze requirements and create technical designs, data models, and migration strategies.\n",
       "• Design, build, and maintain physical databases, dimensional data models, and ETL processes specific to pharmaceutical data.\n",
       "\n",
       "Cloud Expertise:\n",
       "• Evaluate and influence the selection of cloud-based technologies such as Azure, AWS, or Google Cloud.\n",
       "• Implement data warehousing solutions in a cloud environment, ensuring scalability and security.\n",
       "\n",
       "BI Expertise:\n",
       "• Leverage and create PowerBI, Qlik or equivalent technology for data visualization, dashboards, and self-service analytics.\n",
       "\n",
       "Data Pipeline Development:\n",
       "• Design, build, and optimize data pipelines using Databricks and PySpark. Ensure data quality, reliability, and scalability.\n",
       "• Application Transition: Support the migration of internal applications to Databricks (or equivalent) based solutions. Collaborate with application teams to ensure a seamless transition.\n",
       "• Mentorship and Leadership: Lead and mentor junior data engineers. Share best practices, provide technical guidance, and foster a culture of continuous learning.\n",
       "• Data Strategy Contribution: Contribute to the organization’s data strategy by identifying opportunities for data-driven insights and improvements.\n",
       "• Participate in smaller focused mission teams to deliver value driven solutions aligned to our global and bold move priority initiatives and beyond.\n",
       "• Design, develop and implement robust and scalable data analytics using modern technologies.\n",
       "• Collaborate with cross functional teams and practices across the organization including Commercial, Manufacturing, Medical, DataX, GrowthX and support other X (transformation) Hubs and Practices as appropriate, to understand user needs and translate them into technical solutions.\n",
       "• Provide Technical Support to internal users troubleshooting complex issues and ensuring system uptime as soon as possible.\n",
       "• Champion continuous improvement initiatives identifying opportunities to optimize performance security and maintainability of existing data and platform architecture and other technology investments.\n",
       "• Participate in the continuous delivery pipeline. Adhering to DevOps best practices for version control automation and deployment. Ensuring effective management of the FoundationX backlog.\n",
       "• Leverage your knowledge of data engineering principles to integrate with existing data pipelines and explore new possibilities for data utilization.\n",
       "• Stay-up to date on the latest trends and technologies in data engineering and cloud platforms.\n",
       "\n",
       "Qualifications:\n",
       "\n",
       "Required\n",
       "• Bachelor's degree in computer science, Information Technology, or related field (master’s preferred) or equivalent experience\n",
       "• 1-3+ years of experience in data engineering with a strong understanding of BI technologies, PySpark and SQL, building data pipelines and optimization.\n",
       "• 1-3 +years + experience in data engineering and integration tools (e.g., Databricks, Change Data Capture)\n",
       "• 1-3+ years + experience of utilizing cloud platforms (AWS, Azure, GCP). A deeper understanding/certification of AWS and Azure is considered a plus.\n",
       "• Experience with relational and non-relational databases.\n",
       "• Any relevant cloud-based integration certification at foundational level or above. (Any QLIK or BI certification, AWS certified DevOps engineer, AWS Certified Developer, Any Microsoft Certified Azure qualification, Proficient in RESTful APIs, AWS, CDMP, MDM, DBA, SQL, SAP, TOGAF, API, CISSP, VCP or any relevant certification)\n",
       "• Experience in MuleSoft (Anypoint platform, its components, Designing and managing API-led connectivity solutions).\n",
       "• Experience in AWS (environment, services and tools), developing code in at least one high level programming language.\n",
       "• Experience with continuous integration and continuous delivery (CI/CD) methodologies and tools\n",
       "• Experience with Azure services related to computing, networking, storage, and security\n",
       "• Understanding of cloud integration patterns and Azure integration services such as Logic Apps, Service Bus, and API Management\n",
       "\n",
       "Preferred\n",
       "• Subject Matter Expertise: possess a strong understanding of data architecture/ engineering/operations/ reporting within Life Sciences/ Pharma industry across Commercial, Manufacturing and Medical domains.\n",
       "• Other complex and highly regulated industry experience will be considered across diverse areas like Commercial, Manufacturing and Medical.\n",
       "• Data Analysis and Automation Skills: Proficient in identifying, standardizing, and automating critical reporting metrics and modelling tools\n",
       "• Analytical Thinking: Demonstrated ability to lead ad hoc analyses, identify performance gaps, and foster a culture of continuous improvement.\n",
       "• Technical Proficiency: Strong coding skills in SQL, R, and/or Python, coupled with expertise in machine learning techniques, statistical analysis, and data visualization.\n",
       "• Agile Champion: Adherence to DevOps principles and a proven track record with CI/CD pipelines for continuous delivery.\n",
       "\n",
       "Working Environment\n",
       "\n",
       "At Astellas we recognize the importance of work/life balance, and we are proud to offer a hybrid working solution allowing time to connect with colleagues at the office with the flexibility to also work from home. We believe this will optimize the most productive work environment for all employees to succeed and deliver. Hybrid work from certain locations may be permitted in accordance with Astellas’ Responsible Flexibility Guidelines.\n",
       "\n",
       "#LI-CH1\n",
       "\n",
       "Category FoundationX\n",
       "\n",
       "Astellas is committed to equality of opportunity in all aspects of employment.\n",
       "\n",
       "EOE including Disability/Protected Veterans</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=iZaPxzI9k0CsjEDNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOMQrCQBBFsc0RrKaykLgrgiBaGRSJNnaWYRKH3ehmJuxsIDfymkZsPjwePH72mWW7ooQFnDAhPDCSl0Hpj2d2LRNFWMFValDC2HgQhouICzQ_-JR63VurGozThKltTCOdFaZaRvuSWn9TqZ_CfcBE1Wa7Hk3PbmmOmigEVLhPtkNoGQpih2GIQw43jDx9eGMOJT9b_AKj6d-1qgAAAA&shmds=v1_AdeF8Kjlka4T_7a_vBovx_2W3X9NLKJ77Uxg_w9551cIGkplvg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=iZaPxzI9k0CsjEDNAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Exciting Career Opportunity for Azure Data Engineer</td><td>Zensar Technologies</td><td>Pune, Maharashtra, India</td><td>Hi All,\n",
       "\n",
       "This is an exciting career opportunity for Azure Data Engineer position. Please find the below details. If you are interested, please share your resume to jeyaramya.rajendran@zensar.com\n",
       "\n",
       "Experience - 6 to 9years\n",
       "\n",
       "Notice - Immediate or max 10days\n",
       "\n",
       "Location - all\n",
       "• Creates and maintains highly scalable data pipelines across Azure Data Lake Storage, and Azure Data Factory, Databricks and Apache Spark/Scala.\n",
       "• Proficient in SQL, Python and awareness about AI capabilities.\n",
       "• Responsible for managing a growing cloud-based data ecosystem and the reliability of our corporate data lake and analytics data mart.\n",
       "• Contributes to the continued evolution of Corporate Analytics Platform and Integrated data model.\n",
       "• Be part of the Data Engineering team in all phases of work including analysis, design and architecture to develop and implement cutting-edge solutions.\n",
       "• Influences changes outside of the team that continuously shape and improve the Data strategy.\n",
       "• Develop data acquisition and ingestion processes.\n",
       "• Identify automation opportunities through data analysis, advanced data modeling and optimizations.\n",
       "• Collaborates with Azure Cloud Architects and Data Platform Engineers on enterprise solutions</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=pEJNY9qV63s5V4UcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQ4BQRBAo_UJqqmFW5FoqAQREqFQaWRujd2VM7PZmUuOT_N1aF71Xl7_0-vvN51PljjACgtRgWPOUqzlZC-4S4Hluy0EazSEDYfEf2cMe6lBCYuPIAxbkdDQYBHNss6dU22qoIaWfOXl6YSpls49pNY_rhp_r9yg0XU6m3RV5jB0F2LFAmfykaWRkEghMZxaphEc8JegRis4gh3fEn4BRVKBCL0AAAA&shmds=v1_AdeF8KhKqdNrh_q2pblM5bv29dbIMONPJNP2B4AcANI3B1MYnQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=pEJNY9qV63s5V4UcAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Data Engineer Associate</td><td>MSCI</td><td>Pune, Maharashtra, India</td><td>Your Team Responsibilities\n",
       "\n",
       "We are hiring an Associate Data Engineer to support our core data pipeline development efforts and gain hands-on experience with industry-grade tools like PySpark, Databricks, and cloud-based data warehouses. The ideal candidate is curious, detail-oriented, and eager to learn from senior engineers while contributing to the development and operationalization of critical data workflows.\n",
       "\n",
       "Your Key Responsibilities\n",
       "• Assist in the development and maintenance of ETL/ELT pipelines using PySpark and Databricks under senior guidance.\n",
       "• Support data ingestion, validation, and transformation tasks across Rating Modernization and Regulatory programs.\n",
       "• Collaborate with team members to gather requirements and document technical solutions.\n",
       "• Perform unit testing, data quality checks, and process monitoring activities.\n",
       "• Contribute to the creation of stored procedures, functions, and views.\n",
       "• Support troubleshooting of pipeline errors and validation issues.\n",
       "\n",
       "Your skills and experience that will help you excel\n",
       "• Bachelor’s degree in Computer Science, Engineering, or related discipline.\n",
       "• 3+ years of experience in data engineering or internships in data/analytics teams.\n",
       "• Working knowledge of Python, SQL, and ideally PySpark.\n",
       "• Understanding of cloud data platforms (Databricks, BigQuery, Azure/GCP).\n",
       "• Strong problem-solving skills and eagerness to learn distributed data processing.\n",
       "• Good verbal and written communication skills.\n",
       "\n",
       "About MSCI\n",
       "\n",
       "What we offer you\n",
       "• Transparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\n",
       "• Flexible working arrangements, advanced technology, and collaborative workspaces.\n",
       "• A culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\n",
       "• A global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\n",
       "• Global Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\n",
       "• Multi-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\n",
       "• We actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\n",
       "\n",
       "At MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\n",
       "\n",
       "MSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\n",
       "\n",
       "MSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n",
       "\n",
       "To all recruitment agencies\n",
       "\n",
       "MSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n",
       "\n",
       "Note on recruitment scams\n",
       "\n",
       "We are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=CI1G6CZqIzIL-e9VAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMQQrCQAxAUdz2CK6yljojghtdiYpUKAgeoKRjmBmpSZlE6C28snXzV49ffReVO6MhXDhmJipwVJWQ0QjWcJMelLCEBMJwFYkDLQ_JbNS996qDi2poObggby9MvUz-Jb3-02nCQuMwr7rtbjO5keMK2sepgcxw_zDV0OJsUJMVrKHhZ8YfNitOj5IAAAA&shmds=v1_AdeF8KjI9Bc8G-NbjISf7S80W10CEjEWSwl2jV_KohBs5bZoJg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=CI1G6CZqIzIL-e9VAAAAAA%3D%3D</td><td>Data Engineer</td></tr><tr><td>Python Developer</td><td>5100 Kyndryl Solutions Private Limited</td><td>Anywhere</td><td>Who We Are At Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities. The Role Are you passionate about solving complex problems? Do you thrive in a fast-paced environment? Then there’s a good chance you will love being a part of our Software Engineering – Development team at Kyndryl, where you will be able to see the immediate value of your work. As a Software Engineering - Developer at Kyndryl, you will be at the forefront of designing, developing, and implementing cutting-edge software solutions. Your work will play a critical role in our business offering, your code will deliver value to our customers faster than ever before, and your attention to detail and commitment to quality will be critical in ensuring the success of our products. Using design documentation and functional programming specifications, you will be responsible for implementing identified components. You will ensure that implemented components are appropriately documented, unit-tested, and ready for integration into the final product. You will have the opportunity to architect the solution, test the code, and deploy and build a CI/CD pipeline for it. As a valued member of our team, you will provide work estimates for assigned development work, and guide features, functional objectives, or technologies being built for interested parties. Your contributions will have a significant impact on our products' success, and you will be part of a team that is passionate about innovation, creativity, and excellence. Above all else, you will have the freedom to drive innovation and take ownership of your work while honing your problem-solving, collaboration, and automation skills. Together, we can make a difference in the world of cloud-based managed services. Your Future at Kyndryl The career path ahead is full of exciting opportunities to grow and advance within the job family. With dedication and hard work, you can climb the ladder to higher bands, achieving coveted positions such as Principal Engineer or Vice President of Software. These roles not only offer the chance to inspire and innovate, but also bring with them a sense of pride and accomplishment for having reached the pinnacle of your career in the software industry. Who You Are You’re good at what you do and possess the required experience to prove it. However, equally as important – you have a growth mindset; keen to drive your own personal and professional development. You are customer-focused – someone who prioritizes customer success in their work. And finally, you’re open and borderless – naturally inclusive in how you work with others. Required Technical and Professional Experience • 6 years of experience working as a software engineer on complex software projects •Excellent coding skills and solid development experience (Java, Python, .Net etc.) with debugging and problem-solving skills •Software development methodologies, with demonstrated experience developing scalable and robust software •Experienced in relational and NoSQL databases, data mapping, XML/JSON, Rest based web services •Knowledge of architecture design - Microservices architecture, containers (Docker & k8s), messaging queues •Deep understanding of OOP and Design patterns Preferred Technical and Professional Experience •Bachelor's degree in Computer Science, related technical field, or equivalent practical experience •Certification in one or more of the hyperscalers (Azure, AWS, and Google GCP) - otherwise, you can obtain certifications with Kyndryl •Experience with DevOps tools and modern engineering practices Being You Diversity is a whole lot more than what we look like or where we come from, it’s how we think and who we are. We welcome people of all cultures, backgrounds, and experiences. But we’re not doing it single-handily: Our Kyndryl Inclusion Networks are only one of many ways we create a workplace where all Kyndryls can find and provide support and advice. This dedication to welcoming everyone into our company means that Kyndryl gives you – and everyone next to you – the ability to bring your whole self to work, individually and collectively, and support the activation of our equitable culture. That’s the Kyndryl Way. What You Can Expect With state-of-the-art resources and Fortune 100 clients, every day is an opportunity to innovate, build new capabilities, new relationships, new processes, and new value. Kyndryl cares about your well-being and prides itself on offering benefits that give you choice, reflect the diversity of our employees and support you and your family through the moments that matter – wherever you are in your life journey. Our employee learning programs give you access to the best learning in the industry to receive certifications, including Microsoft, Google, Amazon, Skillsoft, and many more. Through our company-wide volunteering and giving platform, you can donate, start fundraisers, volunteer, and search over 2 million non-profit organizations. At Kyndryl, we invest heavily in you, we want you to succeed so that together, we will all succeed. Get Referred! If you know someone that works at Kyndryl, when asked ‘How Did You Hear About Us’ during the application process, select ‘Employee Referral’ and enter your contact's Kyndryl email address. We’re glad you’re here. Take a look around at the many exciting career opportunities we have available and apply today! Can’t find a suitable job opening? Drop off your CV/Resume Drop off your CV/Resume and a Recruiter will reach out with related career information that match your experience and expertise. Sign up for Job Alerts Create your account and then sign up for job alerts. When new jobs become available that meet your criteria, you’ll be alerted right away! At Kyndryl, we achieve progress the world depends on, with purpose. Beginning with the purpose that matters to you. Because here, you will be part of a culture designed with purpose. One that is restless, empathetic and devoted. Where we are committed to sustainable progress for our customers and supporting the communities where we work and live. All of you is what we want. And what we need. Join us, and together, we can advance the vital systems that power human progress.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=rHEEcyrZb39YgP9UAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPQrCQBBAYWxzBKvpBMFsFNJoK4g_RcADhE0yZCdsdpadMSQn8brG5jXfy76bbFct6jjAFSf0HDHBAR7cgKBNrYNVbsy9x-3FqUY5GyPi817UKrV5y6PhgA3PZuBG_qnF2YTRW8X6VBZzHkO_N-WxKOC5hC4tHt7sP0ocBKpE0zrCi0ZS7IAC3ENH9geToaqimgAAAA&shmds=v1_AdeF8KirfgksVQetlCoS78oYAd3u6gmaUJwnC7CS5foNoSOvvA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=rHEEcyrZb39YgP9UAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Software Developer- Python</td><td>BNP Paribas India Solutions</td><td>India</td><td>About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n",
       "\n",
       "About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function:\n",
       "\n",
       "The Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n",
       "\n",
       "The IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "Python Developer\n",
       "\n",
       "Date:\n",
       "\n",
       "June-25\n",
       "\n",
       "Department:\n",
       "\n",
       "ITG- Fresh\n",
       "\n",
       "Location:\n",
       "\n",
       "Chennai, Mumbai\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "Finance Dedicated Solutions\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "(Functional)\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "NA\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "The Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n",
       "\n",
       "A strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "\n",
       "- Good analytical, problem solving, & communication skills\n",
       "\n",
       "- Engage in technical discussions and to help in improving the system, process etc\n",
       "\n",
       "Nice to Have\n",
       "\n",
       "- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n",
       "\n",
       "- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n",
       "\n",
       "- Familiarity with JavaScript, CSS, and HTML.\n",
       "\n",
       "- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n",
       "\n",
       "- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Critical thinking\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Communication skills - oral & written\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Analytical Ability\n",
       "\n",
       "Ability to develop and adapt a process\n",
       "\n",
       "Ability to understand, explain and support change\n",
       "\n",
       "Ability to develop others & improve their skills\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 5 years\n",
       "\n",
       "Other/Specific Qualifications (if required)</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMMQrCQBAAsc0TrLawEpJTwUY7EUQLCeQBYS-uuZPz9rhdNX7E9xqxGZgppvhMimXDV31hJtjTkwInyiXUb3UcoYQTWxDC3DkY_cDcB5punWqSjTEioepFUX1XdXw3HMnyYG5s5YdW3PhNAZXa1XoxVCn289nuXEON2VsUOMaLR2g4PNRzFPDxn74MpNGLmQAAAA&shmds=v1_AdeF8KhddRLnKmMGyyu5XSrgUY0KL77JpalABEFKQDRF04WXHg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Python Developer / Data Analyst - +3 years of experience - Full remote - Contractor in USD</td><td>All European Careers</td><td>Anywhere</td><td>For an international project in Chennai, we are urgently looking for a Full Remote Data Analyst. The ideal candidate will have good understanding of Data integration, Data cleanup for Knowledge 360 and other related products for knowledge system..\n",
       "\n",
       "We are looking for a motivated contractor. Candidates need to be fluent in English.\n",
       "\n",
       "Tasks and responsibilities:\n",
       "\n",
       "Design and implement cloud-based solutions using Azure and Google Cloud platforms. Automate data pipelines to facilitate efficient data flow and processing;\n",
       "\n",
       "Develop and manage data pipelines using technologies like PySpark and Databricks;\n",
       "\n",
       "Ensure the scalability and reliability of data processing workflows;\n",
       "\n",
       "Work with Large Language Models (LLMs) such as GPT and OpenAI models to enhance applications with natural language processing (NLP) capabilities;\n",
       "\n",
       "Design and implement prompt engineering strategies to optimize model performance in business applications;\n",
       "\n",
       "Train, fine-tune, and deploy AI models, analyzing their performance based on real-world results;\n",
       "\n",
       "Optimize models using feedback mechanisms to improve efficiency;\n",
       "\n",
       "Utilize strong communication skills to convey complex technical concepts to non-technical stakeholders;\n",
       "\n",
       "Profile:\n",
       "• Bachelor or Master degree;\n",
       "• +3 years of hands-on experience in Python, Pyspark, Databricks, and cloud platforms like Azure and Google Cloud;\n",
       "• Experience in designing and implementing cloud-based solutions and automating data pipelines;\n",
       "• Experience in Working with Large Language Models (LLMs) such as GPT, OpenAI models, or enhance solutions with natural language processing (NLP) capabilities;\n",
       "• Proven ability in metadata management and schema mapping;\n",
       "• Cloud certifications (Azure, Google Cloud, or similar) are a plus;\n",
       "• Fluent in English;</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=vaNhBJEE36JvMI7HAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOwWrDQAxE6dWfUAjo3BJvaemlOYU4CempUHo28kaxHdaSs5KL_ZX9pSoXwWhmeFP8PRS3r8U6Yajol5KMlCFAhYawZUyLGqzh-Q0WwqwgF6DZIz1xJDcOU0qQaRC7q52wZYwmGXqGn-_Kf5_SgHo3duCMo0ib6HHTmY36EYJqKls1tD6WUYYgTI3M4SqN3k-tHWYaExrVr-8vczly-7TaOnM_ZZ-KDDsPkC9z4InPPf4DndEiodIAAAA&shmds=v1_AdeF8KjvxGq59iD_AXuAUm5WuiRATlMHUuMIIWzMPqMWPAo0zA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=vaNhBJEE36JvMI7HAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Azure + Python Developer-10 yrs -Remote -Contract</td><td>SAPLING INFOSYSTEMS</td><td>Anywhere</td><td>Key Responsibilities:\n",
       "• Design, develop, and optimize applications using Python and Azure cloud services.\n",
       "• Architect and implement scalable cloud solutions leveraging Azure services (App Services, Functions, Storage, Cosmos DB, Event Hub, Service Bus, etc.).\n",
       "• Develop APIs, microservices, and integration solutions for enterprise systems.\n",
       "• Collaborate with cross-functional teams to gather requirements and deliver end-to-end solutions.\n",
       "• Ensure code quality, security, and performance best practices.\n",
       "• Implement CI/CD pipelines using Azure DevOps.\n",
       "• Perform system troubleshooting, optimization, and performance tuning.\n",
       "• Mentor junior developers and contribute to technical best practices across the team.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• 10+ years of experience in software development with a focus on Python.\n",
       "• Strong experience in Azure cloud services and architecture.\n",
       "• Proficiency in Azure-native services such as Azure Functions, Azure App Services, Azure Storage, Azure Data Factory, Azure Key Vault, and Azure Kubernetes Service (AKS).\n",
       "• Solid understanding of RESTful APIs, microservices architecture, and integration patterns.\n",
       "• Experience with Azure DevOps, Git, and CI/CD pipelines.\n",
       "• Strong knowledge of databases (SQL & NoSQL) and ORM frameworks.\n",
       "• Familiarity with containerization tools like Docker and orchestration using Kubernetes.\n",
       "• Strong problem-solving and debugging skills.\n",
       "• Excellent communication and collaboration abilities.\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "Contract length: 12 months\n",
       "\n",
       "Pay: ₹80,000.00 - ₹90,000.00 per month\n",
       "\n",
       "Work Location: Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=xQNSVuqw69hQGrlMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLvQ6CMBAA4LjyCMbhZg0UTVx0Iv4gRtFYFydS6gUw0CPtacD38T3V5ds-7zPwdtH7aREmcO65JANrfGFNLVp_GkJvHfgXbIgR_BUZtkoz-LCnHBwqq0v4lZioqHG4LJlbtxDCuTooHCuudKCpEWQwp048KHd_Mlcqi22tGLPZPOyC1hTjkYzOhySNIUm3J3mT181RQmUgMfdKfQFzAxWTqAAAAA&shmds=v1_AdeF8KitO46RY7AjOGc0Adxg0XNTWnTl_HE7k1WZOICfgqskqw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=xQNSVuqw69hQGrlMAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Python Developer/ Golang Developer</td><td>Hireginie</td><td>India</td><td>Python Developer/ Golang Developer\n",
       "\n",
       "About Our Client: Founded in 2020, the company is a digital platform in the spirituality and wellness sector, offering tailored apps to help users with personal growth and well-being. It combines technology with traditional practices to deliver engaging content and experiences that drive long-term user retention. The brand aims to cater to individuals seeking high-quality and authentic devotional products, blending traditional craftsmanship with modern convenience.\n",
       "\n",
       "Job Description: Python Developer/ Golang Developer\n",
       "\n",
       "Location: HSR Layout, Bangalore\n",
       "\n",
       "Experience: 4-8 years\n",
       "\n",
       "Qualification: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field\n",
       "\n",
       "About the role: We are looking for a senior software developer (Python) with a strong background in backend technologies to join our high-performing engineering team. You will play a key role in designing, building, and maintaining mission-critical services that scale to millions of users.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Develop and maintain robust, scalable backend systems using Python, Golang.\n",
       "• Design efficient data models and queries for PostgreSQL and MongoDB.\n",
       "• Build secure and performant APIs for mobile and web applications.\n",
       "• Drive cloud-native development and infrastructure setup on AWS.\n",
       "• Collaborate with cross-functional teams, including product, mobile, and DevOps.\n",
       "• Optimize systems for performance, reliability, and scalability.\n",
       "• Conduct code reviews, write unit tests, and improve development processes.\n",
       "• Troubleshoot, debug, and resolve production-level issues.\n",
       "\n",
       "Requirements:\n",
       "• Backend development experience with Python, or Golang.\n",
       "• Strong command over relational (PostgreSQL) and document (MongoDB) databases.\n",
       "• Practical experience deploying applications on AWS (EC2, ECS, Lambda, RDS, S3).\n",
       "• Proficiency in designing RESTful APIs and working in service-oriented architectures.\n",
       "• Familiarity with Docker, Git, CI/CD tools, and cloud monitoring practices.\n",
       "• Ability to write clean, testable, and maintainable code.\n",
       "• Strong analytical and debugging skills with a performance-first mindset.\n",
       "\n",
       "About Hireginie: Hireginie is a prominent talent search company specializing in connecting top talent with leading organizations. We are committed to excellence and offer customized recruitment solutions across industries, ensuring a seamless and transparent hiring process. Our mission is to empower both clients and candidates by matching the right talent with the right opportunities, fostering growth and success for all.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=F9NAyXTufaOLO-TcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0XKsQoCMQyAYVxvdHTKLNiK4KCuwqmTb3C0Z2gjNSlNkPMdfGh1cvmHj797z7rd9WVZGI74xCIVm4deSuD0F1jBRSIohjZm-L69SCq4OGSzqnvvVYtLasFodKM8vDBGmfxdov4yaA4NawmGw2a7nlzltJyfqGEiJgRiOPONwgfKTBCSjwAAAA&shmds=v1_AdeF8KhoWLyDDBuP898CHM9YIWMqM9e2aqhcrrBB7VoESHKDUA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=F9NAyXTufaOLO-TcAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Lead Software Engineer - Python</td><td>EPAM Systems</td><td>India</td><td>We are seeking a highly experienced Lead Software Engineer with expertise in Python to lead development efforts, influence technical direction, and deliver robust, scalable solutions.\n",
       "\n",
       "The ideal candidate will bring proven expertise in Python, database systems, and modern software development practices to drive the success of critical projects.\n",
       "\n",
       "Responsibilities\n",
       "• Collaborate with stakeholders to gather requirements, create technical designs, and align solutions with business goals\n",
       "• Lead the development of high-quality, scalable, and maintainable software systems\n",
       "• Conduct technical reviews, including code reviews, to ensure adherence to best practices, coding standards, and performance benchmarks\n",
       "• Coordinate with cross-functional teams to ensure successful implementation of features and solutions\n",
       "• Troubleshoot and resolve complex technical challenges across the development lifecycle\n",
       "• Mentor and guide team members, offering technical leadership and fostering skills development\n",
       "• Drive the adoption of modern development processes and tools, including CI/CD practices through GitHub Actions\n",
       "• Optimize application performance and database queries, ensuring efficiency and scalability\n",
       "• Utilize Agile/Scrum methodologies to manage projects and deliver iterative improvements\n",
       "• Oversee database architecture design and ensure proper integration with applications\n",
       "\n",
       "Requirements\n",
       "• 7-12 years of experience in software development with demonstrated expertise in Python\n",
       "• Proficiency in PostgreSQL and MS SQL Server for database design, optimization, and management\n",
       "• Hands-on expertise with GitHub Actions for automation and CI/CD workflows\n",
       "• Competency in ReactJS for developing interactive, dynamic user interfaces\n",
       "• Solid understanding of Scrum/Agile methodologies for collaborative development and delivery\n",
       "\n",
       "Nice to have\n",
       "• Familiarity with other front-end frameworks beyond ReactJS\n",
       "• Skills in optimizing cross-platform application performance\n",
       "• Understanding of advanced DevOps practices and tools</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=8zsCY6MZPOkQ61E8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMMQrCQBAAsU1vY7W1YE4ELbRKEURRCOQB4e6y3p0kuyG7YPIHH21shplmsu8qOz3QtlDzSz92RCgpJEIcYQfVrJFpkTs7ELSjj7D0lTl0uLlE1UHOxoh0eRC1mnzuuTdM6Hgyb3byRyNx-Q6dVWwOx_2UDxS267IqnlDPotgLJIIbtcn-AMZCw8KPAAAA&shmds=v1_AdeF8KgREb7t01KB27izx3TaHM4VaDUCIOJ26tPSNVDAUnoktQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=8zsCY6MZPOkQ61E8AAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Sr. Python Developer + (Generative AI) 5-9Years( Mandatory)</td><td>Brilliantech Software</td><td>India</td><td>Job description\n",
       "\n",
       "Sr. Python Developer (gen AI) – Onsite\n",
       "\n",
       "Location: Kalyani Nagar, Pune\n",
       "Experience: 6-9 Years( Mandatory)\n",
       "Contract Duration: 6 Months, Full-Time (can be extended),Freelancing\n",
       "Joining: Immediate Joiners Only\n",
       "\n",
       "Required Technical Skills:\n",
       "• Languages: Python , JavaScript/TypeScript, Node.js\n",
       "• Generative Ai & LLM must\n",
       "• Django & Fast Api is must\n",
       "• Frameworks/Tools: Apache Airflow or Google Composer\n",
       "• Kafka, Snowflake, BigQuery, Spark, Hadoop\n",
       "• AWS (Athena, Lambda, EC2, S3), GCP\n",
       "• PostgreSQL, Redis\n",
       "• Docker, Kubernetes (preferred)\n",
       "• CI/CD Pipelines, Trunk-Based Development\n",
       "\n",
       "Other Core Skills:\n",
       "• Strong in SQL and data pipeline optimization\n",
       "• Data structures and algorithms\n",
       "• Exposure to ML/AI-based systems is a plus\n",
       "• SaaS architecture and product development lifecycle\n",
       "\n",
       "Apply Now: sakshi@brilliantechsoft.com\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "\n",
       "Work Location: In person</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=FXn-CbqVfXhXLIlOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBSFYVz7CA5yx1ZpIoKDOilCqSCInZxK2l7bSMwtyaW2j-UbGpcz_By-6DuL7oUTcJu4IwtnHNBQjw5WEGdo0SnWA8IxT2Cb7h6onI_hqmyjmNyUQAoXqsCHXncQgIyoNTg_dMy930vpvRGt56DUoqa3JIsVjfJFlf9P6TvlsDeKsdxs16PobbtcnJw2RivLGNCCnvwJJ9AWctto9QNvkQzLtAAAAA&shmds=v1_AdeF8KimZp_KL7w3D4P0Za9m2h-N4eL3LhvFCnTLCqY7KGq3JQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=FXn-CbqVfXhXLIlOAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Python Developer (REST API, Cloud Platforms and Full-Stack Skills)</td><td>Synechron Technologies Pvt. Ltd._INDIA Company</td><td>India</td><td>Job Summary Synechron is seeking an experienced Python Developer to design, develop, and maintain scalable and robust software solutions across various domains. The role involves working closely with cross-functional teams to translate business requirements into high-quality technical implementations, leveraging Python and related technologies. The ideal candidate will bring expertise in API development, database management, and software best practices to deliver solutions that support the organization’s strategic objectives. This position offers growth opportunities for professionals passionate about innovation, technology, and continuous learning. Software Requirements Required Skills: Proven experience in Python programming, with at least 4 years of hands-on development Strong understanding of object-oriented programming (OOP) principles Experience with Python frameworks such as Django, Flask, or FastAPI Familiarity with RESTful API development and integration Knowledge of relational databases (MySQL, PostgreSQL) and NoSQL databases (MongoDB) Experience with version control systems such as Git Preferred Skills: Cloud platform experience (AWS, Azure, Google Cloud) Containerization with Docker and orchestration with Kubernetes Testing frameworks like PyTest or unittest Automation with CI/CD pipelines (Jenkins, GitLab CI, Azure DevOps) Overall Responsibilities Design, develop, and optimize scalable Python applications and scripts aligned with business needs Collaborate with product managers, UI/UX designers, and fellow developers to gather requirements and translate them into technical solutions Write clean, efficient, and maintainable code following best coding practices and standards Conduct code reviews, identify issues, and troubleshoot bugs to ensure application stability and performance Participate actively in the full software development lifecycle, including planning, testing, deployment, and maintenance Integrate third-party data sources and APIs to extend application functionality Document code, application features, and technical specifications for ongoing support and future enhancements Stay updated with industry trends, emerging technologies, and best practices to incorporate innovative solutions Support team members with technical guidance, knowledge sharing, and resolving complex issues Ensure solutions adhere to security standards and are optimized for performance and scalability Technical Skills (By Category) Programming Languages: Required: Python (4+ years of practical experience) Preferred: Knowledge of additional languages such as JavaScript, Java, or C# for full-stack or integrations Frameworks & Libraries: Django, Flask, or FastAPI API & Data Management: REST API development and consumption Relational databases: MySQL, PostgreSQL NoSQL databases: MongoDB Cloud & DevOps Technologies: Cloud providers: AWS, Azure, or GCP (preferred) Containerization: Docker Container orchestration: Kubernetes (preferred) CI/CD tools: Jenkins, GitLab CI, Azure DevOps Tools & IDEs: IDEs such as Visual Studio Code, PyCharm, or similar Version control tools: Git Testing & Automation: Frameworks like PyTest, unittest API testing and automation techniques Experience Requirements 4+ years of professional experience in Python development Proven track record of designing and implementing scalable applications and APIs Experience working with relational and NoSQL databases Hands-on experience with cloud platforms, containerization, and orchestration tools (preferred) Demonstrated experience working within Agile teams and contributing to development best practices Experience in related domains such as finance, healthcare, or enterprise application development is a plus Day-to-Day Activities Develop and improve Python-based applications, APIs, and automation scripts Collaborate with cross-functional teams on requirements, architecture, and implementation strategies Conduct code reviews and testing to ensure high code quality and adherence to standards Troubleshoot and resolve technical issues promptly to minimize downtime Deploy updates and enhancements via automated pipelines, ensuring seamless delivery Maintain documentation of code, API specifications, and technical processes Participate in daily stand-ups, sprint planning, and retrospectives Review emerging technologies and propose their integration into existing workflows Qualifications Bachelor’s degree or higher in Computer Science, Engineering, Information Technology, or a related field Additional certifications such as Python Institute certifications, cloud certifications (AWS, Azure), are advantageous Commitment to ongoing professional development to stay current with evolving technologies and best practices Professional Competencies Strong analytical and problem-solving skills with attention to detail Excellent communication skills to effectively share ideas and technical information Ability to work independently with minimal supervision and within team environments Adaptability to changing project needs and emerging technologies Proactive approach to learning and process improvement Ethical mindset ensuring security, privacy, and quality in deliverables SYNECHRON’S DIVERSITY & INCLUSION STATEMENT Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more. All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. Candidate Application Notice At Synechron, we believe in the power of digital to transform businesses for the better. Our global consulting firm combines creativity and innovative technology to deliver industry-leading digital solutions. Synechron’s progressive technologies and optimization strategies span end-to-end Artificial Intelligence, Consulting, Digital, Cloud & DevOps, Data, and Software Engineering, servicing an array of noteworthy financial services and technology firms. Through research and development initiatives in our FinLabs we develop solutions for modernization, from Artificial Intelligence and Blockchain to Data Science models, Digital Underwriting, mobile-first applications and more. Over the last 20+ years, our company has been honored with multiple employer awards, recognizing our commitment to our talented teams. With top clients to boast about, Synechron has a global workforce of 14,500+, and has 58 offices in 21 countries within key global markets. For more information on the company, please visit our website or LinkedIn community. At Synechron, we are committed to integrating sustainability into our business strategy, ensuring responsible growth while minimizing environmental impact. Employees play a key role in driving our sustainability initiatives, from reducing our carbon footprint to fostering ethical and sustainable business practices across global operations. All positions are required to adhere to our Sustainability and Health Safety standards, demonstrating a commitment to environmental stewardship, workplace safety, and sustainable practices. Not finding the right fit? Let us know you're interested in a future opportunity by clicking Get Started below or create an account by clicking 'Sign In' at the top of the page to set up email alerts as new job postings become available that meet your interest!</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=TiYMGut_xoJcia96AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNTUvDQBBA8dqf4GmOKnYjggjtqfSLiEgwvfQUNpsxu-1kZ9mZluZn-o-Ml_du781-72bHalTPETZ4ReKEGR6-t_UBVlX5DGviSwcVWf3hPAjY2MHuQjSv1boz1OdAJI8whw9uQdBm52Fq7Zl7wvulV02yKAoRMr2o1eCM46HgiC3fihO38o9GvM2Ypgs2r28vN5Ni__RejxGdz1PuMDkycR9QoLqqgU_tTFN-bcoVrHlINo4QIpSxC_YPQU3MxtQAAAA&shmds=v1_AdeF8KhInAnvI67oQ4DyGYxxwP2nF_xV-My3c_x6k7gQ_gscgw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=TiYMGut_xoJcia96AAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>Brainium Information Technologies Pvt. Ltd.</td><td>Anywhere</td><td>We are looking for Freelance Python Developer for our organization.\n",
       "• * Minimum 4+ years of experience required as a Freelance Python Developer.\n",
       "\n",
       "Brainium Information Technologies Pvt. Ltd.\n",
       "• * Looking for Full-time / Part- time Freelancers.\n",
       "\n",
       "Tentative Job Responsibilities:\n",
       "• Web development – Using frameworks like Django, Flask, or FastAPI.\n",
       "• Data analysis & visualization – With libraries like Pandas, NumPy, Matplotlib, Seaborn.\n",
       "• Automation & scripting – Creating scripts to automate repetitive tasks.\n",
       "• API development & integration – Building or connecting to REST and GraphQL APIs.\n",
       "• Machine learning & AI – Using TensorFlow, PyTorch, Scikit-learn.\n",
       "• Testing & debugging – Writing unit tests, fixing code issues.\n",
       "\n",
       "Interested candidates share your CV to ananya.adhikary@brainiuminfotech.com or Whatsapp +91-XXXXXXXXXX\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹400,000.00 - ₹600,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Health insurance\n",
       "• Paid time off\n",
       "• Provident Fund\n",
       "\n",
       "Education:\n",
       "• Bachelor's (Preferred)\n",
       "\n",
       "Experience:\n",
       "• Freelancing: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMMQ7CMBAERZsnUF2NhA0IGugQAoEoUtBHjnPYRs6dZR9R-BDvJDTbzMxW31m1PmfEaMgi1B_xTHDCASMnzLCEG7dQ0GTrYSIXZhdxfvAiqey1LiUqV8RIsMpyr5mw5VG_uC3_aYo3GVM0gs1mtxpVIrfYHrMJFN49XOnJuZ_i6fmB1hNHdgEL1IMouEunINBkdcH8ACwAmnypAAAA&shmds=v1_AdeF8KiGWFUpMZ9bOvPmhGq10cr3SXf4wAt23khXIHthEAw8eg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Mathematics Graduate-Programmer(Python/Java/C++/C)-Remote</td><td>GiantMind Solutions</td><td>Anywhere</td><td>Global Opportunity – Short-Term -\n",
       "\n",
       "Mathematics Problem Creator & Programmer\n",
       "\n",
       "Remote | Eligible Locations: India\n",
       "\n",
       "Duration: 1 Month |\n",
       "\n",
       "Job description:\n",
       "\n",
       "Qualification: BSc/MSc/PhD in Mathematics\n",
       "\n",
       "Your day to day work will be focused on solving High School/Advanced level mathematics questions and breaking it down step by step and translate complex mathematical and algorithmic solutions into efficient, robust, and readable code, primarily using C/C++/ Python/Java.\n",
       "\n",
       "We need someone who has completed BSc/M.Sc/Ph.d in mathematicsWe need non-teaching experience in one of the languages C++/Java/Python.\n",
       "\n",
       "You will have to clear a technical interview.\n",
       "\n",
       "This engagement is for 1 month.\n",
       "\n",
       "The project requires a time commitment of 40 hrs/week)\n",
       "\n",
       "Interested? Reach out at vijaya.lakshmi@giantmindsolutions.com\n",
       "\n",
       "Tag or refer someone who fits this!\n",
       "\n",
       "Job Types: Full-time, Part-time, Contractual / Temporary\n",
       "\n",
       "Pay: ₹40,000.00 - ₹60,000.00 per month\n",
       "\n",
       "Application Question(s):\n",
       "• Have you completed your Graduate in Mathematics?\n",
       "• Are you fine with short term contract(1 Month)?\n",
       "• Do you have experience in any one of these languages(Python/Java/C++)?\n",
       "\n",
       "Work Location: Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=mjlsp1kl83EjWpwOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQrCQBBAYWw9glhM6Q_JimCjpUUwEAh6AJkkQ3YlOxN2JqKH8o5q88rvzT-zeV2heYpooVUoEnYTGmV1kj5hjJRW9du8sCvxie683brzOrtSFCPIoJQGlDC1HoShEOkHWpy82ahH51SHvFf703kr0QlTIy_3kEb_uavHROPw-933h90rH7nfLIuAbFXgDm4yTBaEFQLDhbuAX3z9uIewAAAA&shmds=v1_AdeF8KjpLvkDMxWj8kIsH_YSsUGWrRK9EzXPskJ_wPFE6MbgPA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=mjlsp1kl83EjWpwOAAAAAA%3D%3D</td><td>Python Developer</td></tr><tr><td>Microsoft ETL / SSIS Developers</td><td>Fluor</td><td>India (+3 others)</td><td>At Fluor, we are proud to design and build projects and careers. We are committed to fostering a welcoming and collaborative work environment that encourages big-picture thinking, brings out the best in our employees, and helps us develop innovative solutions that contribute to building a better world together. If this sounds like a culture you would like to work in, you’re invited to apply for this role.\n",
       "Job Description\n",
       "\n",
       "Motivated and results driven Senior Extract, Transform and Load (ETL) Developer. Sound knowledge of SQL Server Integration Services (SSIS) using ETL in developing, testing, deploying packages, using DTS for import, export and transformation of data. Experience with SQL Server Analysis Services (SSAS) surrounding designing and deploying Multidimensional Cubes and writing MDX queries. Experience writing stored procedures, functions, triggers and views using PL/SQL and T-SQL. Excellent knowledge of Data Warehousing concepts: Star Schema, Snow-Flake Schema, Fact and Dimensional tables, Relational databases, slowing changing dimensions, data marts, aggregation design, logical and physical data models, Normal Forms(NF), On-line Analytical Processing(OLAP) and On-Line Transactional Processing (OLTP), MOLAP and ROLAP, HOLAP, normalized and de-normalized data.\n",
       "\n",
       "Basic Job Requirements\n",
       "• Programming Languages: C#, VB.Net, PHP, HTML, CSS, SQL, TSQL. PL/SQL, JavaScript\n",
       "• Databases: SQL Server 2005/2008/2008/2017 R2/2012/2014, Oracle 10g, MySQL\n",
       "• Platforms: SSIS, SSAS, SSRS, BizTalk, Informatica, Crystal Reports\n",
       "• Other: Red Gate, TFS, SVN, Visio, Erwin, Azure DevOps\n",
       "\n",
       "Other Job Requirements\n",
       "• Thorough understanding of the entire software development lifecycle, including analysis, design, configuring, programming and unit testing and deployment.\n",
       "• Expert knowledge using Microsoft Visual Studio.\n",
       "• Outstanding analytical and trouble-shooting skills, adept to multi-tasking, strong communication and interpersonal skills.\n",
       "• Ability to work efficiently in a high stress customer facing environment.\n",
       "• Proficiency in troubleshooting production related issues\n",
       "• Ability to identify potential performance bottlenecks in code and follow best practices and standards\n",
       "\n",
       "Preferred Qualifications\n",
       "• Accredited degree or global equivalent in Computer Science or related discipline\n",
       "• Experience with Engineering, Procurement & Construction (EPC) industry projects\n",
       "• Experience in computer systems or Information Technology (IT) support, with technical proficiency in operating systems and programming languages\n",
       "• Strong written and verbal communication skills\n",
       "• Strong interpersonal skills\n",
       "• Excellent analytical, technical, planning, and organizational skills\n",
       "\n",
       "To be Considered Candidates:\n",
       "Must be authorized to work in the country where the position is located.\n",
       "\n",
       "We are an equal opportunity employer. All qualified individuals will receive consideration for employment without regard to race, color, age, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, genetic information, or any other criteria protected by governing law.\n",
       "\n",
       "Notice to Candidates:\n",
       "Background checks are carried out as part of any conditional offer made, including (but not limited to & role dependent) education, professional registration, employment, references, passport verifications and Global Watchlist screening.</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=TMInSQBk-W9m3kTOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOw4CIRAA0Nhu4QGspjYRjIkW2vrJGq2w3wCOgEGGMKPZC3hv4yte9510m2vyjZgeAofbBTQY0xvY4wczVWwMCziTA0bbfAQqcCIKGWe7KFJ5qzVzVoHFSvLK00tTQUejfpLjfwNH27BmKzis1stR1RLm02N-U4NUoC_3ZH_8-zGfiAAAAA&shmds=v1_AdeF8KhNydR2dJYdV5inYyRZoMUfX7lo5lOthEGkoAmTvCxB4Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=TMInSQBk-W9m3kTOAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Datastage Developer</td><td>BNP Paribas</td><td>India</td><td>About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability.\n",
       "\n",
       "About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions..\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function :\n",
       "\n",
       "The IT Department is functionally responsible for the IT structures of the companies of the BNL BC Group and coordinates the IT community of the BNPP Group onwards with the aim of obtaining all possible synergies of purpose, ensuring full operational consistency and enabling the digital transformation of our Group\n",
       "\n",
       "As part of the IT Department of BNL, the IT Data Platform is responsible for defining and implementing the technological strategy of the company in terms of Analytics, Business Intelligence, Artificial Intelligence and Data Management.\n",
       "\n",
       "The group therefore has the responsibility of evolving and maintaining the data architecture serving Analytics, as well as creating management reporting and dashboarding solutions.\n",
       "\n",
       "Data Platform also has the responsibility of building Advanced Analytics and Artificial Intelligence solutions, and integrating them with Bank systems, making them fully part of company processes.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "SW Developer 1 (ETL)\n",
       "\n",
       "Date:\n",
       "\n",
       "Department:\n",
       "\n",
       "Location:\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "IT Data Platform\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "ISPL ADM Manager\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "(Functional)\n",
       "\n",
       "BNP PARIBAS BNL Data Platform IT Leader\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "Provide a brief description of the overall purpose of the position, why this position exists and how it will contribute in achieving the team’s goal.\n",
       "\n",
       "The requested position is developer-analyst in an open environment, which requires knowledge of the mainframe, TSO, JCL, OPC environment.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities\n",
       "\n",
       "For a predefined applications scope take care of:\n",
       "\n",
       "·Design\n",
       "\n",
       "·Implementation (coding / parametrization, unit test, assembly test, integration test, system test, support during functional/acceptance test)\n",
       "\n",
       "·Roll-out support\n",
       "\n",
       "·Documentation\n",
       "\n",
       "·Continuous Improvement\n",
       "\n",
       "• Ensure that SLA targets are met for above activities\n",
       "\n",
       "• Handover to Italian teams if knowledge and skills are not available in ISPL\n",
       "\n",
       "• Coordinate closely with Data Platform Teams’s and also all other BNL BNP Paribas IT teams (Incident coordination, Security, Infrastructure, Development teams, etc.)\n",
       "\n",
       "·Collaborate and support Data Platform Teams to Incident Management, Request Management and Change Management\n",
       "\n",
       "Contributing Responsibilities\n",
       "\n",
       "·Contribute to the knowledge transfer with BNL Data Platform team\n",
       "\n",
       "·Help build team spirit and integrate into BNL BNP Paribas culture\n",
       "\n",
       "·Contribute to incidents analysis and associated problem management\n",
       "\n",
       "·Contribute to the acquisition by ISPL team of new skills & knowledge to expand its scope\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "·Fundamental skills:\n",
       "\n",
       "o IBM DataStage\n",
       "\n",
       "o SQL\n",
       "\n",
       "o Experience with Data Modeling and tool ERWin\n",
       "\n",
       "·Important skill - knowledge of at least one of database technologies is required:\n",
       "\n",
       "o Teradata\n",
       "\n",
       "o Oracle\n",
       "\n",
       "o SQL Server.\n",
       "\n",
       "·Basic knowledge about Mainframe usage TSO, ISPF/S, Scheduler IWS, JCL\n",
       "\n",
       "·Nice to have:\n",
       "\n",
       "o Knowledge of MS SSIS\n",
       "\n",
       "o Experience with Service Now ticketing system\n",
       "\n",
       "o Knowledge of Requirements Collection, Analysis, Design, Development and Test activity\n",
       "\n",
       "o Continuous improvement approaches\n",
       "\n",
       "o Knowledge of Python\n",
       "\n",
       "o Knowledge and experience with RedHat Linux, Windows, AIX, WAS, CFT\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Basic knowledge of Italian language can be an advantage\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Ability to share / pass on knowledge\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Adaptability\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Ability to develop others & improve their skills\n",
       "\n",
       "Ability to manage / facilitate a meeting, seminar, committee, training…\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 3 years\n",
       "\n",
       "Other/Specific Qualifications (if required)\n",
       "\n",
       "Qualifications - External\n",
       "\n",
       "Na</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=r_RI4EF0papmr5GyAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQoCMQwAUFxvdnLKLNiK4KKbnIgicoP7kdbQVmpTmiC3--Pq8rbXfWadOd6v0KOiKAaCnt6UuVKDFVzYgRA2H4ELnJhDpsU-qlbZWSuSTfglTd54flku5HiyT3byZ5SIjWpGpXGzXU-mlrCcH24DDNiSQ4FU4FweCb-gaBz5hgAAAA&shmds=v1_AdeF8KjzfVjfCF5mxsrSDO-L7fSb-xrKuBYToid6c3KTgE34Eg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=r_RI4EF0papmr5GyAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>Insight Global</td><td>Insight Global</td><td>Hyderabad, Telangana, India</td><td>The Informatica ETL Developer position focuses on ETL design, development and support. Responsible for ETL processes and tools as well as database loading and manipulation to ensure that the data needs are met throughout the course of the projects and production support. The developer will work to understand the business problems and opportunities in the context of the requirements and recommend solutions that enable the organization to achieve its goals. Additional responsibilities may include interpretation and transformation of information based on business requirements and conducting detailed research of vendor products and assists with general project management.\n",
       "Some day to day responsibilities include:\n",
       "Assist with design and implementation of data warehouses, planning applications and reporting solutions.\n",
       "Develop solutions to leverage ETL tools and suggest process improvements.\n",
       "Provide ongoing maintenance and support of assigned ETL flows and their Client applications.\n",
       "\n",
       "Compensation:\n",
       "$16to $17HR\n",
       "Exact compensation may vary based on several factors, including skills, experience, and education.\n",
       "\n",
       "Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=ilWZRf7UiEHOZ3cHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_13NMQ7CMAwAQLH2CUxeWFCbICQWeEApM3vlJFYSlNpVnKG8gw8jVpZbr_vsusPEmmNqMBZxWGCAhzhQwuoTCMMoEgvtb6m1Va_WqhYTtWHL3nhZrDA52exLnP6YNWGltWCj-Xw5bWbleBz-jsxwfweq6DD08KSCHJGxh4lDxi-RVypzlgAAAA&shmds=v1_AdeF8KhmLbrgMzBBwyUeoUQL_4hKnzeejX-h_elIeYvsdlgyPg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=ilWZRf7UiEHOZ3cHAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Developer- Ab Initio</td><td>Infosys</td><td>Secunderabad, Telangana, India</td><td>The ideal candidate will be responsible for developing high-quality applications. They will also be responsible for designing and implementing testable and scalable code.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Location-Pune, Chennai, Bangalore, Chandigarh, Hyderabad\n",
       "• Minimum 5+ years of experience and strong knowledge in Ab Initio and related activities.\n",
       "• Design, develop, and implement robust ETL solutions using Ab Initio (GDE, EME, Co>Op, Express>It, Continuous Flows, etc.)\n",
       "• Involved in end-to-end implementation of Data warehousing projects, which include Business Requirements gathering, Analysis, System study, technical specifications, Coding, Testing,Code migration, Implementation, System maintenance and Documentation.\n",
       "• Proficiency indeveloping SQL queries and functionality with various databases like Oracle.\n",
       "• Familiarity with Data warehousing and ETL concepts and techniques\n",
       "• Expertise and hands on experience in Ab Initio technology, create generic components, batch and continuous flows, Testing framework and Express IT.\n",
       "• Familiarity with scheduling tools (e.g., Autosys, Control-M) and version control systems (e.g., Git)\n",
       "• UNIX shell scripting will be an added advantage in scheduling/running application jobs.\n",
       "• Understand project requirements and translate them into technical solutions which meets the project quality standards\n",
       "• Ability to work in team in diverse/multiple stakeholder environments and collaborate with upstream/downstream functional teams to identify, troubleshoot and resolve data issues.\n",
       "• Troubleshoot and resolve performance, data quality, and functional issues\n",
       "• Excellent verbal and written communication skills.\n",
       "• Stay up to date with new technologies and industry trends in Development.\n",
       "• Collaborate with business analysts, data architects, and other developers to understand data requirements and ensure the successful delivery of solutions.</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=hXIgJIY_QJV09FmsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU63Km0jgotOgiIVN7uXS3umkXgXclHqp_i36vLWV3xmhTm2FzjQi4JEShXsLTTssxeo4CwWlDD1IwjDScQFmu_GnKNujVENtdOM2fd1Lw8jTFYmcxerfzodMVEMmKlbb1ZTHdktFw3fRN8KnuFK_ZMHSmhxKKGlgOyQsfz9g8cvEPYtGJwAAAA&shmds=v1_AdeF8Kj8aGfXOqTOLIUxElazN0gGyDYLuW66E6RhyUWwXah5zQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=hXIgJIY_QJV09FmsAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Developer- Hyderabad Location</td><td>iitjobs inc</td><td>Hyderabad, Telangana, India</td><td>Greetings!!!We have a very urgent opening for ETL Developer for Hyderabad location. Note:Job Type: Long term ContractualMode: Hybrid for HyderabadExperience: 6+ yearsJoining: Immediate to 20 DaysDescription 6+ years of experience in ETL (Informatica,Mulesoft), SQL, PL/SQL and Tableau is mandatory Experience with Relational databases like PostgreSQL. Experience with data visualization tools like Tableau, Power BI, or similar. Experience with the Salesforce.com platform and related technologies Proficiency in data manipulation and analysis using SQL. Good to have Snowflake, Airflow, Spark Agile Methodologies and well versed with GUS/JIRA If you are interested, please share your resume at: suhas@iitjobs.com</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=V0oKWmJqAnIDHm0TAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WNywrCMBBFcdtPcDXr0jYiCKJbxQdddl8m6ZCmxEzIBKnf44_arlzcAwcO3OK7KY7XroULvclzpFTD_TNQQo0DtGwwOw5Qw5M1CGEyIyx-Y7aetucx5ygnpUR8YyUvsWkMvxQH0jyribWs6GXERNFjpn5_2M1NDLYsnctrAC6YZf_bCjryGCwGrOARBoc_sAhC3aYAAAA&shmds=v1_AdeF8KiontKOXG3bNsK8LGd0DVCbbgszksdymS0_LKVU07oVug&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=V0oKWmJqAnIDHm0TAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Developer-4+yrs exp-Hyderabad only (Must have Exp in Redshift and Data Warehouse)</td><td>Minfy</td><td>Hyderabad, Telangana, India</td><td>As requested, I am sharing JD for ETL developer, modify accordingly as per the requirement and make use of this for our requirements.\n",
       "\n",
       "Since we need someone who can work on Redshift and modified accordingly as per our requirement.\n",
       "\n",
       "Job Title: ETL Developer\n",
       "\n",
       "Location: Hyderabad\n",
       "\n",
       "Experience: 3-5 years\n",
       "\n",
       "Employment Type: Full-time / Contract\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "We are seeking a skilled ETL Developer to design, develop, and optimize data pipelines and ETL processes on Amazon Redshift. The ideal candidate will have strong experience in Redshift, SQL, ETL tools, and data warehousing concepts, along with expertise in building scalable data solutions in cloud environments.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design, develop, and maintain ETL pipelines to extract, transform, and load data into Amazon Redshift.\n",
       "• Work closely with business analysts, data engineers, and stakeholders to understand data requirements and translate them into ETL solutions.\n",
       "• Optimize complex SQL queries and ensure efficient performance in Redshift.\n",
       "• Perform data profiling, data quality checks, and troubleshoot data issues.\n",
       "• Implement incremental loads, change data capture (CDC), and performance tuning techniques.\n",
       "• Monitor ETL jobs, data pipeline health, and manage data recovery procedures when required.\n",
       "• Develop stored procedures, functions, and scripts to support data processing and transformation.\n",
       "• Collaborate with DevOps to automate deployments and manage infrastructure as code (IaC).\n",
       "• Document technical designs, processes, and data flows.\n",
       "• Skills Required:\n",
       "\n",
       "Strong hands-on experience with Amazon Redshift\n",
       "\n",
       "Expertise in SQL (writing, tuning, debugging complex queries)\n",
       "\n",
       "Experience with ETL tools (AWS Glue, Matillion, Informatica, Talend, etc.)\n",
       "\n",
       "Proficiency in Python/Scala for data processing and scripting.\n",
       "\n",
       "Strong knowledge of Data Warehousing concepts, Star Schema, and Dimensional Modeling.\n",
       "\n",
       "Experience with AWS services (S3, Lambda, Step Functions, CloudWatch) is a plus.\n",
       "\n",
       "Familiarity with performance tuning techniques in Redshift (Distribution Keys, Sort Keys, Vacuum, Analyze).\n",
       "\n",
       "Experience with Git, CI/CD pipelines, and version control.\n",
       "\n",
       "Nice to Have:\n",
       "• Experience with Redshift Spectrum and Redshift Serverless.\n",
       "• Exposure to Snowflake or other cloud data platforms.\n",
       "• Understanding of Data Governance, Data Lineage and Metadata Management.\n",
       "• Knowledge of Airflow for orchestration.\n",
       "\n",
       "--\n",
       "\n",
       "Thanks & Regards,\n",
       "\n",
       "Purnima Solanki\n",
       "\n",
       "HR Recruiter\n",
       "\n",
       "Survey No. 10, Divine Babylon Building, Whitefields, Kondapur,\n",
       "\n",
       "Opp. Lane of Jayabheri Silicon Valley, Hyderabad-500084, Telangana.\n",
       "\n",
       "Global Office: India | Kuala Lumpur, Malaysia (Regional Office) Manilla,\n",
       "\n",
       "Philippines | Singapore | USA\n",
       "\n",
       "www.minfytech.com</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=vl84OgJDddKjDtBFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2OMU8CQRBGY8tPsPoKCwXuzhhotIUoRBpDYknmbofbJevMZmchdz_TfyQ0Nq98701-7yZxvf_Eii8cNXGuFrMxG3hI1cfoOFNLDipxxOPubAWeLoz1kBAEX-zMh2MBicOKCuGbMns9Gz-hwlZbGFPu_FWAd9U-8v2bLyXZa9OYxbq3QiV0dac_jQq3OjQnbe2Gg_mrK0UqfHhZPg91kn76sAtyHG_p_7c59hxJehKaYyMu0B9XyTaI1AAAAA&shmds=v1_AdeF8KhqOdLwi-2SfAhAZYVI87vitWhV6zvL65SaEEVKSbDGyQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=vl84OgJDddKjDtBFAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Developer- Hyderabad (2-3+ Years of Experience)</td><td>A Client of Analytics Vidhya</td><td>Hyderabad, Telangana, India</td><td>Role Summary:\n",
       "\n",
       "•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n",
       "\n",
       "•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n",
       "\n",
       "•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n",
       "\n",
       "•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n",
       "\n",
       "•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n",
       "\n",
       "•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n",
       "\n",
       "•Create quality rules in Talend.\n",
       "\n",
       "•Tune Talend jobs for performance optimization.\n",
       "\n",
       "•Write relational and multidimensional database queries.\n",
       "\n",
       "•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n",
       "\n",
       "•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n",
       "\n",
       "•Exposure in Map Reduce components of Talend / Pentaho PDI.\n",
       "\n",
       "•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n",
       "\n",
       "Job Specification:\n",
       "\n",
       "•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n",
       "\n",
       "•Having an experience of 2 – 3+ years.\n",
       "\n",
       "•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n",
       "\n",
       "•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n",
       "\n",
       "•Working knowledge of relational database theory and dimensional database models.\n",
       "\n",
       "•Ability to write complex SQL database queries.\n",
       "\n",
       "•Ability to work independently.</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOuw6CUBBEY8snWG3pg4fR2EhFlKjE0phYmQVWuOa6S9gbAz_odwmVzTQnc2a878TL0usFDvQhKw21AZz6klrMsYTZOtgs4U7YKsgT0m7ghrigOQSQSQ46oKIGYTiKVJamce1co7soUrVhpQ6dKcJC3pEw5dJFL8l1jIfW2FJj0dFjvV11YcPVIk5gbwe_G8cSRtsPbYWbKesewfD_mQ9XssgVMvpw5tLgD84obTrJAAAA&shmds=v1_AdeF8KhNH4dOhsM_ohqTFHeygbi7_Rs650WHWh26bXMFYZ5jQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>Barclays</td><td>Maharashtra, India</td><td>Join us as a ETL Developer at Barclays, responsible for supporting the successful delivery of Location Strategy projects to plan, budget, agreed quality and governance standards. You'll spearhead the evolution of our digital landscape, driving innovation and excellence. You will harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences.\n",
       "\n",
       "To be successful as a ETL Developer you should have experience with:\n",
       "• Good knowledge of Python\n",
       "• Extensive hands-on PySpark\n",
       "• Data Warehousing concept\n",
       "• Strong SQL knowledge\n",
       "• Bigdata technologies (HDFS)\n",
       "• AWS working exposure\n",
       "\n",
       "Some other highly valued skills may include:\n",
       "• Working knowledge of AWS\n",
       "• Familiar with Bigdata\n",
       "\n",
       "You may be assessed on the key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen strategic thinking and digital and technology, as well as job-specific technical skills.\n",
       "\n",
       "This role is based in Pune.\n",
       "\n",
       "Purpose of the role\n",
       "\n",
       "To build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\n",
       "\n",
       "Accountabilities\n",
       "• Build and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\n",
       "• Design and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\n",
       "• Development of processing and analysis algorithms fit for the intended data complexity and volumes.\n",
       "• Collaboration with data scientist to build and deploy machine learning models.\n",
       "\n",
       "Analyst Expectations\n",
       "• Will have an impact on the work of related teams within the area.\n",
       "• Partner with other functions and business areas.\n",
       "• Takes responsibility for end results of a team’s operational processing and activities.\n",
       "• Escalate breaches of policies / procedure appropriately.\n",
       "• Take responsibility for embedding new policies/ procedures adopted due to risk mitigation.\n",
       "• Advise and influence decision making within own area of expertise.\n",
       "• Take ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct.\n",
       "• Maintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function.\n",
       "• Demonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\n",
       "• Make evaluative judgements based on the analysis of factual information, paying attention to detail.\n",
       "• Resolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents.\n",
       "• Guide and persuade team members and communicate complex / sensitive information.\n",
       "• Act as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation.\n",
       "\n",
       "All colleagues will be expected to demonstrate the Barclays Values of Respect, Integrity, Service, Excellence and Stewardship – our moral compass, helping us do what we believe is right. They will also be expected to demonstrate the Barclays Mindset – to Empower, Challenge and Drive – the operating manual for how we behave.</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=N7eHv3R2ndO_i6H7AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQoCMQyAYVzvERwkg5NoK4KLt4kiim7uR1pDe1Kb0gQ5Z1_cc_mnj7_5Tpr58X6FA70pcaEKK7iwAyGsPgJnODGHRNM2qhbZWSuSTBBF7b3x_LKcyfFgn-zkn04iVioJlbrNdj2YksNith9vCT8CfYYbjgIlasUlnPOjxx9aM1wnhgAAAA&shmds=v1_AdeF8KiNb57kZV1yOgH3Jqou_vAYWsQmpnSrtRn5N6coTcSuqw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=N7eHv3R2ndO_i6H7AAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>ARMPL</td><td>Bhopal, Madhya Pradesh, India</td><td>Job Description : - To analyse the user requirements (new / change requests) and develop solutions for ETL implementation.- To follow the Microsoft framework- To understand the existing data source setup and plan for ETL using SSIS and SQL server.- Implementing and maintaining ETL process in platform recommended by client using SSIS- Monitoring performance of the ETL jobs in Platform and provide recommendations (where necessary)- To perform batch and real time production support- Design, develop, and maintain SQL Server databases, stored procedures, functions, and triggers- Optimize complex T-SQL queries for performance and scalability- Perform data analysis, data migration, and transformation- Build and maintain ETL processes (SSIS preferred)- Collaborate with developers and analysts to meet project goals- Ensure data security, consistency, and best practices- SSIS experience ingesting and doing complex transformations.- Create and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organizations data assets- Ensure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis- Work closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organizations data architecture is integrated and aligned with other IT systems and applications- Stay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organizations data architecture- Generates application documentation.- Contributes to systems analysis and design.- Designs and develops moderately complex applications.- Contributes to integration builds.- Contributes to maintenance and support.Requirements :- Experience in ETL by analyzing source data and providing transformation logic- Experience in SSIS for ETL implementation. Min 1-2 years of SSIS experience and the candidate should be willing to work on SSIS (Mandatory).- Experience in DBs like SQL server- Experience in writing scripts for transformation if required- Experience in suggesting tools for clients new requirement- Provide support for production runs and improve performance if required (ref:hirist.tech)</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=v90CsYml292k3YcnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43iIPURAQXOymKKC0UcS_X5GgqMRdyQeo3-NPiG17xnRWL86OGE73Jc6QEa7hxD0KYjAMOcGEePM0rl3OUvdYiXg2SMY9GGX5pDtTzpJ_cy79OHCaKHjN1291mUjEMq-Xh3rQ1jAGOjiP6Ehq07oPQJrQkroRrsCP-AMyvzq6OAAAA&shmds=v1_AdeF8KhweSv_8aJrYESRw-930uLDUPvAZ7z71ssTjQ99ibLEbQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=v90CsYml292k3YcnAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>Streamsets ETL Developer, Associate</td><td>E902 DWS India Private Limited, Maharashtra Branch</td><td>India</td><td>Job Description: Job Title - Streamsets ETL Developer, Associate Location - Pune, India Role Description Currently DWS sources technology infrastructure, corporate functions systems [Finance, Risk, HR, Legal, Compliance, AFC, Audit, Corporate Services etc] and other key services from DB. Project Proteus aims to strategically transform DWS to an Asset Management standalone operating platform; an ambitious and ground-breaking project that delivers separated DWS infrastructure and Corporate Functions in the cloud with essential new capabilities, further enhancing DWS’ highly competitive and agile Asset Management capability. This role offers a unique opportunity to be part of a high performing team implementing a strategic future state technology landscape for all DWS Corporate Functions globally. We are seeking a highly skilled and motivated ETL developer (individual contributor) to join our integration team. The ETL developer will be responsible for developing, testing and maintaining robust and scalable ETL processes to support our data integration initiatives. This role requires a strong understanding of database, Unix and ETL concepts, excellent SQL skills and experience with ETL tools and databases. What we’ll offer you As part of our flexible scheme, here are just some of the benefits that you’ll enjoy Best in class leave policy Gender neutral parental leaves 100% reimbursement under childcare assistance benefit (gender neutral) Sponsorship for Industry relevant certifications and education Employee Assistance Program for you and your family members Comprehensive Hospitalization Insurance for you and your dependents Accident and Term life Insurance Complementary Health screening for 35 yrs. and above Your key responsibilities This role will be primarily responsible for creating good quality software using the standard coding practices. Will get involved with hands-on code development. Thorough testing of developed ETL solutions/pipelines. Do code review of other team members. Take E2E Accountability and ownership of work/projects and work with the right and robust engineering practices. Converting business requirements into technical design Delivery, Deployment, Review, Business interaction and Maintaining environments. Additionally, the role will include other responsibilities, such as: Collaborating across teams Ability to share information, transfer knowledge and expertise to team members Work closely with Stakeholders and other teams like Functional Analysis and Quality Assurance teams. Work with BA and QA to troubleshoot and resolve the reported bugs / issues on applications. Your skills and experience Bachelor’s Degree from an accredited college or university with a concentration in Science or an IT-related discipline (or equivalent) Hands-on experience with StreamSets, SQL Server and Unix. Experience of developing and optimizing ETL Pipelines for data ingestion, manipulation and integration. Strong proficiency in SQL, including complex queries, stored procedures, functions. Solid understanding of relational database concepts. Familiarity with data modeling concepts (Conceptual, Logical, Physical) Familiarity with HDFS, Kafka, Microservices, Splunk. Familiarity with cloud-based platforms (e.g. GCP, AWS) Experience with scripting languages (e.g. Bash, Groovy). Excellent knowledge of SQL. Experience of delivering within an agile delivery framework Experience with distributed version control tool (Git, Github, BitBucket). Experience within Jenkins or pipelines based modern CI/CD systems How we’ll support you Training and development to help you excel in your career Coaching and support from experts in your team A culture of continuous learning to aid progression A range of flexible benefits that you can tailor to suit your needs About us and our teams Please visit our company website for further information: https://www.db.com/company/company.htm For over 150 years, our dedication to being the Global Hausbank for our clients has been driven by our people – in around 60 countries and across more than 150 nationalities. Their deep understanding, insights, expertise, and passion help our clients navigate an increasingly complex world – be it in our Corporate Bank, our Private Bank, our Investment Bank or our Asset Management (DWS) division. Together we can make a great impact for our clients at home and abroad, securing their lasting success and financial security. More information at: Deutsche Bank Careers (db.com)</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=bDLqsL0ZNf2MhCujAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNwQqCUBBFaesntJp1mIrQIlsVShQGgUFLGXXQF_pG3gzil_V9KW3O6p57vO_GSwp1hIOQCmSvHFKaqOeRnA9nEa4NKsEe7lyBELq6A7ZwZW572p461VGSMBTpg1YU1dRBzUPIliqeww9XsqKUDh2N_XJVxodoDkbb7k7ZMYohfRdws41BeDozra3cDEap8eGBi4XSqUO4OLRL2tj_-AfWzd90uQAAAA&shmds=v1_AdeF8Kgc1sPAPO3xWRGm3yrUZuLvaeWGSP4zv7CLx4O1bRJDkA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=bDLqsL0ZNf2MhCujAAAAAA%3D%3D</td><td>ETL Developer</td></tr><tr><td>Spark Engineer</td><td>Staffingine LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3NsQrCMBAA0L1fIE63uAgmIrjoKCJKt35AucRrkhrvQi5DJ79ddHnr6z7dZihYX3DlkJiowg4e4kAJq48gDDeRkGl9jq0VPVmrmk3Qhi154-VthcnJYmdx-mPUiJVKxkbj4bhfTOGwXQ0Npyn9C-j7CySGOz8TfgHTIbRdgAAAAA&shmds=v1_AdeF8KjUl41s9rD9xNd7RywARJoJQ3Nel1qruGSZXxbjxJAYWA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Data Engineer with Scala and Spark</td><td>Albireo Tech System</td><td>Anywhere</td><td>Description:\n",
       "\n",
       "We are Looking for candidate having 7+ Yrs of Experience in Database for Below Requirement\n",
       "\n",
       "Tech skills :\n",
       "Data Engineering, Spark, Hive, Scala, Airflow\n",
       "Time : Morning IST(Flexibility Required for Both morning an Evening for 2 Days\n",
       "Duration : 3 hrs /Day\n",
       "\n",
       "Work Type: Screen Sharing Via Meeting Platforms\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "Contract length: 4 months\n",
       "\n",
       "Pay: ₹30,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Experience:\n",
       "• Scala: 7 years (Required)\n",
       "• Pyspark: 7 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=DRc3KS95GCWtyWMqAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CeJws2AjgoM6CYroWvdySY8kmuZC7sD6I36vurzxNZ9ZszuhIpyzj5mowitqgM5hQsA8QFewPmEFN7YghNUF4AwXZp9ofgiqRfbGiKTWi6JG1zoeDWeyPJkHW_nTS8BKJaFSv9mup7Zkv1wck42VGO70S7u3KI0QM1zzEPELwmnNeZkAAAA&shmds=v1_AdeF8KjvWDUQa9bfitKmr9kDc_mT56uQlftcSO1qwVaf86J8Cw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=DRc3KS95GCWtyWMqAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Senior Apache Spark Engineer (Deployment, Tuning, and Internals)</td><td>AkashX</td><td>Anywhere</td><td>Job Title: Senior Apache Spark Engineer (Deployment, Tuning, and Internals)\n",
       "\n",
       "Location: Remote /India\n",
       "Employment Type: Full-Time\n",
       "\n",
       "About AkashX.ai\n",
       "\n",
       "AkashX.ai is building next-generation analytics infrastructure that combines advanced SQL execution with optimized storage-compute integration. We are looking for an engineer with deep expertise in Apache Spark internals to help us deploy, optimize, and scale high-performance clusters.\n",
       "\n",
       "Role Overview\n",
       "\n",
       "You will be responsible for deploying, tuning, maintaining, and scaling large-scale Apache Spark clusters. This role requires a strong understanding of Spark internals, from the Catalyst optimizer to Tungsten execution engine. Experience with Velox and Gluten for query acceleration is a plus.\n",
       "\n",
       "Key Responsibilities\n",
       "• Deploy, configure, and maintain Apache Spark clusters in production (YARN, Kubernetes, or standalone).\n",
       "• Tune Spark jobs for performance, including shuffle optimization, memory management, and adaptive query execution.\n",
       "• Diagnose and resolve performance bottlenecks at the JVM, Spark, and cluster levels.\n",
       "• Scale workloads efficiently across large clusters while ensuring reliability and cost efficiency.\n",
       "• Work with Velox and Gluten for query acceleration where applicable.\n",
       "• Integrate Spark with data lakes, warehouses, and downstream systems.\n",
       "• Manage Spark upgrades and implement CI/CD pipelines for Spark workloads.\n",
       "• Monitor and maintain cluster health using observability tools.\n",
       "• Document deployment and tuning best practices.\n",
       "\n",
       "Required Skills & Experience\n",
       "• 5+ years of production experience with Apache Spark at scale.\n",
       "• Deep knowledge of Spark internals, including:\n",
       "• Catalyst optimizer\n",
       "• Tungsten execution engine\n",
       "• RDD and DataFrame execution flows\n",
       "• Shuffle and stage execution planning\n",
       "• Strong experience in Spark performance tuning (shuffle, memory, GC tuning, partitioning).\n",
       "• Proficiency in Scala and/or Java (Python experience a plus).\n",
       "• Familiarity with Velox and Gluten.\n",
       "• Experience with distributed systems and JVM tuning.\n",
       "• Knowledge of Parquet, ORC, Arrow, and other columnar formats.\n",
       "• Hands-on experience with monitoring tools such as Spark UI, Prometheus, and Grafana.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Contributions to Apache Spark or related open-source projects.\n",
       "• Familiarity with C++ for query engine optimization.\n",
       "• Experience with large-scale cloud deployments (AWS EMR, Databricks, GCP Dataproc, Azure Synapse).\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹1,243,420.91 - ₹4,538,447.54 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Health insurance\n",
       "• Paid sick time\n",
       "• Paid time off\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Work Location: Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=LJ62fDUbwkOevjRkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFy7uDvdqFJbEVx0KiiiawXdyrU9ktj2LuQi1C_yN63LW1_ynSWPkthJgMJjYwlKj6GDMxvHRAGWJ_K9fAbimML9zY5NCsgtXDlSYOx1BRu4SQ1KGBoLwnARMT0tjjZGr4c8V-0zoxGja7JGhlyYahnzl9T6p1KLYVowUrXbb8fMs1nPiw7VPsHxNLUOf6KxurOqAAAA&shmds=v1_AdeF8KjbctUqTTSP6-SWypiOHgyAT7JGHj-kIuH8cWhoQVsqqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=LJ62fDUbwkOevjRkAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>Visa</td><td>India</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSq1EcFFN1H8mYSCa7m2aRKNdyF3Q30LH1n8hq_4TopDrTgMcERFOJELZG2GeZ0wv0q4f9QzlXDBnjktYAU3bkEs5s4DE5yZXbSzvVdNsjNGJFZOFDV0Vcdvw2RbHs2TW_nXiMdsU0S1zWa7HqtEbjl9BEEIBFfqA_4AG-QyopMAAAA&shmds=v1_AdeF8KjOBn1yJUpUCbIoZfiL9XOirLPsVpNYluZLK1C-IlMUqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Big Data Engineer - Spark/Scala</td><td>The IT Firm</td><td>India</td><td>Position : Big Data Engineer\n",
       "\n",
       "Industry : Information Technology / Data Engineering\n",
       "\n",
       "Department : Data Engineering\n",
       "\n",
       "Job Overview\n",
       "\n",
       "We are looking for a highly skilled and motivated Big Data Engineer with expertise in Apache Spark, Scala, and Java to join our dynamic team. The ideal candidate will have experience in designing, developing, and implementing scalable and high-performance data processing systems. The successful candidate will work on large-scale data sets and build robust data pipelines to extract, transform, and load (ETL) complex data from various sources to facilitate analytics and :\n",
       "• Design and develop scalable, high-performance data pipelines using Apache Spark.\n",
       "• Write efficient code in Scala and Java for distributed data processing tasks.\n",
       "• Collaborate with data scientists, analysts, and other engineers to integrate various data sources and ensure smooth data flow.\n",
       "• Create and maintain ETL processes that handle structured, semi-structured, and unstructured data.\n",
       "• Optimize performance of data processing tasks, identify bottlenecks, and implement solutions to improve efficiency.\n",
       "• Perform data ingestion from various sources, including relational databases, APIs, and file systems.\n",
       "• Develop and maintain real-time data streaming solutions using Apache Kafka and Spark Streaming.\n",
       "• Ensure data quality and integrity throughout the data pipeline.\n",
       "• Troubleshoot and debug issues across the data pipeline.\n",
       "• Work with cloud platforms (e.g., AWS, Azure, or GCP) to deploy and scale Big Data solutions.\n",
       "• Build and maintain data models and assist with data architecture decisions.\n",
       "• Conduct unit tests and ensure the deployment of high-quality code.\n",
       "• Stay up to date with industry trends and emerging technologies in the Big Data ecosystem.\n",
       "\n",
       "Required Skills And Qualifications\n",
       "• Bachelor's or Master's degree in Computer Science, Engineering, or related field.\n",
       "• 5+ years of hands-on experience in Big Data technologies, specifically with Apache Spark.\n",
       "• Proficient in Scala and Java programming languages.\n",
       "• Strong knowledge of Hadoop ecosystem (Hive, HBase, Pig, etc.).\n",
       "• Expertise in Spark SQL and working with structured and unstructured data.\n",
       "• Experience with NoSQL databases (e.g., MongoDB, Cassandra).\n",
       "• Understanding of distributed computing concepts and parallel processing.\n",
       "• Hands-on experience with Apache Kafka, Flume, or similar messaging platforms.\n",
       "• Familiarity with cloud-based Big Data solutions (AWS, Google Cloud, or Microsoft Azure).\n",
       "• Experience in data integration and transformation using ETL tools.\n",
       "• Strong problem-solving skills with the ability to troubleshoot complex data-related issues.\n",
       "• Familiarity with data warehousing concepts and tools.\n",
       "• Experience with version control tools (e.g., Git).\n",
       "• Ability to work in an agile, fast-paced environment.\n",
       "\n",
       "Preferred Skills\n",
       "• Knowledge of Apache Hudi or Delta Lake for managing large datasets.\n",
       "• Experience with Kubernetes and Docker for containerized deployment.\n",
       "• Familiarity with Apache Airflow for orchestrating data workflows.\n",
       "• Understanding of Machine Learning concepts and integration with Big Data platforms.\n",
       "• Prior experience working in an Agile team environment.\n",
       "\n",
       "Benefits\n",
       "• Competitive salary and benefits package.\n",
       "• Work with cutting-edge technologies in a collaborative and fast-paced environment.\n",
       "• Opportunity for career growth and development in Big Data and cloud technologies.\n",
       "• Flexible working hours and remote work options.\n",
       "• Health and wellness programs.\n",
       "\n",
       "(ref:hirist.tech)</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=G4OalA2QwET-irlQAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHvQrCMBAAYFw7OzndLJiIoINu4g91bfdyjUcSTe9CLkPfwZdWlw--5rNoDufo4YIV4co-MlGBDXQZy9t2DhP-9pARlLC4AMJwF_GJVqdQa9ajtarJeK1YozNOJitMo8z2JaP-GTRgoZyw0rDbb2eT2a-XfSBoe7jFMkFkaPkZ8Qu5yf47jgAAAA&shmds=v1_AdeF8KhAh6hgrFZo3jIxPiI_hhf9k-NtPBR_cn2OP0edPnuUJw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=G4OalA2QwET-irlQAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Senior Lead Engineer(Python & Spark )</td><td>IND201 Refinitiv India Shared Services Private Limited</td><td>India</td><td>Senior Lead Engineer (Python + Spark in AWS) Main Responsibilities / Accountabilities Looking for 10 t 14 years of proven experience Design, build and maintain robust, scalable and efficient ETL pipelines using Python and Spark, ensuring alignment with data lakehouse architecture on AWS Develop and optimize workflows leveraging AWS services such as Glue, Glue Data Catalog, Lambda and S3. Implement data quality and governance frameworks to ensure reliable and consistent data processing across the platform. Collaborate with cross-functional teams to gather requirements, provide technical insights and deliver high-quality data solutions. Drive the migration of existing data processing workflows to the lakehouse architecture, leveraging Iceberg capabilities Establish and enforce best practices for coding standards, design patterns, and system architecture. Monitor and improve system performance and data reliability through proactive analysis and optimization techniques. Lead technical discussions, mentor team members, and foster a culture of continuous learning and innovation. Ensure all solutions are secure, compliant, and meet company and industry standards. Key Relationships Senior Management and Architectural Group Development Managers and Team Leads Data Engineers and Analysts Agile team members Crucial Skills/Experience Extensive Expertise in Python and Spark: Consistent track record of designing and implementing complex data processing workflows. AWS Services: Strong experience with AWS Glue, Glue Data Catalog, Lambda, S3 and EMR with a focus on data lakehouse solutions. Data Quality and Governance: Deep understanding of data quality frameworks, data contracts and governance standard processes. Scalable Architecture: Ability to design and implement scalable, maintainable, and secure architectures using modern data technologies. Iceberg and Lakehouse: Hands-on experience with Apache Iceberg and its integration within data lakehouse environments Problem-Solving and Performance Optimization: Expertise in identifying bottlenecks and optimizing data workflows for performance and cost-efficiency Agile Methodologies: Strong experience in Agile development, including sprint planning, reviews, and retrospectives Interpersonal Skills: Excellent verbal and written communication, with the ability to articulate complex technical solutions to diverse audiences Desired Skills/Experience Familiarity with additional programming languages (eg. Java). Experience with serverless computing paradigms. Knowledge of data visualization or reporting tools for stakeholder communication. Certification in AWS or data engineering (eg. AWS Certified Data Analytics, Certified Spark Developer). Education/Certifications A bachelor's degree in Computer Science, Software Engineering or a related field is helpful. Equivalent professional experience or certifications will also be considered. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organization of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone’s race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it’s used for, and how it’s obtained, your rights and how to contact us as a data subject. If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice. Learn more about our graduate and internship programmes. If you want to apply for a job, please click the Apply button. You will then be redirected to our Careers sign-in page where you can enter your existing credentials or set up an account with us. If there is nothing that currently suits you, feel free to send us your Resume/CV LSEG (London Stock Exchange Group) is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our culture of connecting, creating opportunity and delivering excellence shapes how we think, how we do things and how we help our people fulfil their potential. Our Data & Analytics, Capital Markets and Post Trade divisions have a combined power that provides a comprehensive, integrated suite of trusted financial market infrastructure services to help our customers pursue their ambitions. Explore our divisions LSEG is headquartered in the United Kingdom, with significant operations in 70 countries across Europe, the Middle East, Africa, North America, Latin America and Asia Pacific. Find out more Get to know some of our people who are pushing the boundaries of technology, finance and more around the world.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=M33odOQv5NcwSnbJAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWLwQqCQBBA6eondJpTVJCa0KWCLkUYEpIfIOs66ZTOyO4i9m99XEqXd3nved-Zd8yQSQwkqEq4cEWMaJbpx9XCsICsU-YNK9jATQqwqIyuYTRXkarB-aF2rrP7ILC28SvrlCPta2kDYSxkCF5S2Am5rZXBrlEO82gXDn7H1foU389RuIUHPonJUQ8xl6Qgm9oSMjQ9abSQGurHERJqyY2C-B_-AHo-x7G_AAAA&shmds=v1_AdeF8Kg_AQkR9dVinW1WCvWDkDu6ppjbCKxt9wGM47k7T8ExJA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=M33odOQv5NcwSnbJAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Senior Spark Data Engineer</td><td>Nestor Technologies</td><td>India</td><td>Qualifications:Bachelors degree in computer science or a related field10+ years of professional experienceStrong SQL skills and knowledge in python requiredSeveral years experience with Spark and Databricks</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=fF8lrVvnb9SdZ6hNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNoQ4CMQwA0GDvEwiimoQNSDBgIQQE5vCX3mi2wWiXteK-gm8GzLOv-8y6TU-cpUFfsb3giIZw4piZqMEKrjKCEraQQBjOIrHQ_JDMqu69Vy0uqqHl4IK8vTCNMvmnjPpn0ISNakGjYbtbT65yXC5upPb77hQSS5GYSSEzXPiR8QtFFSKbkQAAAA&shmds=v1_AdeF8KhZkjeTrmpbUjnRnYvh_sF6rhv8NM8yxYj0YbYNPJ1Z_g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=fF8lrVvnb9SdZ6hNAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Sr. Software Engineer - Java & Python development, Kafka, Spark, SQL</td><td>Visa</td><td>India</td><td>Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n",
       "\n",
       "Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Visa’s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world’s most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you’ll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms.\n",
       "\n",
       "The Opportunity:\n",
       "\n",
       "We are looking for Versatile, curious, and energetic Software Engineers who embrace solving complex challenges on a global scale. As a Visa Software Engineer, you will be an integral part of a multi-functional development team inventing, designing, building, and testing software products that reach a truly global customer base. While building components of powerful payment technology, you will get to see your efforts shaping the digital future of monetary transactions.\n",
       "\n",
       "The Work itself:\n",
       "• Design code and systems that touch 40% of the world population while influencing Visa’s internal standards for scalability, security, and reusability\n",
       "• Collaborate multi-functionally to create design artifacts and develop best-in-class software solutions for multiple Visa technical offerings\n",
       "• Actively contribute to product quality improvements, valuable service technology, and new business flows in diverse agile squads\n",
       "• Develop robust and scalable products intended for a myriad of customers including end-user merchants, b2b, and business to government solutions.\n",
       "• Leverage innovative technologies to build the next generation of Payment Services, Transaction Platforms, Real-Time Payments, and Buy Now Pay Later Technology\n",
       "• Opportunities to make a difference on a global or local scale through mentorship and continued learning opportunities\n",
       "\n",
       "Essential Functions:\n",
       "• Managing OpenSearch and Apache Druid Clusters.\n",
       "• Write efficient and scalable code in Python and Java to support data engineering projects.\n",
       "• Utilize SQL for data analytics and reporting tasks.\n",
       "• Engineer and develop solutions using OpenSearch/Elastic including Kafka streaming.\n",
       "• Collaborate with cross-functional teams to understand requirements and deliver solutions.\n",
       "• Ensure data integrity, quality, and security throughout the data lifecycle.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelors degree AND 4+ years of relevant work experience\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Bachelor’s degree in computer science, Engineering, or any related field with minimum 4 years of experience\n",
       "• 3+ years of experience in Platform Engineering\n",
       "• 3+ years of experience in Python and Java programming, Ansible for automation.\n",
       "• 3+ years of experience working in a Linux or Unix environment shell scripting.\n",
       "• 3+ years of experience in data analytics, including proficiency in SQL.\n",
       "• 1+ years of experience in OpenSearch/Elastic Platform\n",
       "• 1+ years of experience in Kubernetes, Dockers good to have.\n",
       "• 1+ year of experience in managing Kafka, Druid Clusters.\n",
       "• Strong problem-solving skills and attention to detail.\n",
       "• Excellent communication and collaboration skills.\n",
       "• Experience with cloud-based data engineering solutions is good to have.\n",
       "• Experience with big data technologies such as Hadoop, Kafka and Spark is good to have.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=6Al4qOfxYxNIcPb9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNuwrCMBRAcS34A053cpCaiOCis4iPQSm4OJTb9jaNTXNDEmr9JP_SdjlnOXCS3yx5ZV5AxnX8oCc4WqUtkYc1XLBHWML9Gxu2UFFPhl1HNqZwxbrFFDKHvh31uE05FxAIfdnAmJ-YlaHFoYnRhb2UIRihQsSoS1FyJ9lSwYN8cxEm5KEZ785gpHy72wzCWbWaP3VA0BbOttL4B6Sze2qsAAAA&shmds=v1_AdeF8Kh8EcAMlDRS5BiephThdTDDA0nhPLsjXsJdb-cVl2V7hA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=6Al4qOfxYxNIcPb9AAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Big Data Lead/ Lead Data Engineer/Spark Tech Lead</td><td>Tanisha Systems  Inc</td><td>India</td><td>Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWJuwrCQBBFsc0nCMLUglkRbLSTiA_skj5MNsNmNZlZdraIP-R3Gk1zLvec7LPIrifvoMCE8CBszZ_zP7PzTBRNGTC-oCLbzXUDd2lACeNkhOEi4npaHruUgh6MUe1zpwmTt7mVwQhTI6N5SqM_1NphpNBjonq33455YLdeVch-ClC-NdGgADe24Hma1uMXP5iqRqkAAAA&shmds=v1_AdeF8KgvLioLXzdCbGcM30mO5raEHL9pykXDnB4l0r3zd6_bmQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Staff Engineer, Data Engineer (.Net Core,Apache Spark)</td><td>Nagarro</td><td>India</td><td>Nagarro is a digital product engineering company that focuses on creating products, services, and experiences using AI. The role involves developing and designing overall solutions while ensuring compliance with both functional and non-functional requirements. Responsibilities include writing high-quality code, translating client requirements into technical designs, and conducting proof of concepts (POCs) to validate solutions. The position emphasizes the importance of collaboration and problem-solving in technology integration scenarios.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=9rwNNE5ztmyWTtyqAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z3HsQrCMBAAUFw7OTvdqFITEVx0EhVRsEs_oFzjNYnGXEhu6Nf4reLi8uBVn0l1bwWHAc7R-kiUazih4L8wVw0JHDlTfUhoHEGbML8WsIIb91AIs3HAES7MNtBs70RS2WldSlC2CIo3yvBbc6SeR_3kvvzoisNMKaBQt9muR5WiXU4btJgzg49wjQ-PX2afhXGhAAAA&shmds=v1_AdeF8KhuW93HTvnE5Y1bRxWajGLlkaF5gXE33-OZ4XtWSAfkBQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=9rwNNE5ztmyWTtyqAAAAAA%3D%3D</td><td>Spark Engineer</td></tr><tr><td>Data Analyst with DBA</td><td>ExamRoom.AI</td><td>India</td><td>Position Title: Data Analyst (with DBA Skills)\n",
       "\n",
       "Department: Data Science\n",
       "\n",
       "Reports To: Team Lead / Tech Lead\n",
       "\n",
       "Work Location: On-site\n",
       "\n",
       "Position Summary\n",
       "\n",
       "The Data Analyst with DBA skills will be responsible for supporting both data analytics and database performance needs across the organization. This role includes maintaining data quality, creating actionable reports, administering SQL Server databases, and optimizing query performance. The ideal candidate brings a blend of analytical thinking, database administration expertise, and business intelligence experience.\n",
       "\n",
       "Essential Duties and Responsibilities\n",
       "\n",
       "Key responsibilities include, but are not limited to:\n",
       "• Develop and maintain complex SQL queries for reporting, data analysis, and performance tuning.\n",
       "• Administer SQL Server databases, ensuring uptime, performance, and security of database systems.\n",
       "• Perform database performance analysis, capacity planning, and query optimization.\n",
       "• Monitor and maintain internal database architecture and configuration best practices.\n",
       "• Create insightful dashboards and reports using standard BI tools such as Apache Superset, Power BI, or similar.\n",
       "• Analyze large volumes of data to identify trends, anomalies, and insights that support business decisions.\n",
       "• Collaborate with engineering and business teams to understand reporting needs and deliver accurate and timely insights.\n",
       "• Perform backup and recovery tasks, patch management, and security auditing as part of DBA responsibilities.\n",
       "• Document database configurations, data models, and reporting structures for ongoing support and compliance.\n",
       "• Ensure data accuracy, consistency, and availability through proper monitoring and maintenance practices.\n",
       "\n",
       "Supervisory Responsibilities\n",
       "• None\n",
       "\n",
       "Qualifications\n",
       "\n",
       "To perform this role successfully, candidates must meet the following requirements:\n",
       "\n",
       "Education & Experience\n",
       "• Bachelor’s degree in Computer Science, Information Systems, Data Analytics, or a related field (preferred).\n",
       "• Minimum of 4 years of professional experience in data analysis and database administration.\n",
       "• Strong hands-on experience with SQL Server, including query tuning and performance optimization.\n",
       "• Deep understanding of database administration tasks, including architecture, configuration, backup/recovery, and security.\n",
       "• Experience working with BI/reporting tools such as Apache Superset, Power BI, Tableau, or similar.\n",
       "• Strong data analysis, data visualization, and data profiling skills.\n",
       "• Familiarity with database indexing, partitioning, and high availability strategies.\n",
       "• Strong communication and collaboration skills to work effectively with technical and non-technical stakeholders.\n",
       "\n",
       "Language Skills\n",
       "• Ability to understand and create technical documentation.\n",
       "• Capable of writing professional correspondence and conveying technical information clearly.\n",
       "• Strong verbal communication skills for explaining complex data issues to diverse audiences.\n",
       "\n",
       "Mathematical Skills\n",
       "• Proficient in statistics, percentages, and data-driven calculations used in reporting and analysis.\n",
       "\n",
       "Reasoning Ability\n",
       "• Strong problem-solving and analytical thinking skills.\n",
       "• Ability to diagnose and troubleshoot performance issues independently.\n",
       "• Capable of prioritizing tasks and managing time in a fast-paced, data-driven environment.\n",
       "\n",
       "Computer Skills\n",
       "• Proficient in SQL Server database management and performance tuning tools.\n",
       "• Familiar with BI/reporting platforms like Apache Superset, Power BI, or Tableau.\n",
       "• Knowledge of scripting or automation using SQL or other tools is a plus.\n",
       "\n",
       "Certificates and Licenses\n",
       "• None required (Relevant SQL Server or BI tool certifications are a plus)\n",
       "\n",
       "Travel Requirements\n",
       "• Minimal travel (up to 5%) may be required for training, meetings, or conferences.\n",
       "\n",
       "Work Environment\n",
       "• Work is performed in a standard office environment with climate control.\n",
       "• Reasonable accommodations may be made for individuals with disabilities.\n",
       "• No substantial exposure to adverse environmental conditions.\n",
       "\n",
       "Information Security & Privacy Compliance\n",
       "• Follow secure practices for managing data access and system permissions.\n",
       "• Ensure sensitive data is encrypted and protected according to internal policies.\n",
       "• Comply with data governance and regulatory standards (e.g., GDPR).\n",
       "• Support audits and maintain documentation for system changes and configurations.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=cp90yU9CIij6zP4zAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFw7OzndLJqI4KJTpCJ19AfKJYYmJcmF3oFx9M_FN7zuu-p2PQqCKZg-LPCOEqC_GtjDgyywx8UFoAJ3oin5zSWIVD5rzZzUxIISnXKUNRVvqemZLP8bOeDia0Lx4_F0aKqWabu-NcxPoqzMALHAUF4Rf0mCcPqEAAAA&shmds=v1_AdeF8KhiYGMLKnNOEpXvbzTVzdW1N4DhSRHRC-J7zYzenWw4mg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=cp90yU9CIij6zP4zAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Procurement Data Analyst</td><td>IN10 (FCRS = IN010) Novartis Healthcare Private Limited</td><td>India</td><td>We are seeking a skilled ESO Data and Digital Analyst specializing in manufacturing procurement to lead the transformation and optimization of procurement processes through digital solutions. This role involves handling large datasets, preparing presentations for senior management, and supporting operations and strategy with digitalization. The ideal candidate will collaborate with various stakeholders to enhance efficiency, compliance, and performance across the procurement function within manufacturing settings.Key Responsibilities:Digital Transformation Strategy:Develop and implement a digital strategy for manufacturing procurement.Identify opportunities for automation, data analytics, and integrated solutions to drive efficiencies.Presentation and Reporting:Prepare and present data-driven insights and reports to senior management.Create intuitive data visualizations and dashboards to support strategic decisions.Support Procurement Operations and Strategy:Assist in the digitalization of procurement processes to enhance operational efficiency.Support strategic initiatives with data analysis and digital solutions.Stakeholder Management:Collaborate with cross-functional teams including operations, finance, and IT to understand procurement needs and requirements.Engage with suppliers, vendors, and internal stakeholders to ensure alignment on objectives and requirements.Facilitate workshops and meetings to gather feedback and foster strong relationships.Requirements Analysis:Conduct detailed assessments of existing procurement processes and systems.Document functional requirements and develop comprehensive specifications for digital solutions.System Implementation:Lead the implementation of procurement software and tools, ensuring proper integration with existing systems.Oversee user training and change management initiatives to encourage adoption of new technologies.Performance Metrics:Establish KPIs and benchmarking standards to measure the effectiveness of procurement processes.Analyze procurement data to support decision-making and continuous improvement initiatives.Compliance and Risk Management:Ensure procurement practices comply with organizational policies and legal regulations.Identify potential risks in procurement processes and propose mitigation strategies.Continuous Improvement:Stay updated on market trends, technologies, and best practices in procurement and supply chain management.Propose innovative solutions to enhance procurement efficiencies and reduce costs.Essential Requirement:Education: Bachelor’s degree in Supply Chain Management, Business Administration, Information Technology, or a related field; Master’s degree preferred. Experience: Minimum of 5 years of experience in procurement or supply chain management, with a strong focus on digital transformation initiatives. Proficiency in data analysis tools and procurement software (e.g., SAP Ariba, Coupa). Strong quantitative skills to analyze large datasets and identify trends. Skills in creating intuitive data reports and dashboards. Understanding of procurement processes and financial principles. Precision in running complex calculations. Soft Skills: Excellent communication skills; ability to work collaboratively with diverse teams.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=f5MHJHbIKWjhHrvfAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLwQ7BQBAA0Lj2E5zmiES3JC5EQggq0ggfINM1aVe2O83OaPgrn4jLu73k00vMObJ9RmooKGxREdYB_VsUxnDkEoQw2ho4wJ658tRf1KqtzI0R8WkliupsarkxHKjkl3lwKX9uUmOk1qPSbTrLXmkbqtEqLyYZDHabyxWWkBfZJBtCwR1GdQIHQq-1_TU4R9f9Jpxc45Tu4ALk4e7wC_eltiyzAAAA&shmds=v1_AdeF8Khr63cRSOBQEoIZmJjIyZ_b4e1GUP05ezczOmMe293jGw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=f5MHJHbIKWjhHrvfAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Senior Data Analyst with SQL Development</td><td>Hitachi Careers</td><td>Hyderabad, Telangana, India</td><td>Location: Hyderabad\n",
       "\n",
       "Function: HV Services\n",
       "Requisition ID: 1031134\n",
       "\n",
       "Our Company\n",
       "\n",
       "We're Hitachi Vantara, the data foundation trusted by the world's innovators. Our resilient, high-performance data infrastructure means that customers - from banks to theme parks - can focus on achieving the incredible with data.\n",
       "\n",
       "If you've seen the Las Vegas Sphere, you've seen just one example of how we empower businesses to automate, optimize, innovate - and wow their customers. Right now, we're laying the foundation for our next wave of growth. We're looking for people who love being part of a diverse, global team - and who get excited about making a real-world impact with data.\n",
       "\n",
       "The Role\n",
       "\n",
       "Office Location: Hyderabad (Work from Office)\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead SQL development: Write and maintain complex queries, stored procedures, triggers, functions, and manage views for high-performance data operations.\n",
       "• Advanced Python Programming: Use Python to build ETL pipelines, automate data workflows, and conduct data wrangling and analysis.\n",
       "• Data Modeling & Architecture: Develop scalable data models and work closely with engineering to ensure alignment with database design best practices.\n",
       "• Database Optimization: Design and implement efficient indexing and partitioning strategies to improve query speed and system performance.\n",
       "• Business Insights: Translate large and complex datasets into clear, actionable insights to support strategic business decisions.\n",
       "• Cross-functional Collaboration: Partner with product, tech, and business stakeholders to gather requirements and deliver analytics solutions.\n",
       "• Mentorship & Review: Guide junior analysts and ensure adherence to coding standards, data quality, and best practices.\n",
       "• Visualization & Reporting: Create dashboards and reports using tools like Power BI, Tableau, or Python-based libraries (e.g., Plotly, Matplotlib).\n",
       "• Agile & Version Control: Operate in Agile environments with proficiency in Git, JIRA, and continuous integration for data workflows.\n",
       "About us\n",
       "\n",
       "We're a global team of innovators. Together, we harness engineering excellence and passion for insight to co-create meaningful solutions to complex challenges. We turn organizations into data-driven leaders that can a make positive impact on their industries and society. If you believe that innovation can inspire the future, this is the place to fulfil your purpose and achieve your potential.\n",
       "\n",
       "#LI-RS1\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=yRk2C3CS1DpQuMrGAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCQAwAUFz7CU6ZpbYiuOggYsEqLqJ7SdvQO7km5RK0_Sm_UV3e8pLPLNnfib1EKNAQDoxhUoO3Nwf32xUKelGQoSc2WMJFalDC2DgQhpNIF2i-c2aDbvNcNWSdGppvskb6XJhqGfOn1PqnUoeRhoBG1XqzGrOBu0VWesPGeTj-jqKCZyinliLW2KbwoIDcIWMKZ249fgGCF9fasQAAAA&shmds=v1_AdeF8Kg4_Wgfnakmou8H1JgV1Afbodlsx0K9mPd3fuYY8TpRQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=yRk2C3CS1DpQuMrGAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Senior Data Management Analyst</td><td>Experian</td><td>Hyderabad, Telangana, India</td><td>The Data Management analyst will play an instrumental role in supporting ongoing projects related to data integrity, consumer advocacy, related analytics, and accuracy. The analyst will work with stakeholders to identify opportunities for data accuracy, business process re-engineering, and provide insights to improve data management.You will be reporting to a Senior Manager.You are required to work from Hyderabad as its a Hybrid working (2 days WFO)\n",
       "\n",
       "Key Responsibilities\n",
       "• Identify, analyze, and interpret trends and patterns in core Consumer, Clarity and Rent bureau, and Ops processing data to help make business decisions.\n",
       "• Design new analytical workflows, processes, and/or optimize existing workflows with the goal to streamline processes and enable other analysts to self-service analytics.\n",
       "• Convert high level business requirements into clear technical specifications, process flow diagrams, and queries.\n",
       "• Effectively summarize, present actional insights and recommendations to the management team. Be a great story teller!\n",
       "• Consult with internal clients on data quality issues and partner with them to set up remediation and monitoring programs.\n",
       "• Engage with internal teams like data operation, data governance, compliance, audit, product development, consumer assistance center and gather requirements for business process re-engineering and improving data accuracy.\n",
       "\n",
       "About Experian\n",
       "\n",
       "Experian is a global data and technology company, powering opportunities for people and businesses around the world. We help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. We also assist millions of people to realize their financial goals and help them save time and money.\n",
       "\n",
       "We operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments.\n",
       "\n",
       "We invest in people and new advanced technologies to unlock the power of data. As a FTSE 100 Index company listed on the London Stock Exchange (EXPN), we have a team of 22,500 people across 32 countries. Our corporate headquarters are in Dublin, Ireland. Learn more at experianplc.com.\n",
       "\n",
       "Experience and Skills\n",
       "• Bachelor's degree in Data science, Engineering, Computer Science, Information Management, Statistics, related field, or equivalent experience is required.\n",
       "• 5+ years of experience in Data Analytics roles.\n",
       "• Expertise in SQL and one of the databases like SQL server, MySQL, or Aurora is required.\n",
       "• Experience analyzing large datasets and familiarity with one of analytical tools like Alteryx, Python, SAS, R, or equivalent tool is required.\n",
       "• Experience working with BI tools like Tableau, Qlik, and MS Office tools.\n",
       "• Experience with Metro2 data quality, public records, credit inquiries and consumer disputes\n",
       "• Experience with data modeling, GenAI, machine learning, and tools like Python, Spark, Athena, Hive is desirable.\n",
       "• Navigate a rather complex business environment and willingness to learn new business processes, tools and techniques is needed.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Our uniqueness is that we truly celebrate yours. Experian's culture and people are important differentiators. We take our people agenda very seriously and focus on what truly matters; DEI, work/life balance, development, authenticity, engagement, collaboration, wellness, reward & recognition, volunteering... the list goes on. Experian's strong people first approach is award winning; Great Place To Work™ in 24 countries, FORTUNE Best Companies to work and Glassdoor Best Places to Work (globally 4.4 Stars) to name a few. Check out Experian Life on social or our Careers Site to understand why.\n",
       "\n",
       "Experian is proud to be an Equal Opportunity and Affirmative Action employer. Innovation is a critical part of Experian's DNA and practices, and our diverse workforce drives our success. Everyone can succeed at Experian and bring their whole self to work, irrespective of their gender, ethnicity, religion, color, sexuality, physical ability or age. If you have a disability or special need that requires accommodation, please let us know at the earliest opportunity.\n",
       "\n",
       "Experian Careers - Creating a better tomorrow together\n",
       "\n",
       "Benefits\n",
       "\n",
       "Experian care for employee's work life balance, health, safety and wellbeing. In support of this endeavor, we offer best-in-class family well-being benefits, enhanced medical benefits and paid time off.\n",
       "\n",
       "#LI-Hybrid\n",
       "\n",
       "This is a hybrid /in-office role.\n",
       "\n",
       "Experian Careers - Creating a better tomorrow together\n",
       "\n",
       "Find out what its like to work for Experian by clicking here</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE7ZBKk9EbroJCj-gJPuJW3D9eSaHJcM7cP4ruryzV_xWRT1kzhIhhMawgMZPY3EBkfGOKvBBu7SghLmbgBhuIj4SMvDYJZ075xqrLwaWuiqTkYnTK1M7i2t_ml0wEwpolGzq7dTldivV-cpUQ7IEBiuc08ZW-xLeFFE9r9DCTfuA34BLmjMQaAAAAA&shmds=v1_AdeF8KgmPumRSh5zjXnBPATcQ4TT0jmaqf8aqTIMkmWe6402Ow&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Associate/Analyst - Data Analytics</td><td>D. E. Shaw India</td><td>Hyderabad, Telangana, India</td><td>The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNzQrCMBCE8dpH8LRnqYkIHtRToeLPVe9lk4YkJe6WbsD2kXxLq14G5psPpngvin0lwjZidroiTJNkWEONGeFXc7QygxsbEIeDDcAEZ2af3PIYcu7loLVIUl4yzrKy_NRMzvCoOzbyjUYCDq5P80ez3W1G1ZNf6VrBScE94Auu1EaESHCZWjegwbaEh0tIHgnL__wBfMXaI6wAAAA&shmds=v1_AdeF8KiGYFSm74ttc37WRtV6BAmPdDCIXraWSL4lzvQKbqdoJQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Manager - Data Analyst</td><td>BNP Paribas India Solutions</td><td>India</td><td>About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n",
       "\n",
       "About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function:\n",
       "\n",
       "The jobholder will be a part of the ISPL KPI Factory with a global mission to monitor and foster performance at group & division level.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "Data Analyst\n",
       "\n",
       "Date:\n",
       "\n",
       "7-Jan-2025\n",
       "\n",
       "Department:\n",
       "\n",
       "CAO\n",
       "\n",
       "Location:\n",
       "\n",
       "Chennai\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "Transitions and Project Office\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "Divya Sundar\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "NA\n",
       "\n",
       "(Functional)\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "NA\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "KPI and Reporting team is in charge to produce dashboards and reports to the regional/local Business, Management, Operations and Functions across the Group. The Candidate will be responsible for managing the entire lifecycle, from requirements gathering to deployment, and will work closely with cross-functional teams to ensure successful project delivery.\n",
       "\n",
       "· Monitor data inputs from stakeholders’ teams to make sure there is an alignment with the global strategy and targets.\n",
       "\n",
       "· Contribute to the implementation, maintenance and optimization of the tool dedicated to reports creation.\n",
       "\n",
       "· Business partnering with Business managers on regular basis and contribute to Adhoc requests.\n",
       "\n",
       "· Develop analyses to support strategic initiatives and decision support.\n",
       "\n",
       "· Assist in identifying process improvements in production, control procedures, and workflow organization to increase the team’s efficiency and effectiveness.\n",
       "\n",
       "· Participate in the upgrades / version releases of respective reporting tools, as well as system amendments / enhancements required to facilitate new / modified reporting requirements. In connection with this effort, assist in user acceptance testing and troubleshooting after deployment.\n",
       "\n",
       "· Help document operational process improvements in production, control procedures, and workflow organization to support team’s efficiency and effectiveness initiatives. Also, assist in integrating and streamlining the reporting under the team’s responsibility.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities:\n",
       "\n",
       "· PowerBI Dashboard Development and Maintenance\n",
       "\n",
       "· Produce key reports by various Business Units and Entities\n",
       "\n",
       "· Address any ad-hoc requests and be the backup for performance dashboards.\n",
       "\n",
       "· Contribute to projects whenever necessary, involving Power BI dashboards creation/maintenance and the feeding of the dashboards developed in Python language.\n",
       "\n",
       "· Maintenance/ Enhancements of the ETL tool used to pull, transform and load the data from source applications to our database.\n",
       "\n",
       "· Should be focused on automation initiatives to reduce manual activities.\n",
       "\n",
       "· Propose improvements related to the support activity.\n",
       "\n",
       "· Ensure requests from business users including technical and functional queries are answered immediately with qualified inputs, End user functional support and ensure Business Continuity objectives are met.\n",
       "\n",
       "· Maintains effective relationships with core and extended program team members, peers, senior stakeholders and business managers.\n",
       "\n",
       "· Ensure full compliance to organization and project specific policies, procedure and guidelines.\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "· 4-5 years of experience in reporting, business intelligence\n",
       "\n",
       "· Strong analytical skills\n",
       "\n",
       "· Expertise in PowerBI and SQL\n",
       "\n",
       "· Knowledge of Python and ETL is preferred.\n",
       "\n",
       "· Good knowledge of our environment (products, markets, practices, processes, systems, organization and people).\n",
       "\n",
       "· Ability to deliver the various phases (framing, study / design, execution, completion/closure) of large / complex projects.\n",
       "\n",
       "· Ability to develop a culture of the planning, be able to keep a direction to meet the deadlines in an autonomous manner.\n",
       "\n",
       "· Ability to take decisions linked to the project when necessary.\n",
       "\n",
       "Soft Skills\n",
       "\n",
       "· Strong analytical skills\n",
       "\n",
       "· Strong problem-solving\n",
       "\n",
       "· Comfortable operating with a strong level of autonomy, self-driven\n",
       "\n",
       "· Organized and delivery-focused, with attention to detail.\n",
       "\n",
       "· Good communication and reporting skills.\n",
       "\n",
       "· Working capacity and efficiency.\n",
       "\n",
       "· Team Player/ Networking Skills\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Resilience\n",
       "\n",
       "Critical thinking\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Analytical Ability\n",
       "\n",
       "Ability to manage a project\n",
       "\n",
       "Ability to anticipate business / strategic evolution\n",
       "\n",
       "Ability to understand, explain and support change\n",
       "\n",
       "Ability to develop and adapt a process\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 8 years\n",
       "\n",
       "Other/Specific Qualifications (if required)</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=hRqS-JTYRygERb1SAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXIMQoCMRBAUWz3CFZTWAmbiGCjlSKIgrLgAZbJGpJInAmZEdZreGIVmw_vN-9J056RMPgKLexREbaE-SX65YkdiMc6RGCCA3PIfrqJqkXW1opkE0RR02AGflgm73i0d3bySy8Rqy8Z1ffL1WI0hcJ8trt00GFNDgWOdEsIV85PTUwCif7rA_P3q7SVAAAA&shmds=v1_AdeF8Kj5RFpRe3R0U3v3dda-0pNtvGyyGb3WjomNtLQczBqGeQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=hRqS-JTYRygERb1SAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Data Analyst, WW Returns & Recommerce</td><td>Amazon</td><td>Hyderabad, Telangana, India</td><td>Description\n",
       "\n",
       "We are looking for an experienced Data Analyst to join the Reverse Logistics Services team to help unlock insights which take our team to the next level. The ideal candidate will be excited about understanding and implementing new and repeatable processes, while providing data to improve the Returns transportation business. The successful candidate will have strong data mining and modeling skills and is comfortable facilitating ideas and working from concept through to execution. They will partner with various key stakeholders to deep dive into the business challenges and data to identify insights, providing recommendations for process improvement on a globally scalable level. This role requires an individual with excellent analytical abilities as well as strong business acumen.\n",
       "\n",
       "Key job responsibilities\n",
       "• Retrieving and analyzing data using Excel, SQL, and other data management systems\n",
       "• Monitoring existing metrics, analyzing data and partnering with internal/external teams to identify process and system improvement opportunities\n",
       "• Design, develop and maintain scalable, automated, user-friendly systems, reports, or dashboards to enable stakeholders to manage the business and drive effective decisions\n",
       "• Prepare and deliver business requirements reviews to leadership teams\n",
       "• Excellent writing skills, to create artifacts easily digestible by business and tech partners.\n",
       "• Be self-driven, and show ability to deliver on ambiguous projects with incomplete data\n",
       "\n",
       "About The Team\n",
       "\n",
       "At Amazon Worldwide Returns & ReCommerce (WW R&R), we aspire to zero: zero cost of returns, zero waste, and zero defects.\n",
       "\n",
       "We are an agile, innovative organization dedicated to ‘making zero happen’ to benefit our customers, company, and environment. We are constantly innovating to create long-term value at Amazon by investing in the future and focusing on the planet, not just on the bottom line. WW R&R includes business, product, program, operational, data, and software engineering teams, who together manage the lifecycle of returned and damaged products.\n",
       "\n",
       "Basic Qualifications\n",
       "• Bachelor’s degree in Business, STEM, Operations, Finance, or related field\n",
       "• Experience in a business analyst, data analyst or statistical analysis role\n",
       "• Experience managing and influencing key metrics\n",
       "• Ability to analyze associate and customer inputs to influence internal and external partners\n",
       "• Experience with MS Outlook, Excel, and Word\n",
       "• Strong written communications skills\n",
       "• Experience with data visualization tools like Tableau and/or experience with SQL & QuickSight\n",
       "\n",
       "Preferred Qualifications\n",
       "• Experience in Retail, Transportation or Operations\n",
       "• Advanced SQL skills\n",
       "• Experience with Python, R or other scripting languages\n",
       "• Experience communicating across all levels of management, peers, and partners\n",
       "\n",
       "Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.\n",
       "\n",
       "Company - Amazon Dev Center India - Hyderabad\n",
       "\n",
       "Job ID: A2920316</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=OwQO9R-Bmx5ieTRKAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMuwrCQBAAsc0nWG2jhcScCDZqExB8lCKkDHuX5RK57IbbE4yf45cam2Gamew7y44nTAglYxg15VBVcKf0iqywnMxJ31N0BGu4iQUljK4FYTiL-EDzQ5vSoHtjVEPhNWHqXDFFRpisvM1TrP5Ra4uRhoCJ6u1u8y4G9qtF2eNnenUMl7GhiBabHB4UkD0y5nDlpsMfKUcboKUAAAA&shmds=v1_AdeF8Kj0MQM5XQhNHaUCBdrqmD3v_Ee4wDVD7upBPZI5IJKRFQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=OwQO9R-Bmx5ieTRKAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Sr. Data Analyst</td><td>iCIMS Talent Acquisition</td><td>Rai Durg, Telangana, India</td><td>Job Overview\n",
       "\n",
       "The Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n",
       "\n",
       "About Us\n",
       "\n",
       "When you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n",
       "\n",
       "Responsibilities\n",
       "• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n",
       "• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n",
       "• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n",
       "• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n",
       "• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n",
       "• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n",
       "• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n",
       "• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n",
       "\n",
       "Additional Job Responsibilities: \n",
       "• Produce and adapt data visualizations in response to business requests for internal and external use\n",
       "• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n",
       "• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n",
       "• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n",
       "\n",
       "Qualifications\n",
       "• 5-10 years professional experience working in an analytics capacity\n",
       "• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n",
       "• Strong data analytics and visualization skills\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n",
       "• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n",
       "\n",
       "Preferred\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "\n",
       "EEO Statement\n",
       "\n",
       "iCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n",
       "\n",
       "We are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n",
       "\n",
       "Compensation and Benefits\n",
       "\n",
       "Competitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU63CVITEXTQqViQCi62e7nWkJ7ES82dUH_Gb1WXN77sM8sWdTJQoiIUjOEtCis4xw7EYeoHiAynGH1w88OgOsreWpFgvCgq9aaPDxvZdXGy99jJn1YGTG4MqK7dbNeTGdkvd3SsLjU0GBwrFP3zRUJKv50YrkhQvpLPoXEB2SNjDhXfCL--pAUHoQAAAA&shmds=v1_AdeF8KitaSipmTLybCgiXZCrA1aS7QWwHWuaNBynowT2b-63_w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Data Science Analyst</td><td>IN10 (FCRS = IN010) Novartis Healthcare Private Limited</td><td>India</td><td>About the Role:Job Description Summary: The Insights and Decision Science (IDS) team is dedicated to enabling improved decision-making at Novartis by leveraging data and advanced analytics capabilities to generate actionable insights that drive business growth. We collaborate closely with the US business, bringing in-sights and challenging ideas to empower smarter, data-driven decision-making. Launch and BD&L Insights & Analytics, is crucial in orchestrating the strategic launch of new products within the therapeutic area, ensuring that each launch is informed by comprehensive market research and analytics. This role will be responsible for creating and maintaining various reports and deliverables. They will be involved in delivering analytical solutions for business problems related to Launch and BD&L products. They will also help in using analytical, statistical and Data Science skills to answer ad-hoc business questions and drive data-driven decisions. They will also be collaborating with other teams to facilitate various downstream processes.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:· Deliver analytical solutions as an individual contributor for various projects related to quantification of patient journey, informing & validating forecast assumptions and other ad-hoc business questions related to launch products.· Support exploratory research to identify new areas of application of patient / provider analytics in providing enhanced decision-making support.· Development and automation of project codes and deliverables.· Participate in various knowledge sharing sessions that enable growth and improves quality of deliverables across the function.· Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes.Minimum Requirements:· Minimum 1 year of experience in the pharmaceutical or healthcare industry.· Strong analytical and problem-solving skills to extract insights from complex APLD data sets and identify patterns and trends.· Excellent communication and interpersonal skills, with the ability to influence and collaborate with cross-functional teams.· Proficiency in data analytics tools, platforms and languages like SQL and Python, with the ability to translate data into actionable insights.· Strong problem-solving skills and a strategic mindset, with the ability to anticipate challenges and develop innovative solutions.Education:Bachelor's degree in related field is required; Master of Science and/or MBA strongly preferred.Preferred Skills:· Ability to balance operational execution with high-level strategic thinking, supporting both day-to-day performance and long-term business goals· Ability to work collaboratively with cross-functional teams, including sales, Data Operations, and product development and drive strategic initiatives· Proficiency in insight and hypothesis generation, data science, and primary and secondary research methodologies· Ability to thrive in a fast-paced, dynamic environment and adapt to changing business needs and priorities.Why Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=E8qUCATb85C0rqR8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLzQqCQBAAYLr6CJ3m2A-pBV2KoCgqIyTyAWRcB91Yd8QZxN6ph6wu3-0LPqNgfkJFyIwlbwgOHt1bFBZw4wKEsDM1sIcLc-VovK1VW9lEkYgLK1FUa0LDTcSeCh6iFxfyJ5caO2odKuWrdTyEra9m-yRdxjA5H58Z7CBJ42U8hZR77NQKXAmd1ubX4NHZ_jfhbhurVIL1kPjS4hdRPmF7rwAAAA&shmds=v1_AdeF8KitnZKbCILU3LI82mL07zTe7Dv_v83dUFqGICYpHkWx8w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=E8qUCATb85C0rqR8AAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)</td><td>Dupont</td><td>Hyderabad, Telangana, India</td><td>At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "The Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n",
       "\n",
       "Key Areas of Expertise and Responsibilities:\n",
       "\n",
       "1. Visual Basic for Applications (VBA)\n",
       "• Responsibilities:\n",
       "• Develop and maintain complex VBA applications to automate repetitive tasks.\n",
       "• Incorporate SAP Scripting within VBA to optimize business processes.\n",
       "• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n",
       "• Criteria:\n",
       "• Advanced proficiency in VBA programming.\n",
       "• Demonstrated experience with SAP interfaces and scripting.\n",
       "• Ability to write modular, efficient, and maintainable code.\n",
       "• Knowledge of Excel object model and its functionalities.\n",
       "\n",
       "2. Power Query\n",
       "• Responsibilities:\n",
       "• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n",
       "• Develop and maintain data models in Excel to streamline data preparation.\n",
       "• Create and optimize Power Query scripts for efficient data processing.\n",
       "• Criteria:\n",
       "• Intermediate experience with Power Query including M language for data transformation.\n",
       "• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n",
       "• Ability to perform data cleansing and manipulation through Power Query.\n",
       "\n",
       "3. Power BI\n",
       "• Responsibilities:\n",
       "• Create interactive, user-friendly dashboards and reports using Power BI.\n",
       "• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n",
       "• Optimize Power BI reports for performance and usability.\n",
       "• Criteria:\n",
       "• Intermediate knowledge of Power BI Desktop and Power BI Service.\n",
       "• Ability to create DAX measures and calculated columns for enhanced analytics.\n",
       "• Familiarity with data visualization best practices and techniques.\n",
       "\n",
       "4. Python\n",
       "• Responsibilities:\n",
       "• Develop Python scripts to automate data manipulation and Excel-related tasks.\n",
       "• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n",
       "• Collaborate with the data team to integrate Python solutions with existing tools.\n",
       "• Criteria:\n",
       "• Intermediate proficiency in Python, especially in data manipulation and automation.\n",
       "• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n",
       "• Understanding of APIs and ability to retrieve data programmatically.\n",
       "\n",
       "Qualifications:\n",
       "• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n",
       "• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills and the ability to work collaboratively with diverse teams.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with SQL and relational databases for data querying and data management.\n",
       "• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n",
       "• Knowledge of machine learning principles is an advantage.\n",
       "• Understanding of data warehousing concepts and methodologies.\n",
       "\n",
       "Join our Talent Community to stay connected with us!\n",
       "\n",
       "On May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n",
       "\n",
       "(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n",
       "\n",
       "DuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n",
       "\n",
       "DuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWLMU7DQBBFRZsjUP0GiQRjI5Q0oUoUAUkFEqKNxvbEu2iZWe1OhPeSOROmoPl6T09_drmauR0ZYSMUSjY8eyHpGLef202FN_3hhPczp_Iv2_1ExZzKHMs7FKaUoSfwGDl5nr5z3OOgLfKUOgcVvKgOga-fnFnM66bJOdRDNjLf1Z1-Nyrc6th8aZv_5pgdJY6BjI-Pq4exjjIsbnbnqGLwgtfSc6KW-gofHEgGEqqwl97TL1wZQbrRAAAA&shmds=v1_AdeF8KjE_yEZOuigr5UIe33mQKphFiyPOSsPa5mAJd61oGdJsA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D</td><td>Data Analyst</td></tr><tr><td>AI/ML Engineer – XR Software (All levels))</td><td>Qualcomm</td><td>Bengaluru, Karnataka, India</td><td>Company:\n",
       "Qualcomm India Private Limited\n",
       "\n",
       "Job Area:\n",
       "Engineering Group, Engineering Group > Systems Engineering\n",
       "\n",
       "General Summary:\n",
       "\n",
       "We are seeking a passionate and skilled AI/ML Engineer to join our cutting-edge Extended Reality (XR) Software team. In this role, you will work on next-generation XR products that blend the physical and digital worlds, leveraging artificial intelligence and machine learning to create immersive, intelligent, and responsive experiences.\n",
       "\n",
       "You will collaborate with cross-functional teams of researchers, engineers, and designers to build real-time AI/ML software optimized for XR platforms. A strong background in C++ or embedded firmware development is essential, as you will be working close to hardware and performance-critical systems.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and optimize AI/ML models for XR applications such as computer vision, sensor fusion, gesture recognition, and spatial understanding.\n",
       "• Implement real-time inference pipelines on embedded or edge devices.\n",
       "• Collaborate with firmware and hardware teams to integrate ML models into XR systems.\n",
       "• Analyze system performance and optimize for latency, power, and memory.\n",
       "• Stay up to date with the latest research and trends in AI/ML and XR technologies.\n",
       "• Contribute to the full lifecycle of product development—from prototyping to production.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Electrical Engineering, or a related field.\n",
       "• 1–10 years of industry experience in AI/ML engineering or embedded systems.\n",
       "• Proficiency in C++ and/or embedded firmware development.\n",
       "• Solid understanding of machine learning fundamentals and experience with frameworks like TensorFlow, PyTorch, or ONNX.\n",
       "• Experience with deploying ML models on edge devices\n",
       "• Familiarity with XR technologies (AR/VR/MR), sensor data processing, or 3D spatial computing.\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\n",
       "OR\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\n",
       "OR\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n",
       "\n",
       "Applicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n",
       "\n",
       "Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n",
       "\n",
       "To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n",
       "\n",
       "If you would like more information about this role, please contact Qualcomm Careers.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuw4BURCA4Wj3EVTTuYQ9ItFQISLrUqDRyewaZ5cxsznnLErv4DG8lSdB81df_uhdi6bjxKxXMBNbCJGDz_MF-y3s9BTu6AiaY2ZguhH7Vgu6sNAUPKHLclCBuaplqo_yEEo_NMZ7jq0PGIoszvRqVCjVhzlr6v85-Pz3LBkDHfqD3iMuxbYbmwr5h69QCExILHLlqg4s0QkGvGAHEjkW-AWsJyrJrgAAAA&shmds=v1_AdeF8Kjy55dYa7U3WoErm860Lebff2UCLLCE78OUgv0RP_zcbw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>Lead AI Engineer</td><td>Keywords Studios</td><td>Pune, Maharashtra, India</td><td>About the team:You will be a part of the AI team, which is responsible for developing AI features at Helpshift.The AI team has built product features like User Intent Detection, AI-Powered Answers,Agent CoPilot features, etc by building our own ML models and leveraging generative AI.The team consists of Backend and Frontend Developers, Full-stack Developers, MLEngineers and Data Scientists.About the role:We are looking for a seasoned engineer with a passion for Artificial Intelligence and a knackfor leadership. In this role, you'll be instrumental in shaping our AI products, leading a groupof engineers within the team, and driving the successful delivery of innovative solutions.You'll blend deep technical expertise in AI with strong leadership skills, guiding a team offull-stack, backend, and frontend developers. You'll be responsible for the end-to-enddelivery of AI-powered features and products, from conceptualization and design todeployment and optimization.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKOwrCQBAAUGxzBKvpBIlZEWy0shCJHxA8QJhkh92VOBN2Jqj38MBq86pXfCbF7EzoYVfDnkNiogwLOEoLSpi7CMJwEAk9TbfRbNCNc6p9FdTQUld18nDC1MrL3aXVP41GzDT0aNSs1stXNXCYlyd6PyV7hZuNPolCYriOTCVc8PdRo2UsoWaf8AtFRB7ClwAAAA&shmds=v1_AdeF8KiNR20bIkNkNAivhghX5MFAQu0T9NqGzMXN5rQ3M58NCQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>Senior Application Engineer for Artificial Intelligence (AI)</td><td>MathWorks</td><td>New Delhi, Delhi, India</td><td>Job Summary\n",
       "\n",
       "MathWorks has a hybrid work model that enables staff members to split their time between office and home. The hybrid model provides the advantage of having both in-person time with colleagues and flexible at-home life optimizations. Learn More: https://www.mathworks.com/company/jobs/resources/applying-and-interviewing.html#onboarding.\n",
       "\n",
       "As a Senior Application Engineer for Artificial Intelligence (AI) you will partner with our most innovative customers across many industries to establish MATLAB as a platform for these application areas by\n",
       "• Developing machine learning, and deep learning algorithms in MATLAB\n",
       "• Integrating algorithms developed in MATLAB into full-fledged applications\n",
       "• Deploying applications developed in MATLAB to enterprise-wide systems\n",
       "\n",
       "You will work in a pre-sales manner with our customers to first identify and understand their technical and business challenges and then develop compelling demonstrations of solutions that help them appreciate how the adoption of MathWorks products add value towards enhancing their workflows.\n",
       "\n",
       "Using your technical expertise as well as your excellent interpersonal, communication, and presentation skills, you will engage customers and prospects in seminars, conference calls, meetings, and through product evaluations to develop a shared vision for success.\n",
       "\n",
       "MathWorks nurtures growth, appreciates diversity, encourages initiative, values teamwork, shares success, and rewards excellence.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Provide technical pre-sales support and guidance to the India sales organization, prospects, and customers in the domains of statistics, machine learning, deep learning and application deployment by:\n",
       "• Preparing and delivering presentations, demonstrations, and application examples in customer meetings, seminars, and other public events\n",
       "• Managing product evaluations and developing adoption plans that assist customers in adopting MathWorks products\n",
       "• Partnering with sales representatives to help develop account and territory level selling strategies\n",
       "• Identify new trends and application areas and provide feedback to development and marketing teams. Collaborate with the worldwide team on developing compelling messages and demonstrations.\n",
       "• Advocate for the future direction of MathWorks products based on customer interactions.\n",
       "• Establish rapport and credibility with our customers across multiple hierarchy levels to build champion users and supporters of our solutions.\n",
       "\n",
       "Qualifications\n",
       "• A bachelor's degree and 6 years of professional work experience (or a master's degree and 3 years of professional work experience, or a PhD degree, or equivalent experience) is required.\n",
       "\n",
       "Additional Qualifications\n",
       "• Strong knowledge of mathematical modelling, statistics, optimization, machine learning, and deep learning algorithms & frameworks\n",
       "• Experience programming in MATLAB\n",
       "• Experience programming in Python and C/C++ is a plus\n",
       "• Working knowledge of cloud (AWS / Azure) is a plus\n",
       "• Working knowledge of CI/CD workflows is a plus\n",
       "• Exposure to Medical and healthcare applications is a plus\n",
       "• Excellent verbal and written communication and presentation skills\n",
       "• Highly motivated toward working directly with customers.\n",
       "• This position is based in New Delhi with travel generally throughout India specific to various customer visits and seminars. Travel time can be expected to amount to approximately 25-40% with trips generally no longer than five days. Though far less frequent, some international travel is expected.\n",
       "• Willingness to learn new tools and skills as required</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=lUSHTrfCo_VquIa6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWMsQrCMBRFce0nOL3BQUVbEVx0KihSQZcOjiWNr8nT-F5IAvbT_DzbweWe4XBu9p1kdY1MEqD03pFWiYThxIYYMUA3ipCoI03KQcUJnSODrBHmZbWANVykhYgqaAtDeRYxDqcHm5KP-6KI0eUmpuFW51rehTC20hdPaeM4TbQqoHcqYbPdbfrcs1nOrirZu4RXBGK44QeO6Cyt_qj4QeoHPSLVD7sAAAA&shmds=v1_AdeF8Kj9VxHI4jOIUQMZn1bZZhFKP9HNe6unw0mM_MEcLoasnA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=lUSHTrfCo_VquIa6AAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>AI Engineer for software development</td><td>EcoSmart Energy System LLC</td><td>Karnataka, India</td><td>Develops cloud / on prem applications and systems that utilize AI tools , cloud AI services. The candidate will be able to apply Gen AI models as part of the solution. Could also include deep learning , neural networks and chatbots , image processing. Mu\n",
       "\n",
       "As an AI / ML engineer , you will develop applications and systems utilizing AI tools , cloud AI services and Gen AI models. Your role involves creating cloud or on prem application pipelines with production ready quality, incorporating deep learning, neural networks , chatbots, and image processing.\n",
       "\n",
       "Experience – Minimum 5 years post qualification experience in a similar role.\n",
       "\n",
       "Qualification – Engineering degree in computer science or equivalent.\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: ₹800,000.00 - ₹1,000,000.00 per year</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=TdqlU35Qrt4MqZQAAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNMQ6CUAyA4bhyBKfORsGYuMhkDDEoGwcgBcoDfbTktVG8kqcUl3_8_ui7itJzDhm7gYkCdBJApbM3BoKWXuRlGokNdnCTGpQwND0Iw1XEeVqnvdmkpyRR9bFTQxuauJExEaZa5uQhtf5Tab-Ik0ej6nDcz_HEbhNnjZQjBlv-FNwHyo8ajVAUFxgY7hgYDZ-4hZzbAX-qs52ErQAAAA&shmds=v1_AdeF8Kjv2AX1CL20Z8T1f8eLB-FGavuJuST1yq4vZ26TPsMPDg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=TdqlU35Qrt4MqZQAAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>AI Application Engineer</td><td>Avaya</td><td>Pune, Maharashtra, India</td><td>About Avaya\n",
       "\n",
       "Avaya is an enterprise software leader that helps the world’s largest organizations and government agencies forge unbreakable connections.\n",
       "\n",
       "The Avaya Infinity™ platform unifies fragmented customer experiences, connecting the channels, insights, technologies, and workflows that together create enduring customer and employee relationships.\n",
       "\n",
       "We believe success is built through strong connections – with each other, with our work, and with our mission. At Avaya, you'll find a community that values your contributions and supports your growth every step of the way.\n",
       "\n",
       "Learn more at https://www.avaya.com.\n",
       "\n",
       "Job Information\n",
       "\n",
       "Job Description\n",
       "\n",
       "We are seeking a highly experienced and visionary Lead AI Application Engineer to join our core AI team. This pivotal role will be instrumental in designing, developing, and deploying advanced Generative AI and Agentic AI applications. You will be a key contributor to our strategic initiatives, working on complex problems that require deep technical expertise and a forward-thinking approach. This is an exceptional opportunity for an individual who thrives on building sophisticated, autonomous AI systems and wants to make a significant impact.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "● Provide thought leadership around applying AI and ML technology and help define technical direction and architecture with engineering team members.\n",
       "\n",
       "● Leading the exploration and application of Large Language Models, Agentic AI and Generative AI, venturing into new areas within these fields.\n",
       "\n",
       "● Architect and implement robust, scalable, and efficient AI systems leveraging a variety of AI/ML, data science, deep learning, and agentic AI frameworks.\n",
       "\n",
       "● Drive the integration of complex Model Context Protocol (MCP) and agent-to-agent (A2A) communication protocols, ensuring seamless interaction and optimal performance.\n",
       "\n",
       "● Design, build, and deploy autonomous agents capable of complex decision-making and task execution in dynamic environments.\n",
       "\n",
       "● Design and implement scalable and efficient AI/ML systems, architectures, and pipelines that can handle large volumes of data and deliver accurate and timely results.\n",
       "\n",
       "● Develop and maintain real-world NLP features using the latest techniques for call routing, intent identification, named entity recognition, information retrieval, summarization etc.\n",
       "\n",
       "● Conduct extensive research and experimentation with emerging AI technologies, specifically in Generative AI and Agentic AI, to identify and implement innovative solutions.\n",
       "\n",
       "● Collaborate closely with cross-functional teams including data scientists, software engineers and product managers to translate business requirements into technical solutions.\n",
       "\n",
       "● Regularly self-educate while mentoring and provide technical leadership to other engineers, fostering a culture of continuous learning and excellence.\n",
       "\n",
       "Requirements\n",
       "\n",
       "Qualifications\n",
       "\n",
       "● 10+ years of progressive hands-on technical experience in Artificial Intelligence, Machine Learning, Data Science, and Deep Learning.\n",
       "\n",
       "● Demonstrated experience (3+ years) actively working on and deploying Generative AI and Agentic AI applications.\n",
       "\n",
       "● Extensive hands-on experience with multiple AI and Agentic AI frameworks, such as (but not limited to):\n",
       "\n",
       "○ Generative AI: TensorFlow, PyTorch, Hugging Face Transformers, OpenAI API, LangChain, LlamaIndex, etc.\n",
       "\n",
       "○ Agentic AI: LangChain Agents, Auto-GPT, CrewAI, multi-agent simulation frameworks, etc.\n",
       "\n",
       "● Experienced with leading LLM models such as OpenAI's GPT series, Google's Gemini, Meta's Llama, and Anthropic's Claude, including fine-tuning and deployment for specific applications.\n",
       "\n",
       "● Deep understanding and practical experience with Multi-Component Platforms (MCP) and Agent-to-Agent (A2A) communication patterns.\n",
       "\n",
       "● Experience with cloud platforms (Azure, GCP, AWS) and MLOps practices.\n",
       "\n",
       "● 5 + years of experience writing and deploying production quality models in languages such as Python\n",
       "\n",
       "● Master's or Ph.D. in Computer Science, Artificial Intelligence, Machine Learning, or a related quantitative field.\n",
       "\n",
       "Knowledge, Skills, And Abilities\n",
       "\n",
       "● Expertise in developing on a Azure cloud platform is a big plus\n",
       "\n",
       "● Contributions to open-source AI projects, particularly in Generative AI or Agentic AI\n",
       "\n",
       "● Familiarity with reinforcement learning and multi-agent reinforcement learning.\n",
       "\n",
       "● Outstanding written and verbal communication skills\n",
       "\n",
       "Education\n",
       "\n",
       "Bachelor degree or equivalent experience\n",
       "Advance Degree preferred\n",
       "\n",
       "Footer\n",
       "\n",
       "Avaya is an Equal Opportunity employer and a U.S. Federal Contractor. Our commitment to equality is a core value of Avaya. All qualified applicants and employees receive equal treatment without consideration for race, religion, sex, age, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other protected characteristic. In general, positions at Avaya require the ability to communicate and use office technology effectively. Physical requirements may vary by assigned work location. This job brief/description is subject to change. Nothing in this job description restricts Avaya right to alter the duties and responsibilities of this position at any time for any reason. You may also review the Avaya Global Privacy Policy (accessible at https://www.avaya.com/en/privacy/policy/) and applicable Privacy Statement relevant to this job posting (accessible at https://www.avaya.com/en/documents/info-applicants.pdf).</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=u5m3655fcuIxzgcnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWLQQrCMBAA8doneFqvUhMRvOipB5EKgj8o27gkkbgbslHqL3yy9TKXmWm-i8Z0PXQ5p-iwRmE4sY9MVGADFxlBCYsLMIuziE-0PIZasx6sVU3Ga50vZ5w8rTCNMtmHjPrHoAEL5YSVht1-O5nMfr3q3vhBiAy3F1MLV5wj1FALttDzPeIPThguDZMAAAA&shmds=v1_AdeF8KiZGolIScAKLYhEZ62BinbpGuk1TEcwUrX1P70a1mJ8PA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=u5m3655fcuIxzgcnAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>AI Architect</td><td>Adobe</td><td>Bengaluru, Karnataka, India</td><td>JOB LEVEL\n",
       "P50\n",
       "\n",
       "EMPLOYEE ROLE\n",
       "Individual Contributor\n",
       "\n",
       "Experience: 12-16 years\n",
       "\n",
       "The candidate should have proven expertise in building scalable platforms that are customer facing and have expertise in evangelizing the platform with customers and with internal stakeholders Expert level knowledge of Cloud Computing including aspects of VPC Network Design, Shared Responsibility Matrix, Cloud databases, No SQL Databases, Data Pipelines on the cloud, VM and VM orchestration, Serverless frameworks. This should be across all 3 major cloud providers (AWS, Azure, GCP), preferably at least in 2 of the 3 Public Clouds Expert level knowledge in Data Ingestion paradigms & use of different types of databases like OLTP, OLAP for specific purposes Hands-on experience with Apache Spark, Apache Flink, Kafka, Kinesis, Pub/Sub, Databricks, Apache Airflow, Apache Iceberg, and Presto. Expertise in designing ML Pipelines for experiment management, model management, feature management, model retraining, A/B testing of models and design of APIs for model inferencing at scale. Proven expertise with Kube Flow, SageMaker/Vertex AI/Azure AI. SME in LLM Serving paradigms, deep knowledge of GPU architectures, distributed training and serving of large language models. Expertise in Model and Data parallel training, expertise with frameworks like DeepSpeed and service frameworks like vLLM etc. Proven expertise in Model finetuning and model optimization techniques to achieve better latencies, better accuracies in results. Be an expert in reducing training and resource requirements of finetuning of LLM and LVM models. Have a wide knowledge of different LLM models and have an opinion on aspects of applicability of each model based the usecases. Proven expertise of having worked on specific customer usecases and having seen delivery of a solution end to end from engineering to production. Proven expertise in DevOps and LLMOps, knowledge of Kubernetes, Docker and container orchestration, and deep knowledge of LLM Orchestration frameworks like Flowise, Langflow, Langgraph.\n",
       "\n",
       "Skill Matrix\n",
       "\n",
       "LLM: Hugging Face OSS LLMs, GPT, Gemini, Claude, Mixtral, Llama\n",
       "\n",
       "LLM Ops: ML Flow, Langchain, Langraph, LangFlow, Flowise, LLamaIndex, SageMaker, AWS Bedrock, Vertex AI, Azure AI\n",
       "\n",
       "Dev Ops: Kubernetes, Docker, FluentD, Kibana, Grafana, Prometheus\n",
       "\n",
       "Databases/Datawarehouse: DynamoDB, Cosmos, MongoDB, RDS, MySQL, PostGreSQL, Aurora, Spanner, Google BigQuery.\n",
       "\n",
       "Cloud Expertise: AWS/Azure/GCP\n",
       "\n",
       "Cloud Certifications: AWS Professional Solution Architect, AWS Machine Learning Specialty, Azure Solutions Architect Expert\n",
       "\n",
       "Proficient in Python, SQL, Javascript\n",
       "\n",
       "Internal Opportunities\n",
       "\n",
       "Creativity, curiosity, and constant learning are celebrated aspects of your career growth journey. We’re glad that you’re pursuing a new opportunity at Adobe!\n",
       "\n",
       "Put your best foot forward:\n",
       "\n",
       "1. Update your Resume/CV and Workday profile – don’t forget to include your uniquely ‘Adobe’ experiences and volunteer work.\n",
       "\n",
       "2. Visit the Internal Mobility page on Inside Adobe to learn more about the process and set up a job alert for roles you’re interested in.\n",
       "\n",
       "3. Check out these tips to help you prep for interviews.\n",
       "\n",
       "4. If you are applying for a role outside of your current country, ensure you review the International Resources for Relocating Employees on Inside Adobe, including the impacts to your Benefits, AIP, Equity & Payroll.\n",
       "\n",
       "Once you apply for a role via Workday, the Talent Team will reach out to you within 2 weeks. If you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth.\n",
       "\n",
       "At Adobe, you will be immersed in an exceptional work environment that is recognized around the world. You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely. If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.\n",
       "\n",
       "Adobe is proud to be an Equal Employment Opportunity employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.\n",
       "\n",
       "Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=0ETwn0QL83xo_61WAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIMQ7CMAwAQLH2CUyWYEKlQUgsMJUFFR5ROamVBoJdxa7UJ_BsYLnhqs-q2rQdtCWMySgY7OEuHpTwNyAMN5GYaX0ZzSY9O6eam6iGlkIT5O2EycvinuL1T68jFpoyGvXH02FpJo67bTuIJ0gMV-KIeS5zDQ8sjIYvrKHjIeEXtmlRP4sAAAA&shmds=v1_AdeF8KhyrvoqNhvJ0pdjtjlp2_BOOAjQdNQdK4v6U8P1yoScUQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=0ETwn0QL83xo_61WAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>Sr Specialist Data/AI Engineering</td><td>AT&T</td><td>Bengaluru, Karnataka, India</td><td>Job Description:\n",
       "\n",
       "Job Title: Data/AI Engineer – GenAI & Agentic AI Integration (Azure)\n",
       "Location: Bangalore, India\n",
       "Job Type: Full-Time\n",
       "About the Role\n",
       "We are seeking a highly skilled Data/AI Engineer to join our dynamic team, specializing in integrating cutting-edge Generative AI (GenAI) and Agentic AI solutions within the Azure cloud environment. The ideal candidate will have a strong background in Python, data engineering, and AI model integration, with hands-on experience working on Databricks, Snowflake, Azure Storage, and Palantir platforms. You will play a crucial role in designing, developing, and deploying scalable data and AI pipelines that power next-generation intelligent applications.\n",
       "Key Responsibilities\n",
       "Design, develop, and maintain robust data pipelines and AI integration solutions using Python on Azure Databricks.\n",
       "Integrate Generative AI and Agentic AI models into existing and new workflows to drive business innovation and automation.\n",
       "Collaborate with data scientists, AI researchers, software engineers, and product teams to deliver scalable and efficient AI-powered solutions.\n",
       "Orchestrate data movement and transformation across Azure-native services including Azure Databricks, Azure Storage (Blob, Data Lake), and Snowflake, ensuring data quality, security, and compliance.\n",
       "Integrate enterprise data using Palantir Foundry and leverage Azure services for end-to-end solutions.\n",
       "Develop and implement APIs and services to facilitate seamless AI model deployment and integration.\n",
       "Optimize data workflows for performance and scalability within Azure.\n",
       "Monitor, troubleshoot, and resolve issues related to data and AI pipeline performance.\n",
       "Document architecture, designs, and processes for knowledge sharing and operational excellence.\n",
       "Stay current with advances in GenAI, Agentic AI, Azure data engineering best practices, and cloud technologies.\n",
       "Required Qualifications\n",
       "Bachelor’s or Master’s degree in Computer Science, Engineering, Data Science, or a related field (or equivalent practical experience).\n",
       "5+ years of professional experience in data engineering or AI engineering roles.\n",
       "Strong proficiency in Python for data processing, automation, and AI model integration.\n",
       "Hands-on experience with Azure Databricks and Spark for large-scale data engineering.\n",
       "Proficiency in working with Snowflake for cloud data warehousing.\n",
       "In-depth experience with Azure Storage solutions (Blob, Data Lake) for data ingestion and management.\n",
       "Familiarity with Palantir Foundry or similar enterprise data integration platforms.\n",
       "Demonstrated experience integrating and deploying GenAI or Agentic AI models in production environments.\n",
       "Knowledge of API development and integration for AI and data services.\n",
       "Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.\n",
       "Excellent communication and documentation skills.\n",
       "Preferred Qualifications\n",
       "Experience with Azure Machine Learning, Azure Synapse Analytics, and other Azure AI/data services.\n",
       "Experience with MLOps, model monitoring, and automated deployment pipelines in Azure.\n",
       "Exposure to data governance, privacy, and security best practices.\n",
       "Experience with visualization tools and dashboard development.\n",
       "Knowledge of advanced AI model architectures, including LLMs and agent-based systems.\n",
       "\n",
       "#DataEngineer\n",
       "\n",
       "Weekly Hours:\n",
       "40\n",
       "\n",
       "Time Type:\n",
       "Regular\n",
       "\n",
       "Location:\n",
       "Bangalore, Karnataka, India\n",
       "\n",
       "It is the policy of AT&T to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, AT&T will provide reasonable accommodations for qualified individuals with disabilities. AT&T is a fair chance employer and does not initiate a background check until an offer is made.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=VBue8OZ4hjMI9hO6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU4HgoPURgRBdKooUh3bvVzrkUbjXcil0I_xY9XlrS_7zLJ9HaEO1Dv0ThOcMaEpK7iwdUwUHVtYw006UMLYDyAMVxHraX4cUgp6MEbVF1YTJtcXvbyNMHUymad0-qfVASMFj4na7W4zFYHtalE2ywYcw4nYoh_jmMMdI__6F-ZQ8cPhFyuv7eufAAAA&shmds=v1_AdeF8KiVbtMP89pgbN5skTi7M9LBmafXkZhUjBMaB6ltsKhUhA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=VBue8OZ4hjMI9hO6AAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>Senior AI Engineer I</td><td>MicroStrategy</td><td>Mumbai, Maharashtra, India</td><td>Company Description\n",
       "\n",
       "Strategy (Nasdaq: MSTR) is at the forefront of transforming organizations into intelligent enterprises through data-driven innovation. We don't just follow trends—we set them and drive change. As a market leader in enterprise analytics and mobility software, we've pioneered BI and analytics space, empowering people to make better decisions and revolutionizing how businesses operate.\n",
       "\n",
       "But that's not all. Strategy is also leading to a groundbreaking shift in how companies approach their treasury reserve strategy, boldly adopting Bitcoin as a key asset. This visionary move is reshaping the financial landscape and solidifying our position as a forward-thinking, innovative force in the market. Four years after adopting the Bitcoin Standard, Strategy's stock has outperformed every company in S&P 500.\n",
       "\n",
       "Our people are the core of our success. At Strategy, you'll join a team of smart, creative minds working on dynamic projects with cutting-edge technologies. We thrive on curiosity, innovation, and a relentless pursuit of excellence.\n",
       "\n",
       "Our corporate values—bold, agile, engaged, impactful, and united—are the foundation of our culture. As we lead the charge into the new era of AI and financial innovation, we foster an environment where every employee's contributions are recognized and valued.\n",
       "\n",
       "Join us and be part of an organization that lives and breathes innovation every day. At Strategy, you're not just another employee; you're a crucial part of a mission to push the boundaries of analytics and redefine financial investment.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Job Location: Pune. Full time 5 days a week from Strategy Pune office.\n",
       "\n",
       "Senior AI Engineer I\n",
       "\n",
       "We are looking for a highly skilled AI Engineer to join our team and contribute to the development of AI-powered solutions. The ideal candidate will have expertise in Python programming, REST API development, cloud infrastructure, and AI model integration. This role requires strong problem-solving skills, a deep understanding of AI technologies, and the ability to optimize and deploy AI applications efficiently.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Develop, maintain, and optimize AI-driven applications using Python.\n",
       "• Implement object-oriented programming, module creation, and unit testing to ensure code quality and maintainability.\n",
       "• Design and build RESTful APIs with FastAPI for seamless integration of AI services.\n",
       "• Work with various Large Language Models (LLMs), including OpenAI and Azure OpenAI, to enhance AI-driven functionalities.\n",
       "• Interface with relational and vector databases for efficient AI data retrieval and storage.\n",
       "• Implement text embedding techniques to improve AI-powered natural language processing tasks.\n",
       "• Deploy and manage AI applications on cloud platforms such as AWS, Azure, and GCP.\n",
       "• Utilize Docker for containerization, ensuring scalable and efficient deployment.\n",
       "• Collaborate with cross-functional teams using Git for version control and code management.\n",
       "\n",
       "Requirements:\n",
       "• Bachelor’s/Master’s degree in Computer Science or equivalent.\n",
       "• A minimum of 4 years of hands-on experience in Python software development with a focus on modular, scalable, and efficient coding for AI/LLM-based applications.\n",
       "• Experience developing and maintaining RESTful APIs using FastAPI and interfacing with relational databases.\n",
       "• Experience in implementing LLMs (OpenAI, Azure) using vector databases, text embedding, and RAG.\n",
       "• Hands-on experience with cloud platforms (AWS, Azure, GCP) and docker for AI deployment and scaling.\n",
       "• Strong Git skills for version control and collaborative development.\n",
       "• Ability to adjust and thrive in a fast-paced environment with changing priorities.\n",
       "• Strong analytical and problem-solving skills.\n",
       "• Familiarity with data visualization tools such as MicroStrategy preferred.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "The recruitment process includes online assessments as a first step. We send them via e-mail, please check also your SPAM folder.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=HchZv1aDiP5RZdWOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWKMQ7CMAwAxdonMHmmJUFILDAxIFSkTn1A5QQrCWrtKg5SeQnfpSw33F313VR1T5wkw7WFG4fERBla2MNDHChh9hGE4S4SRtpeYimznq1VHU3QgiV542WywuRksS9x-segETPNIxYajqfDYmYOu7pLPktf8mrDBxJD954cpgY6XHfUuKYGWn4m_AHJnWG9mgAAAA&shmds=v1_AdeF8Kg7hAEdri5RQpm-kTSnlODzqo61RM6-_hvQb-2-gVRN4w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=HchZv1aDiP5RZdWOAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>Snr AI Security Engineer (Detection)</td><td>Zoom</td><td>India</td><td>What you can expect\n",
       "\n",
       "Zoom are seeking to hire an experienced, hands on AI Security Engineer (Detection)with proven experience in AI and LLM Security, to join the Threat Detection & Analytics Engineering team, which is a part of the Detection and Response (D&R) org in Information Security.\n",
       "\n",
       "About the Team\n",
       "\n",
       "The AI security engineer will play a crucial role in ensuring the security of Zoom’s AI products. This team identify, assess, and detect application-level threats and vulnerabilities.\n",
       "\n",
       "Responsibilities:\n",
       "• Collaborate with AI, engineering and DevOps teams to model AI threats and assess risks.\n",
       "• Design and develop threat detection solutions tailored to Zoom’s AI applications and services.\n",
       "• Proactively identify AI application log sources with detection values and facilitate onboarding and tuning in order to have better efficacy.\n",
       "• Build and maintain automations to streamline detection processes.\n",
       "\n",
       "What we are looking for:\n",
       "\n",
       "Have a B.S. in Computer Science, Information Security, or related field.\n",
       "\n",
       "5 - 10 years experience as a Security Engineer with a focus on AI Security, especially LLM security.\n",
       "• Solid understanding of application security concepts, including OWASP Top Ten for LLM applications.\n",
       "• Deep knowledge of security architecture of applications and services deployed to on-premise and cloud environments.\n",
       "• Experience with building security defenses against attacks such as prompt injection, data poisoning and leakage for AI/ML systems.\n",
       "• Proficiency in programming (Python, Java, Scala, etc.).\n",
       "• Proficiency in statistical analysis and machine learning methodologies.\n",
       "• Knowledge of MITRE ATLAS framework preferrable.\n",
       "• Be experienced with cloud environments and cloud security principles. Also have the technical know-how and understanding of APIs, vulnerability management (CVE, CVSS, OWASP Top 10), container, network, and systems security.\n",
       "• Be experienced in collaborating and working effectively with global and remote teams.\n",
       "\n",
       "#India\n",
       "\n",
       "#Remote\n",
       "\n",
       "#RemoteIndia\n",
       "\n",
       "Ways of Working\n",
       "Our structured hybrid approach is centered around our offices and remote work environments. The work style of each role, Hybrid, Remote, or In-Person is indicated in the job description/posting.\n",
       "\n",
       "Benefits\n",
       "As part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways. Click Learn for more information.\n",
       "\n",
       "About Us\n",
       "Zoomies help people stay connected so they can get more done together. We set out to build the best collaboration platform for the enterprise, and today help people communicate better with products like Zoom Contact Center, Zoom Phone, Zoom Events, Zoom Apps, Zoom Rooms, and Zoom Webinars.\n",
       "We’re problem-solvers, working at a fast pace to design solutions with our customers and users in mind. Find room to grow with opportunities to stretch your skills and advance your career in a collaborative, growth-focused environment.\n",
       "\n",
       "Our Commitment​\n",
       "\n",
       "At Zoom, we believe great work happens when people feel supported and empowered. We’re committed to fair hiring practices that ensure every candidate is evaluated based on skills, experience, and potential. If you require an accommodation during the hiring process, let us know—we’re here to support you at every step.\n",
       "\n",
       "If you need assistance navigating the interview process due to a medical disability, please submit an Accommodations Request Form and someone from our team will reach out soon. This form is solely for applicants who require an accommodation due to a qualifying medical disability. Non-accommodation-related requests, such as application follow-ups or technical issues, will not be addressed.\n",
       "\n",
       "#LI-Remote</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=aWSfnf_ubnOAGDwvAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFwL_oDTjSqYiOBiJ0GRunZzKWk8kkh6F3In1E_wr8U3vOa7aNqeKpw76NG_a9IPXCkkQqywvqCi18S0gR3ceQRBV30EJrgxh4yrNqoWOVkrkk0QdZq88TxZJhx5ti8e5d8g0VUs2SkOh-N-NoXCdvlgniARdPRM7gc43eVojAAAAA&shmds=v1_AdeF8KhsqS5HiIfYqXXjqnMGVgJBnreqyc2xm6dUXOwh-SVzFQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=aWSfnf_ubnOAGDwvAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>Senior AI Engineer (5+ Years Exp) – Immediate Joiners Only</td><td>Perimattic</td><td>Mumbai, Maharashtra, India</td><td>Experience: 5+ years\n",
       "\n",
       "Location: Mumbai, Maharashtra\n",
       "\n",
       "Joining: Immediate only\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are hiring an experienced Senior AI Engineer who has successfully built and scaled AI systems in production. This role requires deep expertise in LLMs, ML pipelines, model optimization, and real-time deployment. You should also be comfortable leading junior developers and contributing to architectural decisions.\n",
       "\n",
       "Key Responsibilities\n",
       "• Lead design, development, and deployment of advanced AI systems\n",
       "• Fine-tune large language models and optimize training pipelines\n",
       "• Own the end-to-end lifecycle of AI projects (data to deployment)\n",
       "• Evaluate third-party models and tools, and recommend best-fit solutions\n",
       "• Collaborate with cross-functional teams to align AI efforts with product goals\n",
       "\n",
       "Must-Have Skills\n",
       "• 5+ years of hands-on experience in AI/ML roles\n",
       "• Strong understanding of NLP, deep learning, and large-scale model architectures\n",
       "• Proficiency in Python, PyTorch/TensorFlow, and data engineering tools\n",
       "• Experience with LLMs, vector databases, and MLOps tools\n",
       "• Ability to design and optimize ML pipelines for performance and scalability\n",
       "\n",
       "Preferred Skills\n",
       "• Experience managing AI teams or mentoring juniors\n",
       "• Working knowledge of Retrieval-Augmented Generation (RAG), embeddings\n",
       "• Familiarity with CI/CD, Docker, and cloud deployment pipelines</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=OR6W3HincLy0sLuwAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOvQrCQBCEsc0jWG0Z_xIR0mhlIRIhKKSyCpu4JCe53XB7Qux8B1_Hp_FJPJuBYb4ZJvpMorIkNuJgn8OBW8NEDuJsAVdCp3AYhxl8X2_IraWbQU9wkgCF6Mz9E1bB1qCBbToQhqNI29N013k_6DZNVfukVY_eNEkjNhWmWsb0LrX-pdIOHQ192K022XpMBm7n8YWcsehDBwxD8bA1miUUGFjUzjtcQs7hzA-lfAaKvwAAAA&shmds=v1_AdeF8KjJuZWUSEQ-suqfddJlmbJASMO65LeEZJXEeTp06MDW-g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=OR6W3HincLy0sLuwAAAAAA%3D%3D</td><td>AI Engineer</td></tr><tr><td>AI/ML Engineer</td><td>HPE</td><td>Hyderabad, Telangana, India</td><td>AI/ML Engineer\n",
       "\n",
       "This role has been designed as ‘’Onsite’ with an expectation that you will primarily work from an HPE office.\n",
       "\n",
       "Who We Are:\n",
       "\n",
       "Hewlett Packard Enterprise is the global edge-to-cloud company advancing the way people live and work. We help companies connect, protect, analyze, and act on their data and applications wherever they live, from edge to cloud, so they can turn insights into outcomes at the speed required to thrive in today’s complex world. Our culture thrives on finding new and better ways to accelerate what’s next. We know varied backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good. If you are looking to stretch and grow your career our culture will embrace you. Open up opportunities with HPE.\n",
       "\n",
       "Job Description:\n",
       "\n",
       "Job Family Definition:\n",
       "\n",
       "The Cloud Developer builds from the ground up to meet the needs of mission-critical applications, and is always looking for innovative approaches to deliver end-to-end technical solutions to solve customer problems.\n",
       "\n",
       "Brings technical thinking to break down complex data and to engineer new ideas and methods for solving, prototyping, designing, and implementing cloud-based solutions.\n",
       "Collaborates with project managers and development partners to ensure effective and efficient delivery, deployment, operation, monitoring, and support of Cloud engagements.\n",
       "\n",
       "The Cloud Developer provides business value expertise to drive the development of innovative service offerings that enrich HPE's Cloud Services portfolio across multiple systems, platforms, and applications.\n",
       "\n",
       "Management Level Definition:\n",
       "\n",
       "Contributions impact technical components of HPE products, solutions, or services regularly and sustainable. Applies advanced subject matter knowledge to solve complex business issues and is regarded as a subject matter expert. Provides expertise and partnership to functional and technical project teams and may participate in cross-functional initiatives. Exercises significant independent judgment to determine best method for achieving objectives. May provide team leadership and mentoring to others.\n",
       "\n",
       "What you will do:\n",
       "• Experiment, design, develop and maintain machine learning models and pipelines with a high potential for value and scale.\n",
       "• Collaborate with other ML engineers, data scientists, product managers, and other engineers to ensure successful implementation of machine learning solutions.\n",
       "• Perform research and testing to develop or customize machine learning algorithms; conduct model training and evaluation as needed; integrate, test, tune and monitor the solutions developed.\n",
       "• Research and evaluate new technologies and tools for machine learning.\n",
       "• Maintain and update existing machine learning systems.\n",
       "• Hands-on develop, productionize, and operate Machine Learning models and pipelines at scale, including both batch and real-time use cases.\n",
       "• Work with large scale structured and unstructured data, build and continuously improve cutting edge Machine Learning models.\n",
       "• Leads the project team for design and development of complex products and platforms, including solution design, analysis, coding, testing, and integration for building efficient, scalable and robust cloud subsystems.\n",
       "• Reviews and evaluates designs, test plans, and develops code for compliance with cloud design and development guidelines and standards.\n",
       "• Provides tangible feedback to improve product quality and mitigate risks.\n",
       "• Represents the engineering team in various technical forums and provides guidance and mentoring to less-experienced team members.\n",
       "• Drives innovation and integration of new technologies into projects and activities in the software systems design organization.\n",
       "• Analyzes science, engineering, business, and other data processing problems to develop and implement solutions to complex application problems, system administration issues, or network concerns.\n",
       "\n",
       "What you will need:\n",
       "• Bachelor's or master's degree in computer science, engineering, information systems, or closely related quantitative discipline.\n",
       "• Typically, 7-10 years’ experience.\n",
       "\n",
       "Knowledge and Skills:\n",
       "• Strong programming skills in Python & preferrable familiar with Golang\n",
       "• Understanding microservice architecture and how they can be built in a containerized, Kubernetes-managed environment is central to all modern cloud-native applications.\n",
       "• Designing and integrating software systems running on multiple platform types into the overall architecture.\n",
       "• Evaluating forms and processes for software systems testing and methodology, including writing and executing test plans, debugging, and testing scripts and tools.\n",
       "• Excellent written and verbal communication skills. Ability to effectively communicate product architectures and design proposals at senior management levels.\n",
       "\n",
       "Additional Skills:\n",
       "\n",
       "Cloud Architectures, Cross Domain Knowledge, Design Thinking, Development Fundamentals, DevOps, Distributed Computing, Microservices Fluency, Full Stack Development, Release Management, Security-First Mindset, User Experience (UX)\n",
       "\n",
       "What We Can Offer You:\n",
       "\n",
       "Health & Wellbeing\n",
       "\n",
       "We strive to provide our team members and their loved ones with a comprehensive suite of benefits that supports their physical, financial and emotional wellbeing.\n",
       "\n",
       "Personal & Professional Development\n",
       "\n",
       "We also invest in your career because the better you are, the better we all are. We have specific programs catered to helping you reach any career goals you have — whether you want to become a knowledge expert in your field or apply your skills to another division.\n",
       "\n",
       "Unconditional Inclusion\n",
       "\n",
       "We are unconditionally inclusive in the way we work and celebrate individual uniqueness. We know varied backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good.\n",
       "\n",
       "Let's Stay Connected:\n",
       "\n",
       "Follow @HPECareers on Instagram to see the latest on people, culture and tech at HPE.\n",
       "\n",
       "#india\n",
       "\n",
       "#hybridcloud\n",
       "\n",
       "Job:\n",
       "Engineering\n",
       "\n",
       "Job Level:\n",
       "TCP_04\n",
       "\n",
       "HPE is an Equal Employment Opportunity/ Veterans/Disabled/LGBT employer. We do not discriminate on the basis of race, gender, or any other protected category, and all decisions we make are made on the basis of qualifications, merit, and business need. Our goal is to be one global team that is representative of our customers, in an inclusive environment where we can continue to innovate and grow together. Please click here: Equal Employment Opportunity.\n",
       "\n",
       "Hewlett Packard Enterprise is EEO Protected Veteran/ Individual with Disabilities.\n",
       "\n",
       "HPE will comply with all applicable laws related to employer use of arrest and conviction records, including laws requiring employers to consider for employment qualified applicants with criminal histories.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=K2w_9m4znMEZnhJ5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU6H4CI1EcFFJ4diKwoO7uWSHmkk3pVchvoHfrb4hld9F9X63Nn7DRoOkYkybOEqDpQw-xGE4SISEi1PYymTHq1VTSZowRK98fK2wuRkti9x-q_XETNNCQv1-8NuNhOHzap9NBAZ2s9AGR0ONTwpIQdkrKHjIeIP5hSTu4sAAAA&shmds=v1_AdeF8KgzhY__i-5O2lHxWD-q9EllLuxaHTxtOzj1nFa4uBGxNQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=K2w_9m4znMEZnhJ5AAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>AI/ML Engineer – XR Software (All levels))</td><td>Qualcomm</td><td>Bengaluru, Karnataka, India</td><td>Company:\n",
       "Qualcomm India Private Limited\n",
       "\n",
       "Job Area:\n",
       "Engineering Group, Engineering Group > Systems Engineering\n",
       "\n",
       "General Summary:\n",
       "\n",
       "We are seeking a passionate and skilled AI/ML Engineer to join our cutting-edge Extended Reality (XR) Software team. In this role, you will work on next-generation XR products that blend the physical and digital worlds, leveraging artificial intelligence and machine learning to create immersive, intelligent, and responsive experiences.\n",
       "\n",
       "You will collaborate with cross-functional teams of researchers, engineers, and designers to build real-time AI/ML software optimized for XR platforms. A strong background in C++ or embedded firmware development is essential, as you will be working close to hardware and performance-critical systems.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and optimize AI/ML models for XR applications such as computer vision, sensor fusion, gesture recognition, and spatial understanding.\n",
       "• Implement real-time inference pipelines on embedded or edge devices.\n",
       "• Collaborate with firmware and hardware teams to integrate ML models into XR systems.\n",
       "• Analyze system performance and optimize for latency, power, and memory.\n",
       "• Stay up to date with the latest research and trends in AI/ML and XR technologies.\n",
       "• Contribute to the full lifecycle of product development—from prototyping to production.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Electrical Engineering, or a related field.\n",
       "• 1–10 years of industry experience in AI/ML engineering or embedded systems.\n",
       "• Proficiency in C++ and/or embedded firmware development.\n",
       "• Solid understanding of machine learning fundamentals and experience with frameworks like TensorFlow, PyTorch, or ONNX.\n",
       "• Experience with deploying ML models on edge devices\n",
       "• Familiarity with XR technologies (AR/VR/MR), sensor data processing, or 3D spatial computing.\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\n",
       "OR\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\n",
       "OR\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n",
       "\n",
       "Applicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n",
       "\n",
       "Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n",
       "\n",
       "To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n",
       "\n",
       "If you would like more information about this role, please contact Qualcomm Careers.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuw4BURCA4Wj3EVTTuYQ9ItFQISLrUqDRyewaZ5cxsznnLErv4DG8lSdB81df_uhdi6bjxKxXMBNbCJGDz_MF-y3s9BTu6AiaY2ZguhH7Vgu6sNAUPKHLclCBuaplqo_yEEo_NMZ7jq0PGIoszvRqVCjVhzlr6v85-Pz3LBkDHfqD3iMuxbYbmwr5h69QCExILHLlqg4s0QkGvGAHEjkW-AWsJyrJrgAAAA&shmds=v1_AdeF8Kjkwu3emhikRMcwxsbJzM0PcFN4Cmp6p0XCuEOoHNe2gw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>ML Engineer II - Applied AI</td><td>Uber</td><td>Bengaluru, Karnataka, India</td><td>ABOUT THE ROLE\n",
       "\n",
       "Applied AI is a horizontal AI team at Uber collaborating with business units across the company to deliver cutting-edge AI solutions for core business problems. We work closely with engineering, product and data science teams to understand key business problems and the potential for AI solutions, then deliver those AI solutions end-to-end. Key areas of expertise include Generative AI, Computer Vision, and Personalization.\n",
       "\n",
       "We are looking for a strong Senior ML engineer to be a part of a high-impact team at the intersection of classical machine learning, generative AI, and ML infrastructure. In this role, you’ll be responsible for delivering Uber’s next wave of intelligent experiences by building ML solutions that power core user and business-facing products.\n",
       "\n",
       "WHAT YOU’LL DO\n",
       "• Solve business-critical problems using a mix of classical ML, deep learning, and generative AI.\n",
       "• Collaborate with product, science, and engineering teams to execute on the technical vision and roadmap for Applied AI initiatives.\n",
       "• Deliver high-quality, production-ready ML systems and infrastructure, from experimentation through deployment and monitoring.\n",
       "• Adopt best practices in ML development lifecycle (e.g., data versioning, model training, evaluation, monitoring, responsible AI).\n",
       "• Deliver enduring value in the form of software and model artifacts.\n",
       "\n",
       "BASIC QUALIFICATIONS\n",
       "• Master or PhD or equivalent experience in Computer Science, Engineering, Mathematics or a related field and 2 years of Software Engineering work experience, or 5 years Software Engineering work experience.\n",
       "• Experience in programming with a language such as Python, C, C++, Java, or Go.\n",
       "• Experience with ML packages such as Tensorflow, PyTorch, JAX, and Scikit-Learn.\n",
       "• Experience with SQL and database systems such as Hive, Kafka, and Cassandra.\n",
       "• Experience in the development, training, productionization and monitoring of ML solutions at scale.\n",
       "\n",
       "PREFERRED QUALIFICATIONS\n",
       "• Prior experience working with generative AI (e.g., LLMs, diffusion models) and integrating such technologies into end-user products.\n",
       "• Experience in modern deep learning architectures and probabilistic models.\n",
       "• Machine Learning, Computer Science, Statistics, or a related field with research or applied focus on large-scale ML systems.\n",
       "\n",
       "We welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.\n",
       "\n",
       "Offices continue to be central to collaboration and Uber’s cultural identity. Unless formally approved to work fully remotely, Uber expects employees to spend at least half of their work time in their assigned office. For certain roles, such as those based at green-light hubs, employees are expected to be in-office for 100% of their time. Please speak with your recruiter to better understand in-office expectations for this role.\n",
       "• Accommodations may be available based on religious and/or medical conditions, or as required by applicable law. To request an accommodation, please reach out to accommodations@uber.com.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=75zem_tk1v_fxmEhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNvwrCMBCAcVz7CE4HblIbKbjoVEGk_hmdy6U90mi8C7kUfBKfV7t88Ju-4rso6vsNTuw8EyVoW9hAE2PwNEAz4yIWlDD1IwjDWcQFWh7GnKPujVENldOM2fdVL28jTFY-5ilW53Q6YqIYMFNX77afKrJbrx72f_IMR2KHYUpTCVdMjBlfWELLg8cfNrKN3ZkAAAA&shmds=v1_AdeF8Kip_hUrnQpLhkvbK1ixwB1cbNY3BqXJxBMbZjlUuzpZyw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=75zem_tk1v_fxmEhAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>AI/ML Engineer</td><td>ASSA ABLOY</td><td>Chennai, Tamil Nadu, India</td><td>An Amazing Career Opportunity for AI/ML Engineer Location: Chennai, India (Hybrid) Job ID: 39582 Position Summary A rewarding career at HID Global beckons you! We are looking for an AI/ML Engineer , who is responsible for designing, developing, and deploying advanced AI/ML solutions to solve complex business challenges. This role requires expertise in machine learning, deep learning, MLOps, and AI model optimization, with a focus on building scalable, high-performance AI systems. As an AI/ML Engineer, you will work closely with data engineers, software developers, and business stakeholders to integrate AI-driven insights into real-world applications. You will be responsible for model development, system architecture, cloud deployment, and ensuring responsible AI adoption. We are a leading company in the trusted source for innovative HID Global Human Resources products, solutions and services that help millions of customers around the globe create, manage and use secure identities. Who are we? HID powers the trusted identities of the world’s people, places, and things, allowing people to transact safely, work productively and travel freely. We are a high-tech software company headquartered in Austin, TX, with over 4,000 worldwide employees. Check us out: www.hidglobal.com and https://youtu.be/23km5H4K9Eo About HID Global, Chennai HID Global powers the trusted identities of the world’s people, places and things. We make it possible for people to transact safely, work productively and travel freely. Our trusted identity solutions give people secure and convenient access to physical and digital places and connect things that can be accurately identified, verified and tracked digitally. Millions of people around the world use HID products and services to navigate their everyday lives, and over 2 billion things are connected through HID technology. We work with governments, educational institutions, hospitals, financial institutions, industrial businesses and some of the most innovative companies on the planet. Headquartered in Austin, Texas, HID Global has over 3,000 employees worldwide and operates international offices that support more than 100 countries. HID Global® is an ASSA ABLOY Group brand. For more information, visit www.hidglobal.com. HID Global has is the trusted source for secure identity solutions for millions of customers and users around the world. In India, we have two Engineering Centre (Bangalore and Chennai) over 200+ Engineering Staff. Global Engineering Team is based in Chennai and one of the Business Unit Engineering team is based in Bangalore. Physical Access Control Solutions (PACS) HID's Physical Access Control Solutions Business Area: HID PAC’s Business Unit focuses on the growth of new clients and existing clients where we leverage the latest card and reader technologies to solve the security challenges of our clients. Other areas of focus will include authentication, card sub systems, card encoding, Biometrics, location services and all other aspects of a physical access control infrastructure. Qualifications:- To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Roles & Responsibilities: Design, develop, and deploy robust & scalable AI/ML models in Production environments. Collaborate with business stakeholders to identify AI/ML opportunities and define measurable success metrics. Design and build Retrieval-Augmented Generation (RAG) pipelines integrating vector stores, semantic search, and document parsing for domain-specific knowledge retrieval. Integrate Multimodal Conversational AI platforms (MCP) including voice, vision, and text to deliver rich user interactions. Drive innovation through PoCs, benchmarking, and experiments with emerging models and architectures. Optimize models for performance, latency and scalability. Build data pipelines and workflows to support model training and evaluation. Conduct research & experimentation on the state-of-the-art techniques (DL, NLP, Time series, CV) Partner with MLOps and DevOps teams to implement best practices in model monitoring, version and re-training. Lead code reviews, architecture discussions and mentor junior & peer engineers. Architect and implement end-to-end AI/ML pipelines, ensuring scalability and efficiency. Deploy models in cloud-based (AWS, Azure, GCP) or on-premises environments using tools like Docker, Kubernetes, TensorFlow Serving, or ONNX Ensure data integrity, quality, and preprocessing best practices for AI/ML model development. Ensure compliance with AI ethics guidelines, data privacy laws (GDPR, CCPA), and corporate AI governance. Work closely with data engineers, software developers, and domain experts to integrate AI into existing systems. Conduct AI/ML training sessions for internal teams to improve AI literacy within the organization. Strong analytical and problem solving mindset. Technical Requirements: Strong expertise in AI/ML engineering and software development. Strong experience with RAG architecture, vector databases Proficiency in Python and hands-on experience in using ML frameworks (tensorflow, pytorch, scikit-learn, xgboost etc) Familiarity with MCPs like Google Dialogflow, Rasa, Amazon Lex, or custom-built agents using LLM orchestration. Cloud-based AI/ML experience (AWS Sagemaker, Azure ML, GCP Vertex AI, etc.). Solid understanding of AI/ML life cycle – Data preprocessing, feature engineering, model selection, training, validation and deployment. Experience in production grade ML systems (Model serving, APIs, Pipelines) Familiarity with Data engineering tools (SPARK, Kafka, Airflow etc) Strong knowledge of statistical modeling, NLP, CV, Recommendation systems, Anomaly detection and time series forecasting. Hands-on in Software engineering with knowledge of version control, testing & CI/CD Hands-on experience in deploying ML models in production using Docker, Kubernetes, TensorFlow Serving, ONNX, and MLflow. Experience in MLOps & CI/CD for ML pipelines, including monitoring, retraining, and model drift detection. Proficiency in scaling AI solutions in cloud environments (AWS, Azure & GCP). Experience in data preprocessing, feature engineering, and dimensionality reduction. Exposure to Data privacy, Compliance and Secure ML practices Education and/or Experience: Graduation or master’s in computer science or information technology or AI/ML/Data science 3+ years of hands-on experience in AI/ML development/deployment and optimization Experience in leading AI/ML teams and mentoring junior engineers. Why apply? Empowerment: You’ll work as part of a global team in a flexible work environment, learning and enhancing your expertise. We welcome an opportunity to meet you and learn about your unique talents, skills, and experiences. You don’t need to check all the boxes. If you have most of the skills and experience, we want you to apply. Innovation: You embrace challenges and want to drive change. We are open to ideas, including flexible work arrangements, job sharing or part-time job seekers. Integrity: You are results-orientated, reliable, and straightforward and value being treated accordingly. We want all our employees to be themselves, to feel appreciated and accepted. This opportunity may be open to flexible working arrangements. HID is an Equal Opportunity/Affirmative Action Employer – Minority/Female/Disability/Veteran/Gender Identity/Sexual Orientation. We make it easier for people to get where they want to go! On an average day, think of how many times you tap, twist, tag, push or swipe to get access, find information, connect with others or track something. HID technology is behind billions of interactions, in more than 100 countries. We help you create a verified, trusted identity that can get you where you need to go – without having to think about it. When you join our HID team, you’ll also be part of the ASSA ABLOY Group, the global leader in access solutions. You’ll have 63,000 colleagues in more than 70 different countries. We empower our people to build their career around their aspirations and our ambitions – supporting them with regular feedback, training, and development opportunities. Our colleagues think broadly about where they can make the most impact, and we encourage them to grow their role locally, regionally, or even internationally. As we welcome new people on board, it’s important to us to have diverse, inclusive teams, and we value different perspectives and experiences. #LI-HIDGlobal We make it easier for people to get where they want to go! On an average day, think of how many times you tap, twist, tag, push or swipe to get access, find information, connect with others or track something. HID technology is behind billions of interactions, in more than 100 countries. We help you create a verified, trusted identity that can get you where you need to go – without having to think about it. When you join our HID team, you’ll also be part of the ASSA ABLOY Group, the global leader in access solutions. You’ll have 63,000 colleagues in more than 70 different countries. We empower our people to build their career around their aspirations and our ambitions – supporting them with regular feedback, training, and development opportunities. Our colleagues think broadly about where they can make the most impact, and we encourage them to grow their role locally, regionally, or even internationally. As we welcome new people on board, it’s important to us to have diverse, inclusive teams, and we value different perspectives and experiences. #LI-HIDGlobal</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=bbr4TKvbz_KjqmOOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU63CCK1EcFFpygilapDXZzKpT2SSHpXmgj9Cr9ZfMPLvrNsoUt1q-DM1jPRCGu4ioFIOLYOhOEiYgPNDy6lIe6VijEUNiZMvi1a6ZUwGZnUW0z810SHIw0BEzXb3WYqBrarpa5rDfpYPV7gGU6OmNHn8MTeB7hj98mh5M7jD830oIWRAAAA&shmds=v1_AdeF8Kj2Mp6NCO33Hp2mrtyNB-o4RHb2REYzOhPXCCC1Z8-GMw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=bbr4TKvbz_KjqmOOAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>Machine Learning Engineer – ML Ops & Support</td><td>Aera Technology</td><td>Pune, Maharashtra, India</td><td>Aera Technology is the Decision Intelligence company. Our platform, Aera Decision Cloud™, integrates with enterprise systems to digitize, augment, and automate decisions in real time. We deliver millions of AI-powered recommendations each year, generating significant value for some of the world’s largest brands.\n",
       "\n",
       "We are seeking a Machine Learning Engineer (Support & Ops focus) to ensure our AI-powered decision systems run reliably at scale. This role is less about building models from scratch, and more about keeping production AI systems healthy, observable, and performant, while enabling Data Science teams to deliver faster.\n",
       "\n",
       "This position is also a strong career pathway into ML feature development — you will work closely with Product, Data Science, and Engineering teams, gain exposure to LLMs, Agentic AI, and advanced ML tooling, and progressively take on more responsibilities in building new ML-powered product features.\n",
       "\n",
       "Responsibilities\n",
       "• Monitor, troubleshoot, and maintain ML pipelines and services in production, ensuring high availability and minimal downtime.\n",
       "• Work closely with Data Scientists and Engineers to operationalize ML/LLM models, from development through deployment.\n",
       "• Build and maintain observability tools for tracking data quality, model performance, drift detection, and inference metrics.\n",
       "• Support LLM and Agentic AI features in production, focusing on stability, optimization, and seamless integration into the platform.\n",
       "• Develop and enhance internal ML tooling for faster experimentation, deployment, and feature integration.\n",
       "• Collaborate with Product teams to roll out new ML-driven features and improve existing ones.\n",
       "• Work with DevOps to improve CI/CD workflows for ML code, data pipelines, and models.\n",
       "• Optimize resource usage and costs for large-scale model hosting and inference.\n",
       "• Document workflows, troubleshooting guides, and best practices for ML systems support.\n",
       "\n",
       "About You\n",
       "• B.E./ B.Tech in Computer Science, Engineering, or related field.3–5 years of experience in software engineering, ML Ops, or ML platform support.\n",
       "• Strong Python skills, with experience in production-grade code and automation. Experience with ML pipeline orchestration tools (Airflow, Prefect, Kubeflow, or similar).\n",
       "• Familiarity with containerized microservices (Docker, Kubernetes) and CI/CD pipelines.\n",
       "• Experience monitoring ML systems using tools like Prometheus, Grafana, ELK, Sentry, or equivalent.\n",
       "• Understanding of model packaging and serving frameworks (FastAPI, TorchServe, Triton Inference Server, Hugging Face Inference API).\n",
       "• Strong collaboration skills with cross-functional teams.\n",
       "\n",
       "Good to Have\n",
       "• Exposure to LLM operations (prompt engineering, fine-tuning, inference optimization).Familiarity with Agentic AI workflows and multi-step orchestration (LangChain, LlamaIndex).\n",
       "• Experience with data versioning (DVC, Delta Lake) and experiment tracking (MLflow, Weights & Biases).\n",
       "• Knowledge of vector databases (Pinecone, Weaviate, FAISS).\n",
       "• Experience with streaming data (Kafka) and caching (Redis).\n",
       "• Skills in cost optimization for GPU workloads.\n",
       "• Basic understanding of system design for large-scale AI infrastructure.\n",
       "\n",
       "If you share our passion for building a sustainable, intelligent, and efficient world, you’re in the right place. Established in 2017 and headquartered in Mountain View, California, we're a series D start-up, with teams in Mountain View, San Francisco (California), Bucharest and Cluj-Napoca (Romania), Paris (France), Munich (Germany), London (UK), Pune (India), and Sydney (Australia). So join us, and let’s build this!\n",
       "\n",
       "Aera Technology is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status.\n",
       "\n",
       "Benefits Summary\n",
       "\n",
       "At Aera Technology, we strive to support our Aeranauts and their loved ones through different stages of life with a variety of attractive benefits, and great perks. In addition to offering a competitive salary and company stock options, we have other great benefits available. You’ll find comprehensive medical, Group Medical Insurance, Term Insurance, Accidental Insurance, paid time off, Maternity leave, and much more. We offer unlimited access to online professional courses for both professional and personal development, coupled with people manager development programs. We believe in a flexible working environment, to allow our Aeranauts to perform at their best, ensuring a healthy work-life balance. When you’re working from the office, you’ll also have access to a fully-stocked kitchen with a selection of snacks and beverages.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=pDU2hLyxJBfqhe6DAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPYoCQRCGYTb1CEZfZODPzLJgopGBiOKwC2suNW3R3TJWNV0taOYd9iReyZM4m7zwwjN4fgzWDbkQhbFnyhLFYy2-f854Pf7Q7PGdDCP8XlPSXDDDTltYj12ACjaqvuPhMpSSbFHXZl3lrVCJrnJ6qVW41Vt91tb-c7RAmVNHhY9f889blcSPJyvOhAO7INqpvyMKfq7CUzTUc7JQMk2xlVOkN6KHg3e0AAAA&shmds=v1_AdeF8KjDKCLrvGPgND_vK-DY1qQu4OMxZP9uLc_ZBdRA5BzEAA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=pDU2hLyxJBfqhe6DAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>AI/ML Engineer</td><td>CodeCraft Technologies</td><td>Bengaluru, Karnataka, India (+1 other)</td><td>Position Overview\n",
       "We are looking for an experienced AI/ML Engineer to join our team. The ideal candidate will bring a deep understanding of machine learning, artificial intelligence, and big data technologies, with proven expertise in developing scalable AI/ML solutions. This role shall lead technical efforts, mentor team members, and collaborate with cross-functional teams to design, develop, and deploy cutting edge AI/ML applications.\n",
       "\n",
       "Job Details\n",
       "• Job Category: AI/ML Engineer.\n",
       "• Job Type: Full-Time\n",
       "• Job Location: Bangalore/Mangalore\n",
       "• Experience Required: 4-8 Years\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and deploy deep learning models for object classification, detection, and segmentation using CNNs and Transfer Learning.\n",
       "• Implement image preprocessing and advanced computer vision pipelines.\n",
       "• Optimize deep learning models using pruning, quantization, and ONNX for deployment on edge devices.\n",
       "• Work with PyTorch, TensorFlow, and ONNX frameworks to develop and convert models.\n",
       "• Accelerate model inference using GPU programming with CUDA and cuDNN.\n",
       "• Port and test models on embedded and edge hardware platforms. (Orin, Jetson, Hailo)\n",
       "• Conduct research and experiments to evaluate and integrate GenAI technologies in computer vision tasks.\n",
       "• Explore and implement cloud-based AI workflows, particularly using AWS/Azure AI/ML services.\n",
       "• Collaborate with cross-functional teams for data analytics, data processing, and large-scale model training.\n",
       "\n",
       "Desired Profile:\n",
       "• Strong programming experience in Python.\n",
       "• Solid background in deep learning, CNNs, and transfer learning and Machine learning basics.\n",
       "• Expertise in object detection, classification, segmentation.\n",
       "• Proficiency with PyTorch, TensorFlow, and ONNX.\n",
       "• Experience with GPU acceleration (CUDA, cuDNN).\n",
       "• Hands-on knowledge of model optimization (pruning, quantization).\n",
       "• Experience deploying models to edge devices (e.g., Jetson, mobile, Orin, Hailo )\n",
       "• Understanding of image processing techniques.\n",
       "• Familiarity with data pipelines, data preprocessing, and data analytics.\n",
       "• Willingness to explore and contribute to Generative AI and cloud-based AI solutions.\n",
       "• Good problem-solving and communication skills.\n",
       "\n",
       "Good to have\n",
       "• Experience with C/C++.\n",
       "• Familiarity with AWS Cloud AI/ML tools (e.g., SageMaker, Rekognition).\n",
       "• Exposure to GenAI frameworks like OpenAI, Stable Diffusion, etc.\n",
       "• Knowledge of real-time deployment systems and streaming analytics.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Graduation/Post-graduation in Computers, Engineering, or Statistics from a reputed institute\n",
       "\n",
       "If you are passionate to work in a collaborative and challenging environment, apply now!</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=6NG-cOAKE8257UNnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE63uIg2InTRSYtI_dncyzU9k2i8K7kU-i6-rPgNX_GdFYtDY-43OLELTJRgDRfpQAmT9SAMZxEXab73OQ-6M0Y1lk4z5mBLKx8jTJ1M5iWd_mvVY6IhYqZ2W22mcmC3rGrpqU74zPAg61miuEAKgeFI7DCOaVzBFRNjxjeuoOE-4A_ILc0zngAAAA&shmds=v1_AdeF8KirjKT_jo2XCCNYa5AY1aXL_c1eKOzfI2Xad0tGwJ5Olg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=6NG-cOAKE8257UNnAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>Senior AI/ML Engineer- Python, Genai, NLP, ML,LLM</td><td>UnitedHealth Group</td><td>Hyderabad, Telangana, India</td><td>Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.\n",
       "\n",
       "Optum AI is UnitedHealth Group’s enterprise AI team. We are AI/ML scientists and engineers with deep expertise in AI/ML engineering for health care. We develop AI/ML solutions for the highest impact opportunities across UnitedHealth Group businesses including UnitedHealthcare, Optum Financial, Optum Health, Optum Insight, and Optum Rx. In addition to transforming the health care journey through responsible AI/ML innovation, our charter also includes developing and supporting an enterprise AI/ML development platform.\n",
       "\n",
       "Optum AI team members:\n",
       "• Have impact at scale: We have the data and resources to make an impact at scale. When our solutions are deployed, they have the potential to make health care system work better for everyone\n",
       "• Do ground-breaking work: Many of our current projects involve cutting edge ML, NLP and LLM techniques. Generative AI methods for working with structured and unstructured health care data are continuously being developed and improved. We are working in one of the most important frontiers of AI/ML research and development\n",
       "• Partner with world-class experts on innovative solutions: Our team members are developing novel AI/ML solutions to business challenges. In some cases, this includes the opportunity to file patents and publish papers about the methods we develop. We also collaborate with AI/ML researchers at some of the world’s top universities\n",
       "\n",
       "Primary Responsibilities:\n",
       "• Develop and deploy innovative AI solutions to various business problems. You will have the opportunity to learn and apply some of the most cutting-edge AI technologies\n",
       "• Research and evaluate new data sources, AI technology trends and methodologies to enhance Organization level data science capabilities and solutions\n",
       "• Perform data analysis, pre-processing and feature engineering required to ensure quality and reliability of the AI models and optimize them for performance and robustness\n",
       "• Develop and deploy big data pipelines and frameworks for data ingestion, processing and analysis\n",
       "• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n",
       "\n",
       "Required Qualifications:\n",
       "• Bachelor's degree or higher in computer science or engineering with a focus on language processing.\n",
       "• 7+ years of experience in AI, ML and/or NLP R&D (with demonstrable impact preferred). This requirement maybe relaxed for candidates with a PhD\n",
       "• 6+ years of overall experience\n",
       "• Experience with both traditional machine learning algorithms and modern deep learning and generative AI models\n",
       "• Proven good knowledge of and hands-on skills with repository management and GPU use, and the ability to rapidly set up pipelines for testing new ideas\n",
       "• Proficiency in Python, R, SQL and other programming languages for data analysis and modeling\n",
       "• Proficiency with cloud development environments such as Azure, AWS, and GCP\n",
       "• Proven excellent analytical and problem-solving skills, including the ability to disaggregate issues, identify root causes and recommend solutions\n",
       "\n",
       "At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=cF7V2ieHA0fldXQ3AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43S2xUcNHJQdpKKoI6y7U9kki8K0mE-mt-nfiGV3xnRX0l9hLh0OjWwJGtZ6K4hMsnO2EFFTF6BWdzUdAaZUwLSzhJB4kw9g6EoRKxgeZ7l_OYdlqnFEqbMmbfl728tDB1MumndOnfIzmMNAbM9NhsV1M5sl2s7-wzDTVhyA6qKO8RPEP9GShih4OCGwVki4wKGh48_gAuCopqvQAAAA&shmds=v1_AdeF8Ki94rxPTvuG1T6wimaYNTmJ9St7I61OGcvGJ1jJgZbgDA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=cF7V2ieHA0fldXQ3AAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>Associate AI/ML Engineer</td><td>Maersk</td><td>India</td><td>Assoicate AIML Engineer– Global Data Analytics, Technology (Maersk)\n",
       "\n",
       "This position will be based in India – Bangalore/Pune\n",
       "\n",
       "A.P. Moller - Maersk\n",
       "\n",
       "A.P. Moller – Maersk is the global leader in container shipping services. The business operates in 130 countries and employs 80,000 staff. An integrated container logistics company, Maersk aims to connect and simplify its customers’ supply chains.\n",
       "\n",
       "Today, we have more than 180 nationalities represented in our workforce across 131 Countries and this mean, we have elevated level of responsibility to continue to build inclusive workforce that is truly representative of our customers and their customers and our vendor partners too.\n",
       "\n",
       "We are responsible for moving 20 % of global trade & is on a mission to become the Global Integrator of Container Logistics. To achieve this, we are transforming into an industrial digital giant by combining our assets across air, land, ocean, and ports with our growing portfolio of digital assets to connect and simplify our customer’s supply chain through global end-to-end solutions, all the while rethinking the way we engage with customers and partners.\n",
       "\n",
       "The Brief\n",
       "\n",
       "In this role as an Associate AIML Engineer on the Global Data and Analytics (GDA) team, you will support the development of strategic, visibility-driven recommendation systems that serve both internal stakeholders and external customers. This initiative aims to deliver actionable insights that enhance supply chain execution, support strategic decision-making, and enable innovative service offerings.\n",
       "\n",
       "Data AI/ML (Artificial Intelligence and Machine Learning) Engineering involves the use of algorithms and statistical models to enable systems to analyse data, learn patterns, and make data-driven predictions or decisions without explicit human programming. AI/ML applications leverage vast amounts of data to identify insights, automate processes, and solve complex problems across a wide range of fields, including healthcare, finance, e-commerce, and more. AI/ML processes transform raw data into actionable intelligence, enabling automation, predictive analytics, and intelligent solutions. Data AI/ML combines advanced statistical modelling, computational power, and data engineering to build intelligent systems that can learn, adapt, and automate decisions.\n",
       "\n",
       "What I'll be doing – your accountabilities?\n",
       "• Build and maintain machine learning models for various applications, such as natural language processing, computer vision, and recommendation systems\n",
       "• Perform exploratory data analysis (EDA) to identify patterns and trends in data\n",
       "• Clean, preprocess, perform hyperparameter tuning and analyze large datasets to prepare them for AI/ML model training\n",
       "• Build, test, and optimize machine learning models and experiment with algorithms and frameworks to improve model performance\n",
       "• Use programming languages, machine learning frameworks and libraries, algorithms, data structures, statistics and databases to optimize and fine-tune machine learning models to ensure scalability and efficiency\n",
       "• Learn to define user requirements and align solutions with business needs\n",
       "• Work on AI/ML engineering projects, perform feature engineering and collaborate with teams to understand business problems\n",
       "• Learn best practices in data / AI/ML engineering and performance optimization\n",
       "• Contribute to research papers and technical documentation\n",
       "• Contribute to project documentation and maintain data quality standards\n",
       "\n",
       "Foundational Skills\n",
       "• Understands Programming skills beyond the fundamentals and can demonstrate this skill in most situations without guidance.\n",
       "• Understands the below skills beyond the fundamentals and can demonstrate in most situations without guidance\n",
       "• AI & Machine Learning\n",
       "• Data Analysis\n",
       "• Machine Learning Pipelines\n",
       "• Model Deployment\n",
       "\n",
       "Specialized Skills\n",
       "• To be able to understand beyond the fundamentals and can demonstrate in most situations without guidance for the following skills:\n",
       "• Deep Learning\n",
       "• Statistical Analysis\n",
       "• Data Engineering\n",
       "• Big Data Technologies\n",
       "• Natural Language Processing (NPL)\n",
       "• Data Architecture\n",
       "• Data Processing Frameworks\n",
       "• Proficiency in Python programming.\n",
       "• Proficiency in Python-based statistical analysis and data visualization tool\n",
       "• While having limited understanding of Technical Documentation but are focused on growing this skill\n",
       "\n",
       "Qualifications & Requirements\n",
       "• BSc/MSc/PhD in computer science, data science or related discipline with 1+ years of industry experience building cloud-based ML solutions for production at scale, including solution architecture and solution design experience\n",
       "• Good problem solving skills, for both technical and non-technical domains\n",
       "• Good broad understanding of ML and statistics covering standard ML for regression and classification, forecasting and time-series modeling, deep learning\n",
       "• 3+ years of hands-on experience building ML solutions in Python, incl knowledge of common python data science libraries (e.g. scikit-learn, PyTorch, etc)\n",
       "• Hands-on experience building end-to-end data products based on AI/ML technologies\n",
       "• Some experience with scenario simulations.\n",
       "• Experience with collaborative development workflow: version control (we use github), code reviews, DevOps (incl automated testing), CI/CD\n",
       "• Team player, eager to collaborate and good collaborator\n",
       "\n",
       "Preferred Experiences\n",
       "\n",
       "In addition to basic qualifications, would be great if you have…\n",
       "• Hands-on experience with common OR solvers such as Gurobi\n",
       "• Experience with a common dashboarding technology (we use PowerBI) or web-based frontend such as Dash, Streamlit, etc.\n",
       "• Experience working in cross-functional product engineering teams following agile development methodologies (scrum/Kanban/…)\n",
       "• Experience with Spark and distributed computing\n",
       "• Strong hands-on experience with MLOps solutions, including open-source solutions.\n",
       "• Experience with cloud-based orchestration technologies, e.g. Airflow, KubeFlow, etc\n",
       "• Experience with containerization (Kubernetes & Docker)\n",
       "\n",
       "As a performance-oriented company, we strive to always recruit the best person for the job – regardless of gender, age, nationality, sexual orientation or religious beliefs. We are proud of our diversity and see it as a genuine source of strength for building high-performing teams.\n",
       "\n",
       "Maersk is committed to a diverse and inclusive workplace, and we embrace different styles of thinking. Maersk is an equal opportunities employer and welcomes applicants without regard to race, colour, gender, sex, age, religion, creed, national origin, ancestry, citizenship, marital status, sexual orientation, physical or mental disability, medical condition, pregnancy or parental leave, veteran status, gender identity, genetic information, or any other characteristic protected by applicable law. We will consider qualified applicants with criminal histories in a manner consistent with all legal requirements.\n",
       "\n",
       "We are happy to support your need for any adjustments during the application and hiring process. If you need special assistance or an accommodation to use our website, apply for a position, or to perform a job, please contact us by emailing accommodationrequests@maersk.com.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=O_zgrdd16QeeSZLFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMoQ7CMBAAUD-DR50mYUdIMKAmCBlh37Bcy6UtdHdNr2KOXwfMk6_7dDiYqU_UGIYRpwdcJSRhrrCHuzowpuojqMBNNWTeXmJrxc6IZrkP1qgl33tdUIWdrvhSZ39mi1S55N88H0-HtS8SdpuJuNobksAoz0RfVctudYEAAAA&shmds=v1_AdeF8Kh84bXdOYpezjdJ6xmvg0QMtdmv7CctA2z-vQTJqAke3Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=O_zgrdd16QeeSZLFAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>Lead AI Engineer</td><td>Keywords Studios</td><td>Pune, Maharashtra, India</td><td>About the team:You will be a part of the AI team, which is responsible for developing AI features at Helpshift.The AI team has built product features like User Intent Detection, AI-Powered Answers,Agent CoPilot features, etc by building our own ML models and leveraging generative AI.The team consists of Backend and Frontend Developers, Full-stack Developers, MLEngineers and Data Scientists.About the role:We are looking for a seasoned engineer with a passion for Artificial Intelligence and a knackfor leadership. In this role, you'll be instrumental in shaping our AI products, leading a groupof engineers within the team, and driving the successful delivery of innovative solutions.You'll blend deep technical expertise in AI with strong leadership skills, guiding a team offull-stack, backend, and frontend developers. You'll be responsible for the end-to-enddelivery of AI-powered features and products, from conceptualization and design todeployment and optimization.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKOwrCQBAAUGxzBKvpBIlZEWy0shCJHxA8QJhkh92VOBN2Jqj38MBq86pXfCbF7EzoYVfDnkNiogwLOEoLSpi7CMJwEAk9TbfRbNCNc6p9FdTQUld18nDC1MrL3aXVP41GzDT0aNSs1stXNXCYlyd6PyV7hZuNPolCYriOTCVc8PdRo2UsoWaf8AtFRB7ClwAAAA&shmds=v1_AdeF8Kj6B1cePoeK-rZ__bHkWYIkydErgHmkalRQ7Y1AXJ3GZw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr><tr><td>Machine Learning Engineer</td><td>ExxonMobil</td><td>Bengaluru, Karnataka, India</td><td>About us\n",
       "\n",
       "At ExxonMobil, our vision is to lead in energy innovations that advance modern living and a net-zero future. As one of the world’s largest publicly traded energy and chemical companies, we are powered by a unique and diverse workforce fueled by the pride in what we do and what we stand for.\n",
       "\n",
       "The success of our Upstream, Product Solutions and Low Carbon Solutions businesses is the result of the talent, curiosity and drive of our people. They bring solutions every day to optimize our strategy in energy, chemicals, lubricants and lower-emissions technologies.\n",
       "\n",
       "We invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society’s evolving needs. Learn more about our What and our Why and how we can work together.\n",
       "\n",
       "What role you will play in our team\n",
       "\n",
       "We are looking to hire candidates to work on challenging technology and engineering problems that span oil and gas exploration & production, chemicals/fuels/lubricants products and low carbon solutions. A successful candidate would understand a business problem (both commercially and technically), translate it into a computational, data science or machine learning problem and apply engineering, numerical, data science and programming skills to tackle it. The Machine Learning Engineer will work as part of a team to design, develop, deploy and sustain data science solutions that are scalable, reproducible and with commercial-grade quality.\n",
       "\n",
       "What you will do\n",
       "• Applies software development practices, DevOps skills and Machine Learning (ML) techniques to orchestrate an end-to-end machine learning workflow that effectively brings ML models to production.\n",
       "• Participates in scoping of deployment of new data science solutions and implements the appropriate solution design.\n",
       "• Sustain data science solutions by enabling continuous ML model and/or service performance monitoring, training, and re-training of models, including the implementation of proactive alerting methods.\n",
       "• Works effectively with computational scientists, data scientists, engineers, software developers, and domain experts across the globe to develop and apply computational and data science solutions in support of our business.\n",
       "\n",
       "About You\n",
       "\n",
       "Skills and Qualifications\n",
       "• Bachelor’s degree from a recognized university in Computer Science, IT, Applied Mathematics, Engineering or related disciplines with minimum 7.0 CGPA or equivalent.\n",
       "• Minimum 3 years of experience in Data Science and Machine Learning or related computational domain.\n",
       "• Competent to expert level programming experience in C/C++/Python.\n",
       "• Strong foundation in application design.\n",
       "• Experience with refactoring legacy code and leveraging third-party libraries/APIs during software development.\n",
       "• Experience with Source code version control (Git), Azure Cloud platform and containers, Databricks and MLflow.\n",
       "• Continuous Integration and Continuous Deployment.\n",
       "• Familiarity with statistical analysis, regression and classification.\n",
       "\n",
       "Preferred Qualifications / Experience\n",
       "• Experience with time series analysis, computer vision, natural language processing.\n",
       "• Knowledge or hands on experience on Matlab & SQL.\n",
       "• Strong written and verbal communication skills.\n",
       "• Prior knowledge of commercial software development and/or experience in commercial software teams.\n",
       "• Familiarity with Oil and Gas Industry.\n",
       "\n",
       "Your benefits\n",
       "\n",
       "An ExxonMobil career is one designed to last. Our commitment to you runs deep our employees grow personally and professionally, with benefits built on our core categories of health, security, finance and life. We offer you:\n",
       "• Competitive compensation\n",
       "• Medical plans, maternity leave and benefits, life, accidental death and dismemberment benefits\n",
       "• Retirement benefits\n",
       "• Global networking & cross-functional opportunities\n",
       "• Annual vacations & holidays\n",
       "• Day care assistance program\n",
       "• Training and development program\n",
       "• Tuition assistance program\n",
       "• Workplace flexibility policy\n",
       "• Relocation program\n",
       "• Transportation facility\n",
       "\n",
       "Please note benefits may change from time to time without notice, subject to applicable laws. The benefits programs are based on the Company’s eligibility guidelines.\n",
       "\n",
       "Stay connected with us\n",
       "• Learn more about ExxonMobil in India, visit ExxonMobil India and Energy Factor India.\n",
       "• Follow us on LinkedIn and ExxonMobil (@exxonmobil) • Instagram photos and videos\n",
       "• Like us on Facebook\n",
       "• Subscribe our channel at YouTube\n",
       "\n",
       "EEO Statement\n",
       "\n",
       "ExxonMobil is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin or disability status.\n",
       "\n",
       "Business solicitation and recruiting scams\n",
       "\n",
       "ExxonMobil does not use recruiting or placement agencies that charge candidates an advance fee of any kind (e.g., placement fees, immigration processing fees, etc.). Follow the LINK to understand more about recruitment scams in the name of ExxonMobil.\n",
       "\n",
       "Nothing herein is intended to override the corporate separateness of local entities. Working relationships discussed herein do not necessarily represent a reporting connection, but may reflect a functional guidance, stewardship, or service relationship.\n",
       "\n",
       "Exxon Mobil Corporation has numerous affiliates, many with names that include ExxonMobil, Exxon, Esso and Mobil. For convenience and simplicity, those terms and terms like corporation, company, our, we and its are sometimes used as abbreviated references to specific affiliates or affiliate groups. Abbreviated references describing global or regional operational organizations and global or regional business lines are also sometimes used for convenience and simplicity. Similarly, ExxonMobil has business relationships with thousands of customers, suppliers, governments, and others. For convenience and simplicity, words like venture, joint venture, partnership, co-venturer, and partner are used to indicate business relationships involving common activities and interests, and those words may not indicate precise legal relationships.</td><td>https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=Gf14xxeb1PMFW9slAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNSwrCMBBAcdsjuJqtUpMiuNGdUMRPz1AmcUiicaYkKeQsnta6efDe5jXfVdMNaH1gggdh4sAOenaLU4Id3MRAXrr1IAwXERdpffKlTPmodc5RuVywBKusfLQwGan6JSb_MWaPiaaIhcb9oatqYrfd9LUKD2JChMBwJnYY5zS3cF_2WPCNLVz5GfAHD4WFHp0AAAA&shmds=v1_AdeF8KiVtfTQyhVeWOhG-JmTM6JDpzh-E2JeatkyNt7NWSzqDQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=Gf14xxeb1PMFW9slAAAAAA%3D%3D</td><td>AI/ML Engineer</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Data Engineer- Associate",
         "Federal Express Corporation AMEA",
         "Anywhere",
         "Responsible for developing, optimize, and maintaining business intelligence and data warehouse systems, ensuring secure, efficient data storage and retrieval, enabling self-service data exploration, and supporting stakeholders with insightful reporting and analysis.\n\n1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n6. Design and implement data models to organize and structure data for analytical purposes.\n7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n9. Assist in training and support to users on business intelligence tools and applications.\n10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n\nEducation: Bachelor's degree or equivalent in Computer Science, MIS, Mathematics, Statistics, or similar discipline. Master's degree or PhD preferred.\n\nRelevant work experience in data engineering based on the following number of years:\nStandard I: Two (2) years\nStandard II: Three (3) years\nSenior I: Four (4) years\nSenior II: Five (5) years\n\nKnowledge, Skills and Abilities\n• Fluency in English\n• Analytical Skills\n• Accuracy & Attention to Detail\n• Numerical Skills\n• Planning & Organizing Skills\n• Presentation Skills\n• Data Modeling and Database Design\n• ETL (Extract, Transform, Load) Skills\n• Programming Skills\n\nFedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n\nAll qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\nOur Company\n\nFedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\nOur Philosophy\n\nThe People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\nOur Culture\n\nOur culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=maoME4iLxcapPLHYAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNuw7CMAwAxdpPYPKMSoOQWGBAFZSXxMZeOa2VBgU7ijOUr-IXaZebTnfFb1GYM2aEhp1norSGWlU6j5lgDQ-xoISpG0AYriIu0PIw5Bx1b4xqqJxmzL6rOvkYYbIymrdYndHqgIlimFLtdrcZq8hudbxQTwkDNGNMpAonSVHS1JgG9bOpwTPcvrNjsS_hRQHZIWMJd-49_gFTfTOCsgAAAA&shmds=v1_AdeF8Ki1EEWj9FdGES3eD1oVB_vceQKrOD0l6VHbac6JR12j_Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=maoME4iLxcapPLHYAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Senior Data Engineer",
         "Mastercard",
         "Pune, Maharashtra, India",
         "Job Title:\n\nSenior Data Engineer\n\nOverview:\n\nPosition Overview:\n\nThe Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n\nThis role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n\nThe ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n\n1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n\nRole:\n\n• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n\nAll About You:\n\n• Strong understanding of Windows and Linux server.\n• Good understanding of SQL Server or Oracle DB.\n• Solid understanding of Essbase technology – understand how this technology works, for both BSO\nand ASO cubes.\n• Develop BSO and ASO cubes with a strong eye for performance.\n• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCQAwAUFz7CU6ZHLT2RHDRVRGFguAHlPQa7k5qUpIIfodfrC5vetVnVq3uxEUUjugIJ06FiRTWcJUejFBjBmE4i6SR5ofsPtk-BLOxSeboJTZRnkGYenmHh_T2p7OMStOITt12t3k3E6flokVz0og6QGG4vZhqaPE30bIr1nDhoeAXlqvX1JUAAAA&shmds=v1_AdeF8Kjilf5bxnv3Bc7iM6HGmbYmIZhgjXWK3XVsMCSyUs2b_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "AWS Data Engineer",
         "The Value Maximizer",
         "Madhavaram, Telangana, India",
         "Job Title: AWS Data Engineer Location: Gurgaon / Hyderabad Experience: 1.6 - 6 yearsAbout the Role: We are seeking a skilled AWS Data Engineer with strong expertise in data engineering tools and cloud technologies. The ideal candidate should have hands-on experience in Snowflake, Python, and SQL, with exposure to AWS Glue. Candidates with experience in DBT, Redshift, and Lambda will have an added advantage.Key Responsibilities:\n• Design, develop, and maintain scalable data pipelines and ETL workflows on AWS\n• Work extensively with Snowflake, Python, and SQL to process, transform, and analyze data\n• Optimize and maintain existing data architectures to ensure high performance and cost efficiency\n• Integrate data from multiple sources into a unified data warehouse\n• Collaborate with data analysts, data scientists, and business stakeholders to meet data requirements\n• Implement data governance, quality checks, and best practices in data engineering\n• Leverage AWS services such as Redshift and Lambda for data processing tasks\n• Use DBT (if applicable) for data modeling and transformation in the ELT framework\n• Troubleshoot data pipeline issues and perform root cause analysis\n\nRequired Skills (Mandatory):\n• Snowflake - Data warehouse development, performance tuning, and query optimization\n• Python - Scripting for automation and data processing\n• SQL - Advanced query writing and optimization skills\n• AWS Glue\n\nPreferred / Optional Skills:\n• DBT - Data transformation and modeling\n• AWS Redshift - Data warehousing and analytics\n• AWS Lambda - Serverless data processing\n\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field\n• 1.6 - 6 years of hands-on experience in data engineering roles\n• Strong problem-solving skills and ability to work in fast-paced environments\n• Good communication and collaboration skills",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=yiNOHEMeOuiOnDinAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBCAYVz7CE43itRGFBedBEUUnCw6lmt7JJH0ruSiFJ_Fh7UuP3zTn30n2Wz_uMEBE8KRrWeiCAu4SA1KGBsHwnASsYGmO5dSr1tjVENhNWHyTdFIZ4SplsE8pdZ_KnUYqQ-YqFptlkPRs52vS0dwx_AiuOLgO_8ZR55HtA7fGLHLoaSAbJExhzO3Hn_YDysOnwAAAA&shmds=v1_AdeF8KjzDld6mLRFYpwvRbl80Cf9scknY3E5o_gVqfWYnAcvXA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=yiNOHEMeOuiOnDinAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Senior Data Engineer - Azure",
         "EPAM Systems",
         "Chennai, Tamil Nadu, India",
         "EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n\nWe are seeking a skilled and detail-oriented Azure Data Engineer to join our data engineering team. The ideal candidate will have hands-on experience with Azure Data Factory, Azure Synapse or Microsoft Fabric, and strong programming skills in Python or PySpark. Proficiency in SQL and experience in designing and maintaining modern data pipelines and data lakes is essential.\n\nRESPONSIBILITIES\n• Design, build, and manage scalable data pipelines using Azure Data Factory (ADF)\n• Develop and optimize data transformations using Azure Synapse Analytics or Microsoft Fabric\n• Write efficient and reusable code in Python or PySpark for data wrangling and processing\n• Create, maintain, and optimize complex SQL queries and stored procedures\n• Collaborate with data scientists, analysts, and business stakeholders to understand data needs\n• Ensure data accuracy, consistency, and security across pipelines and storage layers\n• Monitor pipeline performance and troubleshoot data issues in production environments\n• Participate in architectural discussions and propose scalable, secure data solutions on Azure\n\nREQUIREMENTS\n• Bachelor's degree in Computer Science, Information Technology, or a related field\n• 3-7 years of experience in data engineering or related roles\n• Strong experience with Azure Data Factory (ADF) for ETL/ELT workflows\n• Proficiency in Azure Synapse Analytics and/or Microsoft Fabric\n• Solid coding skills in Python or PySpark for data processing and automation\n• Advanced knowledge of SQL – including writing, optimizing, and troubleshooting queries\n• Experience with data modeling, data lakes, and data warehouse concepts\n• Good communication skills and the ability to work in a collaborative, agile team environment\n• Ability to communicate effectively in both written and spoken English (B2 level and higher)\n\nNICE TO HAVE\n• Azure certifications (e.g., Azure Data Engineer Associate or Azure Fundamentals)\n• Familiarity with DevOps practices and CI/CD pipelines in data projects\n• Experience with Azure DevOps, Git, or other version control tools\n• Understanding of data governance, security, and compliance within Azure\n\nWE OFFER\n• Opportunity to work on technical challenges that may impact across geographies\n• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n• Opportunity to share your ideas on international platforms\n• Sponsored Tech Talks & Hackathons\n• Unlimited access to LinkedIn learning solutions\n• Possibility to relocate to any EPAM office for short and long-term projects\n• Focused individual development\n• Benefit package\n• Health benefits\n• Retirement benefits\n• Paid time off\n• Flexible benefits\n• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ONHPfHeGGN8tjaXVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFwQqCQBAAULr6CZ3mLOZG0aVOUhIFRWB3GXVwN9YZ2dnA-pi-tbq8l3xmyboidhLggBGh5N4xUYAFFO9noN9naUAJQ2tBGI4ivaf5zsY46tYYVZ_3GjG6Nm9lMMLUyGQe0uifWi0GGj1Gqleb5ZSP3KdpeSsuUL000qDgGPaWmNFlcMfBebhi98zgxJ3DL2PuWH-hAAAA&shmds=v1_AdeF8KhzU6NKnB_yH5r4Hg-94JiVes164BbSavVIIbnamHK6SA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ONHPfHeGGN8tjaXVAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Data Engineer - Periscope",
         "McKinsey & Company",
         "Gurugram, Haryana, India (+1 other)",
         "Your Growth Driving lasting impact and building long-term capabilities with our clients is not easy work. You are the kind of person who thrives in a high performance/high reward culture - doing hard things, picking yourself up when you stumble, and having the resilience to try another way forward.\n\nIn return for your drive, determination, and curiosity, we'll provide the resources, mentorship, and opportunities you need to become a stronger leader faster than you ever thought possible. Your colleagues—at all levels—will invest deeply in your development, just as much as they invest in delivering exceptional results for clients. Every day, you'll receive apprenticeship, coaching, and exposure that will accelerate your growth in ways you won’t find anywhere else.\n\nWhen you join us, you will have:\n• Continuous learning: Our learning and apprenticeship culture, backed by structured programs, is all about helping you grow while creating an environment where feedback is clear, actionable, and focused on your development. The real magic happens when you take the input from others to heart and embrace the fast-paced learning experience, owning your journey.\n• A voice that matters: From day one, we value your ideas and contributions. You’ll make a tangible impact by offering innovative ideas and practical solutions. We not only encourage diverse perspectives, but they are critical in driving us toward the best possible outcomes.\n• Global community: With colleagues across 65+ countries and over 100 different nationalities, our firm’s diversity fuels creativity and helps us come up with the best solutions for our clients. Plus, you’ll have the opportunity to learn from exceptional colleagues with diverse backgrounds and experiences.\n• World-class benefits: On top of a competitive salary (based on your location, experience, and skills), we provide a comprehensive benefits package to enable holistic well-being for you and your family.\n\nYour ImpactYou will be part of the data delivery team and will have the opportunity to develop a deep understanding of the domain/function.\n\nYou will design and drive the work plan for the optimization/automation and standardization of the processes incorporating best practices to achieve efficiency gains.\n\nYou will run data engineering pipelines, link raw client data with data model, conduct data assessment, perform data quality checks, and transform data using ETL tools.\n\nYou will perform data transformations, modeling, and validation activities, as well as configure applications to the client context. You will also develop scripts to validate, transform, and load raw data using programming languages such as Python/PySpark/SparkSQL. You’ll also be required to analyze and visualize data using Power BI to provide insights to support business decisions.\n\nIn this role, you will determine database structural requirements by analyzing client operations, applications, and programming.\n\nGiven the pace of this role, you will develop cross-site relationships to enhance idea generation, and manage stakeholders. Lastly, you will collaborate with the team to support ongoing business processes by delivering high-quality end products on-time and perform quality checks wherever required.\n\nYou’ll work with our Periscope team in our Gurgaon and Bangalore, India office. Periscope is the technology backbone of McKinsey’s Growth, Marketing & Sales Practice. Founded in 2007, it combines world-leading Intellectual Property, prescriptive analytics, and cloud-based tools, with expert support and training. This unique combination drives revenue growth – now, and in the future. The platform offers a suite of Growth, Marketing & Sales solutions that accelerate and sustain commercial transformation for businesses.\n\nThe Growth, Marketing & Sales Practice strives to help clients in both consumer and business-to-business environments on a wide variety of marketing and sales topics. Our clients benefit from our experience in core areas of sales and marketing topics such as sales and channel management, branding, customer insights, marketing ROI, digital marketing, CLM, and pricing. Our Practice offers an exceptional opportunity to work at the intersection of sales, marketing, and consulting. Focusing on issues like redefining sales and marketing operations and commercial transformation, our people help clients build capabilities and transform how companies go to market-moving them to customer-centric organizations.\n\nPeriscope leverages its world-leading IP (largely from McKinsey but also other partners) and best-in-class technology to enable transparency into Big Data, create actionable insights, and new ways of working that drive lasting performance improvement, and typically sustain a 2-7% increase in return on sales (ROS). With a truly global reach, the portfolio of solutions is comprised of: Marketing Solutions, Customer Experience Solutions, Category Solutions, B2C Pricing Solutions, B2B Pricing Solutions, and Sales Solutions. These are complemented by ongoing client service and custom capability building programs.\n\nPeriscope has a presence in 27 locations across 16 countries with a team of 800+ IT and business professionals and a network of 300+ experts. To learn more about how Periscope’s solutions and experts are helping businesses continually drive better performance, visit www.mckinsey.com/periscope\n\nYour qualifications and skills\n• Bachelor’s degree/master's degree with high rankings, with 3+ years of professional work experience\n• Ability to design and drive the work plan for the optimization/automation and standardization of the processes incorporating best practices to achieve efficiency gains.\n• Expertise to run data engineering pipelines, link raw client data with data model, conduct data assessment, perform data quality checks, and transform data using ETL tools.\n• Perform data transformations, modeling, and validation activities, as well as configure applications to the client context. Develop scripts to validate, transform, and load raw data using programming languages such as Python/PySpark/SparkSQL. Analyze and visualize data using Power BI to provide insights to support business decisions.\n• Determine database structural requirements by analyzing client operations, applications, and programming..\n• Collaborate with the team to support ongoing business processes by delivering high-quality end products on-time and perform quality checks wherever required.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Tj0hmhPFvTY9OVviAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMTQrCMBBAYdz2CK5m5UJqUgQ3ulTxD8EblGkc0kg7EzIptKfxqtbN41u94rsoqhNmhDP7wEQJNvCiFNRJpNl3aUAJk2tBGC4ivqPloc056t5a1c54zZiDM056K0yNjPYjjf5Ta4uJYoeZ6u2uGk1kvzZP9wisNMEKjtJH5AnCvB7S4BP2JVwxTchYwo3fAX_kGvoWogAAAA&shmds=v1_AdeF8Kh_qZOoWsCGwx3jd10Ic3QMbDA4ou9RxXvWCM0D8cP6mQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Tj0hmhPFvTY9OVviAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Engineering Manager - Data Engineer",
         "Motive",
         "Anywhere",
         "Who we are:\n\nMotive empowers the people who run physical operations with tools to make their work safer, more productive, and more profitable. For the first time ever, safety, operations and finance teams can manage their drivers, vehicles, equipment, and fleet related spend in a single system. Combined with industry leading AI, the Motive platform gives you complete visibility and control, and significantly reduces manual workloads by automating and simplifying tasks.\n\nMotive serves more than 100,000 customers – from Fortune 500 enterprises to small businesses – across a wide range of industries, including transportation and logistics, construction, energy, field service, manufacturing, agriculture, food and beverage, retail, and the public sector.\n\nVisit gomotive.com to learn more.\n\nAbout the Role:\n\nAs a Data Engineering Manager you will be part of the core data team building out world class data products that will be in front of our largest customers. You will be working on the intersection of all data streams in the company from our IOT data consuming 100s of thousands of data points per minute to our user data. You will partner closely with both the Product, Engineering, as well as the Strategic Analytics teams. We are seeking strong team players who thrive on innovation and continuous improvement. We pride ourselves on our culture, and ability to work effectively across a highly diversified team.\nWhat You'll Do:\n• Build data roadmap for the data engineering team building both internal and external data products.\n• Drive innovation in our data products with the application of AI in both the processes we work on to the products we deliver.\n• Build teams to scale out our maturing set of analytics products.\n• Be deeply technical and continue to code and drive technical standards within the team.\n• Architect and design data models in collaboration with data and product teams\n• Communicate effectively and drive strategy across multiple teams and projects.\nWhat We're Looking For:\n• Bachelor's degree or higher in a quantitative field, e.g. Computer Science, Math, Economics, or Statistics\n• 10+ years experience in Data Engineering, and 3+ years managing a team\n• Expertise with data engineering stack, dBt, Snowflake, airflow, data observability tools, AWS, pyspark.\n• Expertise in SQL and Python\n• Great planning and roadmapping skills\n• Excellent communication, collaboration, and people skills\n\nCreating a diverse and inclusive workplace is one of Motive's core values. We are an equal opportunity employer and welcome people of different backgrounds, experiences, abilities and perspectives.\n\nPlease review our Candidate Privacy Notice here .\n\nUK Candidate Privacy Notice here.\n\nThe applicant must be authorized to receive and access those commodities and technologies controlled under U.S. Export Administration Regulations. It is Motive's policy to require that employees be authorized to receive access to Motive products and technology.\n\n#LI-Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Zx0eSLYQ3BIkxjrfAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zXJsQoCMQyAYVxvcXfKLNiK4HKuiijcMxxpDW3lLilNkHsGn1odXH74-Lv3qusvnAoTtcIJBmRM1GAHZzSE__r6LgGUsMUMwnAVSRNtTtmsau-96uSSGlqJLsrshSnI4p8S9JdRMzaqExqNh-N-cZXTdj2IlRdBYbjxo-AHsH6x5Y0AAAA&shmds=v1_AdeF8KjHMtnogMcO6BGQQWTzxgKuOWuTRWGvKC3hl5EHa29uXA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Zx0eSLYQ3BIkxjrfAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Sr. Data Engineer - Tietoevry create (m/f/d)",
         "Tietoevry",
         "Pune, Maharashtra, India",
         "Job Description\n\nAs an Data Engineer, you'll be responsible for designing, implementing, and managing data solutions on the Azure platform. Here are some common use cases and examples of tasks performed by Azure Data Engineers:\n\nData Ingestion & Integration: Use tools like Azure Data Factory to extract, transform, and load (ETL) data from sources such as databases, files, APIs, and streams into Azure.\n\nData Transformation & Processing: Prepare data for analysis using platforms like Azure Databricks or Synapse Analytics with Apache Spark.\n\nData Storage & Management: Choose appropriate storage solutions—Azure SQL Database or Cosmos DB for structured data, Data Lake Storage for big data, and Blob Storage for files.\n\nData Warehousing: Implement scalable analytics with Azure Synapse Analytics for large datasets.\n\nData Modeling & Analysis: Collaborate on data models using Azure Analysis Services or Databricks to support analytics.\n\nReal-time Processing: Use Azure Stream Analytics to process and analyze live data from sources like IoT or social media.\n\nGovernance & Security: Ensure data security and compliance through access controls, encryption, monitoring, and retention policies.\n\nDevOps & Automation: Automate pipelines and deployments using Azure DevOps, Monitor, and Automation tools.\n\nAdditional Information\n\nAt Tietoevry, we believe in the power of diversity, equity, and inclusion. We encourage applicants of all backgrounds, genders (m/f/d), and walks of life to join our team, as we believe that this fosters an inspiring workplace and fuels innovation. Our commitment to openness, trust, and diversity is at the heart of our mission to create digital futures that benefit businesses, societies, and humanity.\n\nDiversity, equity and inclusion (tietoevry.com)",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=biY8iWwr_GPeA2AFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WNMQrCQBBFsc0RrKYRVGJWBBstVURBELSXyWbcXUlmws4q8Uje0ljZ_OY93s8-g2xziQVsMSHs2AUmijCDa6Ak9IpvsJEwEYwbczfVpEdHKUEJo_UgDHsRV9Nw7VNqdWWMal04TZiCLaw0RphK6cxDSv3NTT1Gaus-eVss513RspuO_m-B4fxkyuGEvYjqU8QcDlwF_ALoF33trAAAAA&shmds=v1_AdeF8Kif_rKEpiYN9VgkSQxKz-DeMLFr-tyhftLA4r1zLF3LoA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=biY8iWwr_GPeA2AFAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "BI & Data Warehouse Data Engineer",
         "Astellas Pharma",
         "Bengaluru, Karnataka, India",
         "As part of the Astellas commitment to delivering value for our patients, our organization is currently undergoing transformation to achieve this critical goal. This is an opportunity to work on digital transformation and make a real impact within a company dedicated to improving lives.\n\nDigitalX our new information technology function is spearheading this value driven transformation across Astellas. We are looking for people who excel in embracing change, manage technical challenges and have exceptional communication skills.\n\nPurpose and Scope:\n\nAs a Junior Data Engineer, you will play a crucial role in assisting in the design, build, and maintenance of our data infrastructure focusing on BI and DWH capabilities. Working with the Senior Data Engineer, your foundational expertise in BI, Databricks, PySpark, SQL, Talend and other related technologies, will be instrumental in driving data-driven decision-making across the organization. You will play a pivotal role in building maintaining and enhancing our systems across the organization. This is a fantastic global opportunity to use your proven agile delivery skills across a diverse range of initiatives, utilize your development skills, and contribute to the continuous improvement/delivery of critical IT solutions.\n\nThis position is based in Bangalore, India. We recognize the importance of work/life balance & believe in optimizing the most productive work environment for all employees to succeed and deliver.\n\nEssential Job Responsibilities:\n• Collaborate with FoundationX Engineers to design and maintain scalable data systems.\n• Assist in building robust infrastructure using technologies like PowerBI, Qlik or alternative, Databricks, PySpark, and SQL.\n• Contribute to ensuring system reliability by incorporating accurate business-driving data.\n• Gain experience in BI engineering through hands-on projects.\n\nData Modelling and Integration:\n• Collaborate with cross-functional teams to analyze requirements and create technical designs, data models, and migration strategies.\n• Design, build, and maintain physical databases, dimensional data models, and ETL processes specific to pharmaceutical data.\n\nCloud Expertise:\n• Evaluate and influence the selection of cloud-based technologies such as Azure, AWS, or Google Cloud.\n• Implement data warehousing solutions in a cloud environment, ensuring scalability and security.\n\nBI Expertise:\n• Leverage and create PowerBI, Qlik or equivalent technology for data visualization, dashboards, and self-service analytics.\n\nData Pipeline Development:\n• Design, build, and optimize data pipelines using Databricks and PySpark. Ensure data quality, reliability, and scalability.\n• Application Transition: Support the migration of internal applications to Databricks (or equivalent) based solutions. Collaborate with application teams to ensure a seamless transition.\n• Mentorship and Leadership: Lead and mentor junior data engineers. Share best practices, provide technical guidance, and foster a culture of continuous learning.\n• Data Strategy Contribution: Contribute to the organization’s data strategy by identifying opportunities for data-driven insights and improvements.\n• Participate in smaller focused mission teams to deliver value driven solutions aligned to our global and bold move priority initiatives and beyond.\n• Design, develop and implement robust and scalable data analytics using modern technologies.\n• Collaborate with cross functional teams and practices across the organization including Commercial, Manufacturing, Medical, DataX, GrowthX and support other X (transformation) Hubs and Practices as appropriate, to understand user needs and translate them into technical solutions.\n• Provide Technical Support to internal users troubleshooting complex issues and ensuring system uptime as soon as possible.\n• Champion continuous improvement initiatives identifying opportunities to optimize performance security and maintainability of existing data and platform architecture and other technology investments.\n• Participate in the continuous delivery pipeline. Adhering to DevOps best practices for version control automation and deployment. Ensuring effective management of the FoundationX backlog.\n• Leverage your knowledge of data engineering principles to integrate with existing data pipelines and explore new possibilities for data utilization.\n• Stay-up to date on the latest trends and technologies in data engineering and cloud platforms.\n\nQualifications:\n\nRequired\n• Bachelor's degree in computer science, Information Technology, or related field (master’s preferred) or equivalent experience\n• 1-3+ years of experience in data engineering with a strong understanding of BI technologies, PySpark and SQL, building data pipelines and optimization.\n• 1-3 +years + experience in data engineering and integration tools (e.g., Databricks, Change Data Capture)\n• 1-3+ years + experience of utilizing cloud platforms (AWS, Azure, GCP). A deeper understanding/certification of AWS and Azure is considered a plus.\n• Experience with relational and non-relational databases.\n• Any relevant cloud-based integration certification at foundational level or above. (Any QLIK or BI certification, AWS certified DevOps engineer, AWS Certified Developer, Any Microsoft Certified Azure qualification, Proficient in RESTful APIs, AWS, CDMP, MDM, DBA, SQL, SAP, TOGAF, API, CISSP, VCP or any relevant certification)\n• Experience in MuleSoft (Anypoint platform, its components, Designing and managing API-led connectivity solutions).\n• Experience in AWS (environment, services and tools), developing code in at least one high level programming language.\n• Experience with continuous integration and continuous delivery (CI/CD) methodologies and tools\n• Experience with Azure services related to computing, networking, storage, and security\n• Understanding of cloud integration patterns and Azure integration services such as Logic Apps, Service Bus, and API Management\n\nPreferred\n• Subject Matter Expertise: possess a strong understanding of data architecture/ engineering/operations/ reporting within Life Sciences/ Pharma industry across Commercial, Manufacturing and Medical domains.\n• Other complex and highly regulated industry experience will be considered across diverse areas like Commercial, Manufacturing and Medical.\n• Data Analysis and Automation Skills: Proficient in identifying, standardizing, and automating critical reporting metrics and modelling tools\n• Analytical Thinking: Demonstrated ability to lead ad hoc analyses, identify performance gaps, and foster a culture of continuous improvement.\n• Technical Proficiency: Strong coding skills in SQL, R, and/or Python, coupled with expertise in machine learning techniques, statistical analysis, and data visualization.\n• Agile Champion: Adherence to DevOps principles and a proven track record with CI/CD pipelines for continuous delivery.\n\nWorking Environment\n\nAt Astellas we recognize the importance of work/life balance, and we are proud to offer a hybrid working solution allowing time to connect with colleagues at the office with the flexibility to also work from home. We believe this will optimize the most productive work environment for all employees to succeed and deliver. Hybrid work from certain locations may be permitted in accordance with Astellas’ Responsible Flexibility Guidelines.\n\n#LI-CH1\n\nCategory FoundationX\n\nAstellas is committed to equality of opportunity in all aspects of employment.\n\nEOE including Disability/Protected Veterans",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=iZaPxzI9k0CsjEDNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOMQrCQBBFsc0RrKaykLgrgiBaGRSJNnaWYRKH3ehmJuxsIDfymkZsPjwePH72mWW7ooQFnDAhPDCSl0Hpj2d2LRNFWMFValDC2HgQhouICzQ_-JR63VurGozThKltTCOdFaZaRvuSWn9TqZ_CfcBE1Wa7Hk3PbmmOmigEVLhPtkNoGQpih2GIQw43jDx9eGMOJT9b_AKj6d-1qgAAAA&shmds=v1_AdeF8Kjlka4T_7a_vBovx_2W3X9NLKJ77Uxg_w9551cIGkplvg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=iZaPxzI9k0CsjEDNAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Exciting Career Opportunity for Azure Data Engineer",
         "Zensar Technologies",
         "Pune, Maharashtra, India",
         "Hi All,\n\nThis is an exciting career opportunity for Azure Data Engineer position. Please find the below details. If you are interested, please share your resume to jeyaramya.rajendran@zensar.com\n\nExperience - 6 to 9years\n\nNotice - Immediate or max 10days\n\nLocation - all\n• Creates and maintains highly scalable data pipelines across Azure Data Lake Storage, and Azure Data Factory, Databricks and Apache Spark/Scala.\n• Proficient in SQL, Python and awareness about AI capabilities.\n• Responsible for managing a growing cloud-based data ecosystem and the reliability of our corporate data lake and analytics data mart.\n• Contributes to the continued evolution of Corporate Analytics Platform and Integrated data model.\n• Be part of the Data Engineering team in all phases of work including analysis, design and architecture to develop and implement cutting-edge solutions.\n• Influences changes outside of the team that continuously shape and improve the Data strategy.\n• Develop data acquisition and ingestion processes.\n• Identify automation opportunities through data analysis, advanced data modeling and optimizations.\n• Collaborates with Azure Cloud Architects and Data Platform Engineers on enterprise solutions",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=pEJNY9qV63s5V4UcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQ4BQRBAo_UJqqmFW5FoqAQREqFQaWRujd2VM7PZmUuOT_N1aF71Xl7_0-vvN51PljjACgtRgWPOUqzlZC-4S4Hluy0EazSEDYfEf2cMe6lBCYuPIAxbkdDQYBHNss6dU22qoIaWfOXl6YSpls49pNY_rhp_r9yg0XU6m3RV5jB0F2LFAmfykaWRkEghMZxaphEc8JegRis4gh3fEn4BRVKBCL0AAAA&shmds=v1_AdeF8KhKqdNrh_q2pblM5bv29dbIMONPJNP2B4AcANI3B1MYnQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=pEJNY9qV63s5V4UcAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Data Engineer Associate",
         "MSCI",
         "Pune, Maharashtra, India",
         "Your Team Responsibilities\n\nWe are hiring an Associate Data Engineer to support our core data pipeline development efforts and gain hands-on experience with industry-grade tools like PySpark, Databricks, and cloud-based data warehouses. The ideal candidate is curious, detail-oriented, and eager to learn from senior engineers while contributing to the development and operationalization of critical data workflows.\n\nYour Key Responsibilities\n• Assist in the development and maintenance of ETL/ELT pipelines using PySpark and Databricks under senior guidance.\n• Support data ingestion, validation, and transformation tasks across Rating Modernization and Regulatory programs.\n• Collaborate with team members to gather requirements and document technical solutions.\n• Perform unit testing, data quality checks, and process monitoring activities.\n• Contribute to the creation of stored procedures, functions, and views.\n• Support troubleshooting of pipeline errors and validation issues.\n\nYour skills and experience that will help you excel\n• Bachelor’s degree in Computer Science, Engineering, or related discipline.\n• 3+ years of experience in data engineering or internships in data/analytics teams.\n• Working knowledge of Python, SQL, and ideally PySpark.\n• Understanding of cloud data platforms (Databricks, BigQuery, Azure/GCP).\n• Strong problem-solving skills and eagerness to learn distributed data processing.\n• Good verbal and written communication skills.\n\nAbout MSCI\n\nWhat we offer you\n• Transparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\n• Flexible working arrangements, advanced technology, and collaborative workspaces.\n• A culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\n• A global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\n• Global Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\n• Multi-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\n• We actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\n\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\n\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\n\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n\nTo all recruitment agencies\n\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n\nNote on recruitment scams\n\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=CI1G6CZqIzIL-e9VAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMQQrCQAxAUdz2CK6yljojghtdiYpUKAgeoKRjmBmpSZlE6C28snXzV49ffReVO6MhXDhmJipwVJWQ0QjWcJMelLCEBMJwFYkDLQ_JbNS996qDi2poObggby9MvUz-Jb3-02nCQuMwr7rtbjO5keMK2sepgcxw_zDV0OJsUJMVrKHhZ8YfNitOj5IAAAA&shmds=v1_AdeF8KjI9Bc8G-NbjISf7S80W10CEjEWSwl2jV_KohBs5bZoJg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=CI1G6CZqIzIL-e9VAAAAAA%3D%3D",
         "Data Engineer"
        ],
        [
         "Python Developer",
         "5100 Kyndryl Solutions Private Limited",
         "Anywhere",
         "Who We Are At Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities. The Role Are you passionate about solving complex problems? Do you thrive in a fast-paced environment? Then there’s a good chance you will love being a part of our Software Engineering – Development team at Kyndryl, where you will be able to see the immediate value of your work. As a Software Engineering - Developer at Kyndryl, you will be at the forefront of designing, developing, and implementing cutting-edge software solutions. Your work will play a critical role in our business offering, your code will deliver value to our customers faster than ever before, and your attention to detail and commitment to quality will be critical in ensuring the success of our products. Using design documentation and functional programming specifications, you will be responsible for implementing identified components. You will ensure that implemented components are appropriately documented, unit-tested, and ready for integration into the final product. You will have the opportunity to architect the solution, test the code, and deploy and build a CI/CD pipeline for it. As a valued member of our team, you will provide work estimates for assigned development work, and guide features, functional objectives, or technologies being built for interested parties. Your contributions will have a significant impact on our products' success, and you will be part of a team that is passionate about innovation, creativity, and excellence. Above all else, you will have the freedom to drive innovation and take ownership of your work while honing your problem-solving, collaboration, and automation skills. Together, we can make a difference in the world of cloud-based managed services. Your Future at Kyndryl The career path ahead is full of exciting opportunities to grow and advance within the job family. With dedication and hard work, you can climb the ladder to higher bands, achieving coveted positions such as Principal Engineer or Vice President of Software. These roles not only offer the chance to inspire and innovate, but also bring with them a sense of pride and accomplishment for having reached the pinnacle of your career in the software industry. Who You Are You’re good at what you do and possess the required experience to prove it. However, equally as important – you have a growth mindset; keen to drive your own personal and professional development. You are customer-focused – someone who prioritizes customer success in their work. And finally, you’re open and borderless – naturally inclusive in how you work with others. Required Technical and Professional Experience • 6 years of experience working as a software engineer on complex software projects •Excellent coding skills and solid development experience (Java, Python, .Net etc.) with debugging and problem-solving skills •Software development methodologies, with demonstrated experience developing scalable and robust software •Experienced in relational and NoSQL databases, data mapping, XML/JSON, Rest based web services •Knowledge of architecture design - Microservices architecture, containers (Docker & k8s), messaging queues •Deep understanding of OOP and Design patterns Preferred Technical and Professional Experience •Bachelor's degree in Computer Science, related technical field, or equivalent practical experience •Certification in one or more of the hyperscalers (Azure, AWS, and Google GCP) - otherwise, you can obtain certifications with Kyndryl •Experience with DevOps tools and modern engineering practices Being You Diversity is a whole lot more than what we look like or where we come from, it’s how we think and who we are. We welcome people of all cultures, backgrounds, and experiences. But we’re not doing it single-handily: Our Kyndryl Inclusion Networks are only one of many ways we create a workplace where all Kyndryls can find and provide support and advice. This dedication to welcoming everyone into our company means that Kyndryl gives you – and everyone next to you – the ability to bring your whole self to work, individually and collectively, and support the activation of our equitable culture. That’s the Kyndryl Way. What You Can Expect With state-of-the-art resources and Fortune 100 clients, every day is an opportunity to innovate, build new capabilities, new relationships, new processes, and new value. Kyndryl cares about your well-being and prides itself on offering benefits that give you choice, reflect the diversity of our employees and support you and your family through the moments that matter – wherever you are in your life journey. Our employee learning programs give you access to the best learning in the industry to receive certifications, including Microsoft, Google, Amazon, Skillsoft, and many more. Through our company-wide volunteering and giving platform, you can donate, start fundraisers, volunteer, and search over 2 million non-profit organizations. At Kyndryl, we invest heavily in you, we want you to succeed so that together, we will all succeed. Get Referred! If you know someone that works at Kyndryl, when asked ‘How Did You Hear About Us’ during the application process, select ‘Employee Referral’ and enter your contact's Kyndryl email address. We’re glad you’re here. Take a look around at the many exciting career opportunities we have available and apply today! Can’t find a suitable job opening? Drop off your CV/Resume Drop off your CV/Resume and a Recruiter will reach out with related career information that match your experience and expertise. Sign up for Job Alerts Create your account and then sign up for job alerts. When new jobs become available that meet your criteria, you’ll be alerted right away! At Kyndryl, we achieve progress the world depends on, with purpose. Beginning with the purpose that matters to you. Because here, you will be part of a culture designed with purpose. One that is restless, empathetic and devoted. Where we are committed to sustainable progress for our customers and supporting the communities where we work and live. All of you is what we want. And what we need. Join us, and together, we can advance the vital systems that power human progress.",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=rHEEcyrZb39YgP9UAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPQrCQBBAYWxzBKvpBMFsFNJoK4g_RcADhE0yZCdsdpadMSQn8brG5jXfy76bbFct6jjAFSf0HDHBAR7cgKBNrYNVbsy9x-3FqUY5GyPi817UKrV5y6PhgA3PZuBG_qnF2YTRW8X6VBZzHkO_N-WxKOC5hC4tHt7sP0ocBKpE0zrCi0ZS7IAC3ENH9geToaqimgAAAA&shmds=v1_AdeF8KirfgksVQetlCoS78oYAd3u6gmaUJwnC7CS5foNoSOvvA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=rHEEcyrZb39YgP9UAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Software Developer- Python",
         "BNP Paribas India Solutions",
         "India",
         "About BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function:\n\nThe Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n\nThe IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n\nJob Title:\n\nPython Developer\n\nDate:\n\nJune-25\n\nDepartment:\n\nITG- Fresh\n\nLocation:\n\nChennai, Mumbai\n\nBusiness Line / Function:\n\nFinance Dedicated Solutions\n\nReports to:\n\n(Direct)\n\nGrade:\n\n(if applicable)\n\n(Functional)\n\nNumber of Direct Reports:\n\nNA\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nThe Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n\nA strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n\nResponsibilities\n\nDirect Responsibilities\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\nTechnical & Behavioral Competencies\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n\n- Good analytical, problem solving, & communication skills\n\n- Engage in technical discussions and to help in improving the system, process etc\n\nNice to Have\n\n- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n\n- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n\n- Familiarity with JavaScript, CSS, and HTML.\n\n- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n\n- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\nSpecific Qualifications (if required)\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nAbility to deliver / Results driven\n\nCommunication skills - oral & written\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to develop and adapt a process\n\nAbility to understand, explain and support change\n\nAbility to develop others & improve their skills\n\nChoose an item.\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 5 years\n\nOther/Specific Qualifications (if required)",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMMQrCQBAAsc0TrLawEpJTwUY7EUQLCeQBYS-uuZPz9rhdNX7E9xqxGZgppvhMimXDV31hJtjTkwInyiXUb3UcoYQTWxDC3DkY_cDcB5punWqSjTEioepFUX1XdXw3HMnyYG5s5YdW3PhNAZXa1XoxVCn289nuXEON2VsUOMaLR2g4PNRzFPDxn74MpNGLmQAAAA&shmds=v1_AdeF8KhddRLnKmMGyyu5XSrgUY0KL77JpalABEFKQDRF04WXHg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Python Developer / Data Analyst - +3 years of experience - Full remote - Contractor in USD",
         "All European Careers",
         "Anywhere",
         "For an international project in Chennai, we are urgently looking for a Full Remote Data Analyst. The ideal candidate will have good understanding of Data integration, Data cleanup for Knowledge 360 and other related products for knowledge system..\n\nWe are looking for a motivated contractor. Candidates need to be fluent in English.\n\nTasks and responsibilities:\n\nDesign and implement cloud-based solutions using Azure and Google Cloud platforms. Automate data pipelines to facilitate efficient data flow and processing;\n\nDevelop and manage data pipelines using technologies like PySpark and Databricks;\n\nEnsure the scalability and reliability of data processing workflows;\n\nWork with Large Language Models (LLMs) such as GPT and OpenAI models to enhance applications with natural language processing (NLP) capabilities;\n\nDesign and implement prompt engineering strategies to optimize model performance in business applications;\n\nTrain, fine-tune, and deploy AI models, analyzing their performance based on real-world results;\n\nOptimize models using feedback mechanisms to improve efficiency;\n\nUtilize strong communication skills to convey complex technical concepts to non-technical stakeholders;\n\nProfile:\n• Bachelor or Master degree;\n• +3 years of hands-on experience in Python, Pyspark, Databricks, and cloud platforms like Azure and Google Cloud;\n• Experience in designing and implementing cloud-based solutions and automating data pipelines;\n• Experience in Working with Large Language Models (LLMs) such as GPT, OpenAI models, or enhance solutions with natural language processing (NLP) capabilities;\n• Proven ability in metadata management and schema mapping;\n• Cloud certifications (Azure, Google Cloud, or similar) are a plus;\n• Fluent in English;",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=vaNhBJEE36JvMI7HAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOwWrDQAxE6dWfUAjo3BJvaemlOYU4CempUHo28kaxHdaSs5KL_ZX9pSoXwWhmeFP8PRS3r8U6Yajol5KMlCFAhYawZUyLGqzh-Q0WwqwgF6DZIz1xJDcOU0qQaRC7q52wZYwmGXqGn-_Kf5_SgHo3duCMo0ib6HHTmY36EYJqKls1tD6WUYYgTI3M4SqN3k-tHWYaExrVr-8vczly-7TaOnM_ZZ-KDDsPkC9z4InPPf4DndEiodIAAAA&shmds=v1_AdeF8KjvxGq59iD_AXuAUm5WuiRATlMHUuMIIWzMPqMWPAo0zA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=vaNhBJEE36JvMI7HAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Azure + Python Developer-10 yrs -Remote -Contract",
         "SAPLING INFOSYSTEMS",
         "Anywhere",
         "Key Responsibilities:\n• Design, develop, and optimize applications using Python and Azure cloud services.\n• Architect and implement scalable cloud solutions leveraging Azure services (App Services, Functions, Storage, Cosmos DB, Event Hub, Service Bus, etc.).\n• Develop APIs, microservices, and integration solutions for enterprise systems.\n• Collaborate with cross-functional teams to gather requirements and deliver end-to-end solutions.\n• Ensure code quality, security, and performance best practices.\n• Implement CI/CD pipelines using Azure DevOps.\n• Perform system troubleshooting, optimization, and performance tuning.\n• Mentor junior developers and contribute to technical best practices across the team.\n\nRequired Skills & Qualifications:\n• 10+ years of experience in software development with a focus on Python.\n• Strong experience in Azure cloud services and architecture.\n• Proficiency in Azure-native services such as Azure Functions, Azure App Services, Azure Storage, Azure Data Factory, Azure Key Vault, and Azure Kubernetes Service (AKS).\n• Solid understanding of RESTful APIs, microservices architecture, and integration patterns.\n• Experience with Azure DevOps, Git, and CI/CD pipelines.\n• Strong knowledge of databases (SQL & NoSQL) and ORM frameworks.\n• Familiarity with containerization tools like Docker and orchestration using Kubernetes.\n• Strong problem-solving and debugging skills.\n• Excellent communication and collaboration abilities.\n\nJob Types: Full-time, Contractual / Temporary, Freelance\nContract length: 12 months\n\nPay: ₹80,000.00 - ₹90,000.00 per month\n\nWork Location: Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=xQNSVuqw69hQGrlMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLvQ6CMBAA4LjyCMbhZg0UTVx0Iv4gRtFYFydS6gUw0CPtacD38T3V5ds-7zPwdtH7aREmcO65JANrfGFNLVp_GkJvHfgXbIgR_BUZtkoz-LCnHBwqq0v4lZioqHG4LJlbtxDCuTooHCuudKCpEWQwp048KHd_Mlcqi22tGLPZPOyC1hTjkYzOhySNIUm3J3mT181RQmUgMfdKfQFzAxWTqAAAAA&shmds=v1_AdeF8KitO46RY7AjOGc0Adxg0XNTWnTl_HE7k1WZOICfgqskqw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=xQNSVuqw69hQGrlMAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Python Developer/ Golang Developer",
         "Hireginie",
         "India",
         "Python Developer/ Golang Developer\n\nAbout Our Client: Founded in 2020, the company is a digital platform in the spirituality and wellness sector, offering tailored apps to help users with personal growth and well-being. It combines technology with traditional practices to deliver engaging content and experiences that drive long-term user retention. The brand aims to cater to individuals seeking high-quality and authentic devotional products, blending traditional craftsmanship with modern convenience.\n\nJob Description: Python Developer/ Golang Developer\n\nLocation: HSR Layout, Bangalore\n\nExperience: 4-8 years\n\nQualification: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field\n\nAbout the role: We are looking for a senior software developer (Python) with a strong background in backend technologies to join our high-performing engineering team. You will play a key role in designing, building, and maintaining mission-critical services that scale to millions of users.\n\nKey Responsibilities:\n• Develop and maintain robust, scalable backend systems using Python, Golang.\n• Design efficient data models and queries for PostgreSQL and MongoDB.\n• Build secure and performant APIs for mobile and web applications.\n• Drive cloud-native development and infrastructure setup on AWS.\n• Collaborate with cross-functional teams, including product, mobile, and DevOps.\n• Optimize systems for performance, reliability, and scalability.\n• Conduct code reviews, write unit tests, and improve development processes.\n• Troubleshoot, debug, and resolve production-level issues.\n\nRequirements:\n• Backend development experience with Python, or Golang.\n• Strong command over relational (PostgreSQL) and document (MongoDB) databases.\n• Practical experience deploying applications on AWS (EC2, ECS, Lambda, RDS, S3).\n• Proficiency in designing RESTful APIs and working in service-oriented architectures.\n• Familiarity with Docker, Git, CI/CD tools, and cloud monitoring practices.\n• Ability to write clean, testable, and maintainable code.\n• Strong analytical and debugging skills with a performance-first mindset.\n\nAbout Hireginie: Hireginie is a prominent talent search company specializing in connecting top talent with leading organizations. We are committed to excellence and offer customized recruitment solutions across industries, ensuring a seamless and transparent hiring process. Our mission is to empower both clients and candidates by matching the right talent with the right opportunities, fostering growth and success for all.",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=F9NAyXTufaOLO-TcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0XKsQoCMQyAYVxvdHTKLNiK4KCuwqmTb3C0Z2gjNSlNkPMdfGh1cvmHj797z7rd9WVZGI74xCIVm4deSuD0F1jBRSIohjZm-L69SCq4OGSzqnvvVYtLasFodKM8vDBGmfxdov4yaA4NawmGw2a7nlzltJyfqGEiJgRiOPONwgfKTBCSjwAAAA&shmds=v1_AdeF8KhoWLyDDBuP898CHM9YIWMqM9e2aqhcrrBB7VoESHKDUA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=F9NAyXTufaOLO-TcAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Lead Software Engineer - Python",
         "EPAM Systems",
         "India",
         "We are seeking a highly experienced Lead Software Engineer with expertise in Python to lead development efforts, influence technical direction, and deliver robust, scalable solutions.\n\nThe ideal candidate will bring proven expertise in Python, database systems, and modern software development practices to drive the success of critical projects.\n\nResponsibilities\n• Collaborate with stakeholders to gather requirements, create technical designs, and align solutions with business goals\n• Lead the development of high-quality, scalable, and maintainable software systems\n• Conduct technical reviews, including code reviews, to ensure adherence to best practices, coding standards, and performance benchmarks\n• Coordinate with cross-functional teams to ensure successful implementation of features and solutions\n• Troubleshoot and resolve complex technical challenges across the development lifecycle\n• Mentor and guide team members, offering technical leadership and fostering skills development\n• Drive the adoption of modern development processes and tools, including CI/CD practices through GitHub Actions\n• Optimize application performance and database queries, ensuring efficiency and scalability\n• Utilize Agile/Scrum methodologies to manage projects and deliver iterative improvements\n• Oversee database architecture design and ensure proper integration with applications\n\nRequirements\n• 7-12 years of experience in software development with demonstrated expertise in Python\n• Proficiency in PostgreSQL and MS SQL Server for database design, optimization, and management\n• Hands-on expertise with GitHub Actions for automation and CI/CD workflows\n• Competency in ReactJS for developing interactive, dynamic user interfaces\n• Solid understanding of Scrum/Agile methodologies for collaborative development and delivery\n\nNice to have\n• Familiarity with other front-end frameworks beyond ReactJS\n• Skills in optimizing cross-platform application performance\n• Understanding of advanced DevOps practices and tools",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=8zsCY6MZPOkQ61E8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMMQrCQBAAsU1vY7W1YE4ELbRKEURRCOQB4e6y3p0kuyG7YPIHH21shplmsu8qOz3QtlDzSz92RCgpJEIcYQfVrJFpkTs7ELSjj7D0lTl0uLlE1UHOxoh0eRC1mnzuuTdM6Hgyb3byRyNx-Q6dVWwOx_2UDxS267IqnlDPotgLJIIbtcn-AMZCw8KPAAAA&shmds=v1_AdeF8KgREb7t01KB27izx3TaHM4VaDUCIOJ26tPSNVDAUnoktQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=8zsCY6MZPOkQ61E8AAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Sr. Python Developer + (Generative AI) 5-9Years( Mandatory)",
         "Brilliantech Software",
         "India",
         "Job description\n\nSr. Python Developer (gen AI) – Onsite\n\nLocation: Kalyani Nagar, Pune\nExperience: 6-9 Years( Mandatory)\nContract Duration: 6 Months, Full-Time (can be extended),Freelancing\nJoining: Immediate Joiners Only\n\nRequired Technical Skills:\n• Languages: Python , JavaScript/TypeScript, Node.js\n• Generative Ai & LLM must\n• Django & Fast Api is must\n• Frameworks/Tools: Apache Airflow or Google Composer\n• Kafka, Snowflake, BigQuery, Spark, Hadoop\n• AWS (Athena, Lambda, EC2, S3), GCP\n• PostgreSQL, Redis\n• Docker, Kubernetes (preferred)\n• CI/CD Pipelines, Trunk-Based Development\n\nOther Core Skills:\n• Strong in SQL and data pipeline optimization\n• Data structures and algorithms\n• Exposure to ML/AI-based systems is a plus\n• SaaS architecture and product development lifecycle\n\nApply Now: sakshi@brilliantechsoft.com\n\nJob Types: Full-time, Contractual / Temporary, Freelance\n\nJob Types: Full-time, Contractual / Temporary, Freelance\n\nWork Location: In person",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=FXn-CbqVfXhXLIlOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBSFYVz7CA5yx1ZpIoKDOilCqSCInZxK2l7bSMwtyaW2j-UbGpcz_By-6DuL7oUTcJu4IwtnHNBQjw5WEGdo0SnWA8IxT2Cb7h6onI_hqmyjmNyUQAoXqsCHXncQgIyoNTg_dMy930vpvRGt56DUoqa3JIsVjfJFlf9P6TvlsDeKsdxs16PobbtcnJw2RivLGNCCnvwJJ9AWctto9QNvkQzLtAAAAA&shmds=v1_AdeF8KimZp_KL7w3D4P0Za9m2h-N4eL3LhvFCnTLCqY7KGq3JQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=FXn-CbqVfXhXLIlOAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Python Developer (REST API, Cloud Platforms and Full-Stack Skills)",
         "Synechron Technologies Pvt. Ltd._INDIA Company",
         "India",
         "Job Summary Synechron is seeking an experienced Python Developer to design, develop, and maintain scalable and robust software solutions across various domains. The role involves working closely with cross-functional teams to translate business requirements into high-quality technical implementations, leveraging Python and related technologies. The ideal candidate will bring expertise in API development, database management, and software best practices to deliver solutions that support the organization’s strategic objectives. This position offers growth opportunities for professionals passionate about innovation, technology, and continuous learning. Software Requirements Required Skills: Proven experience in Python programming, with at least 4 years of hands-on development Strong understanding of object-oriented programming (OOP) principles Experience with Python frameworks such as Django, Flask, or FastAPI Familiarity with RESTful API development and integration Knowledge of relational databases (MySQL, PostgreSQL) and NoSQL databases (MongoDB) Experience with version control systems such as Git Preferred Skills: Cloud platform experience (AWS, Azure, Google Cloud) Containerization with Docker and orchestration with Kubernetes Testing frameworks like PyTest or unittest Automation with CI/CD pipelines (Jenkins, GitLab CI, Azure DevOps) Overall Responsibilities Design, develop, and optimize scalable Python applications and scripts aligned with business needs Collaborate with product managers, UI/UX designers, and fellow developers to gather requirements and translate them into technical solutions Write clean, efficient, and maintainable code following best coding practices and standards Conduct code reviews, identify issues, and troubleshoot bugs to ensure application stability and performance Participate actively in the full software development lifecycle, including planning, testing, deployment, and maintenance Integrate third-party data sources and APIs to extend application functionality Document code, application features, and technical specifications for ongoing support and future enhancements Stay updated with industry trends, emerging technologies, and best practices to incorporate innovative solutions Support team members with technical guidance, knowledge sharing, and resolving complex issues Ensure solutions adhere to security standards and are optimized for performance and scalability Technical Skills (By Category) Programming Languages: Required: Python (4+ years of practical experience) Preferred: Knowledge of additional languages such as JavaScript, Java, or C# for full-stack or integrations Frameworks & Libraries: Django, Flask, or FastAPI API & Data Management: REST API development and consumption Relational databases: MySQL, PostgreSQL NoSQL databases: MongoDB Cloud & DevOps Technologies: Cloud providers: AWS, Azure, or GCP (preferred) Containerization: Docker Container orchestration: Kubernetes (preferred) CI/CD tools: Jenkins, GitLab CI, Azure DevOps Tools & IDEs: IDEs such as Visual Studio Code, PyCharm, or similar Version control tools: Git Testing & Automation: Frameworks like PyTest, unittest API testing and automation techniques Experience Requirements 4+ years of professional experience in Python development Proven track record of designing and implementing scalable applications and APIs Experience working with relational and NoSQL databases Hands-on experience with cloud platforms, containerization, and orchestration tools (preferred) Demonstrated experience working within Agile teams and contributing to development best practices Experience in related domains such as finance, healthcare, or enterprise application development is a plus Day-to-Day Activities Develop and improve Python-based applications, APIs, and automation scripts Collaborate with cross-functional teams on requirements, architecture, and implementation strategies Conduct code reviews and testing to ensure high code quality and adherence to standards Troubleshoot and resolve technical issues promptly to minimize downtime Deploy updates and enhancements via automated pipelines, ensuring seamless delivery Maintain documentation of code, API specifications, and technical processes Participate in daily stand-ups, sprint planning, and retrospectives Review emerging technologies and propose their integration into existing workflows Qualifications Bachelor’s degree or higher in Computer Science, Engineering, Information Technology, or a related field Additional certifications such as Python Institute certifications, cloud certifications (AWS, Azure), are advantageous Commitment to ongoing professional development to stay current with evolving technologies and best practices Professional Competencies Strong analytical and problem-solving skills with attention to detail Excellent communication skills to effectively share ideas and technical information Ability to work independently with minimal supervision and within team environments Adaptability to changing project needs and emerging technologies Proactive approach to learning and process improvement Ethical mindset ensuring security, privacy, and quality in deliverables SYNECHRON’S DIVERSITY & INCLUSION STATEMENT Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more. All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. Candidate Application Notice At Synechron, we believe in the power of digital to transform businesses for the better. Our global consulting firm combines creativity and innovative technology to deliver industry-leading digital solutions. Synechron’s progressive technologies and optimization strategies span end-to-end Artificial Intelligence, Consulting, Digital, Cloud & DevOps, Data, and Software Engineering, servicing an array of noteworthy financial services and technology firms. Through research and development initiatives in our FinLabs we develop solutions for modernization, from Artificial Intelligence and Blockchain to Data Science models, Digital Underwriting, mobile-first applications and more. Over the last 20+ years, our company has been honored with multiple employer awards, recognizing our commitment to our talented teams. With top clients to boast about, Synechron has a global workforce of 14,500+, and has 58 offices in 21 countries within key global markets. For more information on the company, please visit our website or LinkedIn community. At Synechron, we are committed to integrating sustainability into our business strategy, ensuring responsible growth while minimizing environmental impact. Employees play a key role in driving our sustainability initiatives, from reducing our carbon footprint to fostering ethical and sustainable business practices across global operations. All positions are required to adhere to our Sustainability and Health Safety standards, demonstrating a commitment to environmental stewardship, workplace safety, and sustainable practices. Not finding the right fit? Let us know you're interested in a future opportunity by clicking Get Started below or create an account by clicking 'Sign In' at the top of the page to set up email alerts as new job postings become available that meet your interest!",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=TiYMGut_xoJcia96AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNTUvDQBBA8dqf4GmOKnYjggjtqfSLiEgwvfQUNpsxu-1kZ9mZluZn-o-Ml_du781-72bHalTPETZ4ReKEGR6-t_UBVlX5DGviSwcVWf3hPAjY2MHuQjSv1boz1OdAJI8whw9uQdBm52Fq7Zl7wvulV02yKAoRMr2o1eCM46HgiC3fihO38o9GvM2Ypgs2r28vN5Ni__RejxGdz1PuMDkycR9QoLqqgU_tTFN-bcoVrHlINo4QIpSxC_YPQU3MxtQAAAA&shmds=v1_AdeF8KhInAnvI67oQ4DyGYxxwP2nF_xV-My3c_x6k7gQ_gscgw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=TiYMGut_xoJcia96AAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "Brainium Information Technologies Pvt. Ltd.",
         "Anywhere",
         "We are looking for Freelance Python Developer for our organization.\n• * Minimum 4+ years of experience required as a Freelance Python Developer.\n\nBrainium Information Technologies Pvt. Ltd.\n• * Looking for Full-time / Part- time Freelancers.\n\nTentative Job Responsibilities:\n• Web development – Using frameworks like Django, Flask, or FastAPI.\n• Data analysis & visualization – With libraries like Pandas, NumPy, Matplotlib, Seaborn.\n• Automation & scripting – Creating scripts to automate repetitive tasks.\n• API development & integration – Building or connecting to REST and GraphQL APIs.\n• Machine learning & AI – Using TensorFlow, PyTorch, Scikit-learn.\n• Testing & debugging – Writing unit tests, fixing code issues.\n\nInterested candidates share your CV to ananya.adhikary@brainiuminfotech.com or Whatsapp +91-XXXXXXXXXX\n\nJob Type: Full-time\n\nPay: ₹400,000.00 - ₹600,000.00 per year\n\nBenefits:\n• Flexible schedule\n• Health insurance\n• Paid time off\n• Provident Fund\n\nEducation:\n• Bachelor's (Preferred)\n\nExperience:\n• Freelancing: 4 years (Required)\n\nWork Location: Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMMQ7CMBAERZsnUF2NhA0IGugQAoEoUtBHjnPYRs6dZR9R-BDvJDTbzMxW31m1PmfEaMgi1B_xTHDCASMnzLCEG7dQ0GTrYSIXZhdxfvAiqey1LiUqV8RIsMpyr5mw5VG_uC3_aYo3GVM0gs1mtxpVIrfYHrMJFN49XOnJuZ_i6fmB1hNHdgEL1IMouEunINBkdcH8ACwAmnypAAAA&shmds=v1_AdeF8KiGWFUpMZ9bOvPmhGq10cr3SXf4wAt23khXIHthEAw8eg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Mathematics Graduate-Programmer(Python/Java/C++/C)-Remote",
         "GiantMind Solutions",
         "Anywhere",
         "Global Opportunity – Short-Term -\n\nMathematics Problem Creator & Programmer\n\nRemote | Eligible Locations: India\n\nDuration: 1 Month |\n\nJob description:\n\nQualification: BSc/MSc/PhD in Mathematics\n\nYour day to day work will be focused on solving High School/Advanced level mathematics questions and breaking it down step by step and translate complex mathematical and algorithmic solutions into efficient, robust, and readable code, primarily using C/C++/ Python/Java.\n\nWe need someone who has completed BSc/M.Sc/Ph.d in mathematicsWe need non-teaching experience in one of the languages C++/Java/Python.\n\nYou will have to clear a technical interview.\n\nThis engagement is for 1 month.\n\nThe project requires a time commitment of 40 hrs/week)\n\nInterested? Reach out at vijaya.lakshmi@giantmindsolutions.com\n\nTag or refer someone who fits this!\n\nJob Types: Full-time, Part-time, Contractual / Temporary\n\nPay: ₹40,000.00 - ₹60,000.00 per month\n\nApplication Question(s):\n• Have you completed your Graduate in Mathematics?\n• Are you fine with short term contract(1 Month)?\n• Do you have experience in any one of these languages(Python/Java/C++)?\n\nWork Location: Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=mjlsp1kl83EjWpwOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQrCQBBAYWw9glhM6Q_JimCjpUUwEAh6AJkkQ3YlOxN2JqKH8o5q88rvzT-zeV2heYpooVUoEnYTGmV1kj5hjJRW9du8sCvxie683brzOrtSFCPIoJQGlDC1HoShEOkHWpy82ahH51SHvFf703kr0QlTIy_3kEb_uavHROPw-933h90rH7nfLIuAbFXgDm4yTBaEFQLDhbuAX3z9uIewAAAA&shmds=v1_AdeF8KjpLvkDMxWj8kIsH_YSsUGWrRK9EzXPskJ_wPFE6MbgPA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=mjlsp1kl83EjWpwOAAAAAA%3D%3D",
         "Python Developer"
        ],
        [
         "Microsoft ETL / SSIS Developers",
         "Fluor",
         "India (+3 others)",
         "At Fluor, we are proud to design and build projects and careers. We are committed to fostering a welcoming and collaborative work environment that encourages big-picture thinking, brings out the best in our employees, and helps us develop innovative solutions that contribute to building a better world together. If this sounds like a culture you would like to work in, you’re invited to apply for this role.\nJob Description\n\nMotivated and results driven Senior Extract, Transform and Load (ETL) Developer. Sound knowledge of SQL Server Integration Services (SSIS) using ETL in developing, testing, deploying packages, using DTS for import, export and transformation of data. Experience with SQL Server Analysis Services (SSAS) surrounding designing and deploying Multidimensional Cubes and writing MDX queries. Experience writing stored procedures, functions, triggers and views using PL/SQL and T-SQL. Excellent knowledge of Data Warehousing concepts: Star Schema, Snow-Flake Schema, Fact and Dimensional tables, Relational databases, slowing changing dimensions, data marts, aggregation design, logical and physical data models, Normal Forms(NF), On-line Analytical Processing(OLAP) and On-Line Transactional Processing (OLTP), MOLAP and ROLAP, HOLAP, normalized and de-normalized data.\n\nBasic Job Requirements\n• Programming Languages: C#, VB.Net, PHP, HTML, CSS, SQL, TSQL. PL/SQL, JavaScript\n• Databases: SQL Server 2005/2008/2008/2017 R2/2012/2014, Oracle 10g, MySQL\n• Platforms: SSIS, SSAS, SSRS, BizTalk, Informatica, Crystal Reports\n• Other: Red Gate, TFS, SVN, Visio, Erwin, Azure DevOps\n\nOther Job Requirements\n• Thorough understanding of the entire software development lifecycle, including analysis, design, configuring, programming and unit testing and deployment.\n• Expert knowledge using Microsoft Visual Studio.\n• Outstanding analytical and trouble-shooting skills, adept to multi-tasking, strong communication and interpersonal skills.\n• Ability to work efficiently in a high stress customer facing environment.\n• Proficiency in troubleshooting production related issues\n• Ability to identify potential performance bottlenecks in code and follow best practices and standards\n\nPreferred Qualifications\n• Accredited degree or global equivalent in Computer Science or related discipline\n• Experience with Engineering, Procurement & Construction (EPC) industry projects\n• Experience in computer systems or Information Technology (IT) support, with technical proficiency in operating systems and programming languages\n• Strong written and verbal communication skills\n• Strong interpersonal skills\n• Excellent analytical, technical, planning, and organizational skills\n\nTo be Considered Candidates:\nMust be authorized to work in the country where the position is located.\n\nWe are an equal opportunity employer. All qualified individuals will receive consideration for employment without regard to race, color, age, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, genetic information, or any other criteria protected by governing law.\n\nNotice to Candidates:\nBackground checks are carried out as part of any conditional offer made, including (but not limited to & role dependent) education, professional registration, employment, references, passport verifications and Global Watchlist screening.",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=TMInSQBk-W9m3kTOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOw4CIRAA0Nhu4QGspjYRjIkW2vrJGq2w3wCOgEGGMKPZC3hv4yte9510m2vyjZgeAofbBTQY0xvY4wczVWwMCziTA0bbfAQqcCIKGWe7KFJ5qzVzVoHFSvLK00tTQUejfpLjfwNH27BmKzis1stR1RLm02N-U4NUoC_3ZH_8-zGfiAAAAA&shmds=v1_AdeF8KhNydR2dJYdV5inYyRZoMUfX7lo5lOthEGkoAmTvCxB4Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=TMInSQBk-W9m3kTOAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Datastage Developer",
         "BNP Paribas",
         "India",
         "About BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability.\n\nAbout BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions..\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function :\n\nThe IT Department is functionally responsible for the IT structures of the companies of the BNL BC Group and coordinates the IT community of the BNPP Group onwards with the aim of obtaining all possible synergies of purpose, ensuring full operational consistency and enabling the digital transformation of our Group\n\nAs part of the IT Department of BNL, the IT Data Platform is responsible for defining and implementing the technological strategy of the company in terms of Analytics, Business Intelligence, Artificial Intelligence and Data Management.\n\nThe group therefore has the responsibility of evolving and maintaining the data architecture serving Analytics, as well as creating management reporting and dashboarding solutions.\n\nData Platform also has the responsibility of building Advanced Analytics and Artificial Intelligence solutions, and integrating them with Bank systems, making them fully part of company processes.\n\nJob Title:\n\nSW Developer 1 (ETL)\n\nDate:\n\nDepartment:\n\nLocation:\n\nBusiness Line / Function:\n\nIT Data Platform\n\nReports to:\n\n(Direct)\n\nISPL ADM Manager\n\nGrade:\n\n(if applicable)\n\n(Functional)\n\nBNP PARIBAS BNL Data Platform IT Leader\n\nNumber of Direct Reports:\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nProvide a brief description of the overall purpose of the position, why this position exists and how it will contribute in achieving the team’s goal.\n\nThe requested position is developer-analyst in an open environment, which requires knowledge of the mainframe, TSO, JCL, OPC environment.\n\nResponsibilities\n\nDirect Responsibilities\n\nFor a predefined applications scope take care of:\n\n·Design\n\n·Implementation (coding / parametrization, unit test, assembly test, integration test, system test, support during functional/acceptance test)\n\n·Roll-out support\n\n·Documentation\n\n·Continuous Improvement\n\n• Ensure that SLA targets are met for above activities\n\n• Handover to Italian teams if knowledge and skills are not available in ISPL\n\n• Coordinate closely with Data Platform Teams’s and also all other BNL BNP Paribas IT teams (Incident coordination, Security, Infrastructure, Development teams, etc.)\n\n·Collaborate and support Data Platform Teams to Incident Management, Request Management and Change Management\n\nContributing Responsibilities\n\n·Contribute to the knowledge transfer with BNL Data Platform team\n\n·Help build team spirit and integrate into BNL BNP Paribas culture\n\n·Contribute to incidents analysis and associated problem management\n\n·Contribute to the acquisition by ISPL team of new skills & knowledge to expand its scope\nTechnical & Behavioral Competencies\n\n·Fundamental skills:\n\no IBM DataStage\n\no SQL\n\no Experience with Data Modeling and tool ERWin\n\n·Important skill - knowledge of at least one of database technologies is required:\n\no Teradata\n\no Oracle\n\no SQL Server.\n\n·Basic knowledge about Mainframe usage TSO, ISPF/S, Scheduler IWS, JCL\n\n·Nice to have:\n\no Knowledge of MS SSIS\n\no Experience with Service Now ticketing system\n\no Knowledge of Requirements Collection, Analysis, Design, Development and Test activity\n\no Continuous improvement approaches\n\no Knowledge of Python\n\no Knowledge and experience with RedHat Linux, Windows, AIX, WAS, CFT\nSpecific Qualifications (if required)\n\nBasic knowledge of Italian language can be an advantage\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nAbility to share / pass on knowledge\n\nAbility to deliver / Results driven\n\nAdaptability\n\nTransversal Skills: (Please select up to 5 skills)\n\nAbility to develop others & improve their skills\n\nAbility to manage / facilitate a meeting, seminar, committee, training…\n\nChoose an item.\n\nChoose an item.\n\nChoose an item.\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 3 years\n\nOther/Specific Qualifications (if required)\n\nQualifications - External\n\nNa",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=r_RI4EF0papmr5GyAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQoCMQwAUFxvdnLKLNiK4KKbnIgicoP7kdbQVmpTmiC3--Pq8rbXfWadOd6v0KOiKAaCnt6UuVKDFVzYgRA2H4ELnJhDpsU-qlbZWSuSTfglTd54flku5HiyT3byZ5SIjWpGpXGzXU-mlrCcH24DDNiSQ4FU4FweCb-gaBz5hgAAAA&shmds=v1_AdeF8KjzfVjfCF5mxsrSDO-L7fSb-xrKuBYToid6c3KTgE34Eg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=r_RI4EF0papmr5GyAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "Insight Global",
         "Insight Global",
         "Hyderabad, Telangana, India",
         "The Informatica ETL Developer position focuses on ETL design, development and support. Responsible for ETL processes and tools as well as database loading and manipulation to ensure that the data needs are met throughout the course of the projects and production support. The developer will work to understand the business problems and opportunities in the context of the requirements and recommend solutions that enable the organization to achieve its goals. Additional responsibilities may include interpretation and transformation of information based on business requirements and conducting detailed research of vendor products and assists with general project management.\nSome day to day responsibilities include:\nAssist with design and implementation of data warehouses, planning applications and reporting solutions.\nDevelop solutions to leverage ETL tools and suggest process improvements.\nProvide ongoing maintenance and support of assigned ETL flows and their Client applications.\n\nCompensation:\n$16to $17HR\nExact compensation may vary based on several factors, including skills, experience, and education.\n\nBenefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=ilWZRf7UiEHOZ3cHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_13NMQ7CMAwAQLH2CUxeWFCbICQWeEApM3vlJFYSlNpVnKG8gw8jVpZbr_vsusPEmmNqMBZxWGCAhzhQwuoTCMMoEgvtb6m1Va_WqhYTtWHL3nhZrDA52exLnP6YNWGltWCj-Xw5bWbleBz-jsxwfweq6DD08KSCHJGxh4lDxi-RVypzlgAAAA&shmds=v1_AdeF8KhmLbrgMzBBwyUeoUQL_4hKnzeejX-h_elIeYvsdlgyPg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=ilWZRf7UiEHOZ3cHAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Developer- Ab Initio",
         "Infosys",
         "Secunderabad, Telangana, India",
         "The ideal candidate will be responsible for developing high-quality applications. They will also be responsible for designing and implementing testable and scalable code.\n\nJob Description\n\nLocation-Pune, Chennai, Bangalore, Chandigarh, Hyderabad\n• Minimum 5+ years of experience and strong knowledge in Ab Initio and related activities.\n• Design, develop, and implement robust ETL solutions using Ab Initio (GDE, EME, Co>Op, Express>It, Continuous Flows, etc.)\n• Involved in end-to-end implementation of Data warehousing projects, which include Business Requirements gathering, Analysis, System study, technical specifications, Coding, Testing,Code migration, Implementation, System maintenance and Documentation.\n• Proficiency indeveloping SQL queries and functionality with various databases like Oracle.\n• Familiarity with Data warehousing and ETL concepts and techniques\n• Expertise and hands on experience in Ab Initio technology, create generic components, batch and continuous flows, Testing framework and Express IT.\n• Familiarity with scheduling tools (e.g., Autosys, Control-M) and version control systems (e.g., Git)\n• UNIX shell scripting will be an added advantage in scheduling/running application jobs.\n• Understand project requirements and translate them into technical solutions which meets the project quality standards\n• Ability to work in team in diverse/multiple stakeholder environments and collaborate with upstream/downstream functional teams to identify, troubleshoot and resolve data issues.\n• Troubleshoot and resolve performance, data quality, and functional issues\n• Excellent verbal and written communication skills.\n• Stay up to date with new technologies and industry trends in Development.\n• Collaborate with business analysts, data architects, and other developers to understand data requirements and ensure the successful delivery of solutions.",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=hXIgJIY_QJV09FmsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU63Km0jgotOgiIVN7uXS3umkXgXclHqp_i36vLWV3xmhTm2FzjQi4JEShXsLTTssxeo4CwWlDD1IwjDScQFmu_GnKNujVENtdOM2fd1Lw8jTFYmcxerfzodMVEMmKlbb1ZTHdktFw3fRN8KnuFK_ZMHSmhxKKGlgOyQsfz9g8cvEPYtGJwAAAA&shmds=v1_AdeF8Kj8aGfXOqTOLIUxElazN0gGyDYLuW66E6RhyUWwXah5zQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=hXIgJIY_QJV09FmsAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Developer- Hyderabad Location",
         "iitjobs inc",
         "Hyderabad, Telangana, India",
         "Greetings!!!We have a very urgent opening for ETL Developer for Hyderabad location. Note:Job Type: Long term ContractualMode: Hybrid for HyderabadExperience: 6+ yearsJoining: Immediate to 20 DaysDescription 6+ years of experience in ETL (Informatica,Mulesoft), SQL, PL/SQL and Tableau is mandatory Experience with Relational databases like PostgreSQL. Experience with data visualization tools like Tableau, Power BI, or similar. Experience with the Salesforce.com platform and related technologies Proficiency in data manipulation and analysis using SQL. Good to have Snowflake, Airflow, Spark Agile Methodologies and well versed with GUS/JIRA If you are interested, please share your resume at: suhas@iitjobs.com",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=V0oKWmJqAnIDHm0TAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WNywrCMBBFcdtPcDXr0jYiCKJbxQdddl8m6ZCmxEzIBKnf44_arlzcAwcO3OK7KY7XroULvclzpFTD_TNQQo0DtGwwOw5Qw5M1CGEyIyx-Y7aetucx5ygnpUR8YyUvsWkMvxQH0jyribWs6GXERNFjpn5_2M1NDLYsnctrAC6YZf_bCjryGCwGrOARBoc_sAhC3aYAAAA&shmds=v1_AdeF8KiontKOXG3bNsK8LGd0DVCbbgszksdymS0_LKVU07oVug&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=V0oKWmJqAnIDHm0TAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Developer-4+yrs exp-Hyderabad only (Must have Exp in Redshift and Data Warehouse)",
         "Minfy",
         "Hyderabad, Telangana, India",
         "As requested, I am sharing JD for ETL developer, modify accordingly as per the requirement and make use of this for our requirements.\n\nSince we need someone who can work on Redshift and modified accordingly as per our requirement.\n\nJob Title: ETL Developer\n\nLocation: Hyderabad\n\nExperience: 3-5 years\n\nEmployment Type: Full-time / Contract\n\nJob Summary:\n\nWe are seeking a skilled ETL Developer to design, develop, and optimize data pipelines and ETL processes on Amazon Redshift. The ideal candidate will have strong experience in Redshift, SQL, ETL tools, and data warehousing concepts, along with expertise in building scalable data solutions in cloud environments.\n\nKey Responsibilities:\n• Design, develop, and maintain ETL pipelines to extract, transform, and load data into Amazon Redshift.\n• Work closely with business analysts, data engineers, and stakeholders to understand data requirements and translate them into ETL solutions.\n• Optimize complex SQL queries and ensure efficient performance in Redshift.\n• Perform data profiling, data quality checks, and troubleshoot data issues.\n• Implement incremental loads, change data capture (CDC), and performance tuning techniques.\n• Monitor ETL jobs, data pipeline health, and manage data recovery procedures when required.\n• Develop stored procedures, functions, and scripts to support data processing and transformation.\n• Collaborate with DevOps to automate deployments and manage infrastructure as code (IaC).\n• Document technical designs, processes, and data flows.\n• Skills Required:\n\nStrong hands-on experience with Amazon Redshift\n\nExpertise in SQL (writing, tuning, debugging complex queries)\n\nExperience with ETL tools (AWS Glue, Matillion, Informatica, Talend, etc.)\n\nProficiency in Python/Scala for data processing and scripting.\n\nStrong knowledge of Data Warehousing concepts, Star Schema, and Dimensional Modeling.\n\nExperience with AWS services (S3, Lambda, Step Functions, CloudWatch) is a plus.\n\nFamiliarity with performance tuning techniques in Redshift (Distribution Keys, Sort Keys, Vacuum, Analyze).\n\nExperience with Git, CI/CD pipelines, and version control.\n\nNice to Have:\n• Experience with Redshift Spectrum and Redshift Serverless.\n• Exposure to Snowflake or other cloud data platforms.\n• Understanding of Data Governance, Data Lineage and Metadata Management.\n• Knowledge of Airflow for orchestration.\n\n--\n\nThanks & Regards,\n\nPurnima Solanki\n\nHR Recruiter\n\nSurvey No. 10, Divine Babylon Building, Whitefields, Kondapur,\n\nOpp. Lane of Jayabheri Silicon Valley, Hyderabad-500084, Telangana.\n\nGlobal Office: India | Kuala Lumpur, Malaysia (Regional Office) Manilla,\n\nPhilippines | Singapore | USA\n\nwww.minfytech.com",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=vl84OgJDddKjDtBFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2OMU8CQRBGY8tPsPoKCwXuzhhotIUoRBpDYknmbofbJevMZmchdz_TfyQ0Nq98701-7yZxvf_Eii8cNXGuFrMxG3hI1cfoOFNLDipxxOPubAWeLoz1kBAEX-zMh2MBicOKCuGbMns9Gz-hwlZbGFPu_FWAd9U-8v2bLyXZa9OYxbq3QiV0dac_jQq3OjQnbe2Gg_mrK0UqfHhZPg91kn76sAtyHG_p_7c59hxJehKaYyMu0B9XyTaI1AAAAA&shmds=v1_AdeF8KhqOdLwi-2SfAhAZYVI87vitWhV6zvL65SaEEVKSbDGyQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=vl84OgJDddKjDtBFAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Developer- Hyderabad (2-3+ Years of Experience)",
         "A Client of Analytics Vidhya",
         "Hyderabad, Telangana, India",
         "Role Summary:\n\n•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n\n•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n\n•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n\n•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n\n•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n\n•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n\n•Create quality rules in Talend.\n\n•Tune Talend jobs for performance optimization.\n\n•Write relational and multidimensional database queries.\n\n•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n\n•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n\n•Exposure in Map Reduce components of Talend / Pentaho PDI.\n\n•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n\nJob Specification:\n\n•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n\n•Having an experience of 2 – 3+ years.\n\n•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n\n•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n\n•Working knowledge of relational database theory and dimensional database models.\n\n•Ability to write complex SQL database queries.\n\n•Ability to work independently.",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOuw6CUBBEY8snWG3pg4fR2EhFlKjE0phYmQVWuOa6S9gbAz_odwmVzTQnc2a878TL0usFDvQhKw21AZz6klrMsYTZOtgs4U7YKsgT0m7ghrigOQSQSQ46oKIGYTiKVJamce1co7soUrVhpQ6dKcJC3pEw5dJFL8l1jIfW2FJj0dFjvV11YcPVIk5gbwe_G8cSRtsPbYWbKesewfD_mQ9XssgVMvpw5tLgD84obTrJAAAA&shmds=v1_AdeF8KhNH4dOhsM_ohqTFHeygbi7_Rs650WHWh26bXMFYZ5jQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "Barclays",
         "Maharashtra, India",
         "Join us as a ETL Developer at Barclays, responsible for supporting the successful delivery of Location Strategy projects to plan, budget, agreed quality and governance standards. You'll spearhead the evolution of our digital landscape, driving innovation and excellence. You will harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences.\n\nTo be successful as a ETL Developer you should have experience with:\n• Good knowledge of Python\n• Extensive hands-on PySpark\n• Data Warehousing concept\n• Strong SQL knowledge\n• Bigdata technologies (HDFS)\n• AWS working exposure\n\nSome other highly valued skills may include:\n• Working knowledge of AWS\n• Familiar with Bigdata\n\nYou may be assessed on the key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen strategic thinking and digital and technology, as well as job-specific technical skills.\n\nThis role is based in Pune.\n\nPurpose of the role\n\nTo build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\n\nAccountabilities\n• Build and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\n• Design and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\n• Development of processing and analysis algorithms fit for the intended data complexity and volumes.\n• Collaboration with data scientist to build and deploy machine learning models.\n\nAnalyst Expectations\n• Will have an impact on the work of related teams within the area.\n• Partner with other functions and business areas.\n• Takes responsibility for end results of a team’s operational processing and activities.\n• Escalate breaches of policies / procedure appropriately.\n• Take responsibility for embedding new policies/ procedures adopted due to risk mitigation.\n• Advise and influence decision making within own area of expertise.\n• Take ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct.\n• Maintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function.\n• Demonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\n• Make evaluative judgements based on the analysis of factual information, paying attention to detail.\n• Resolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents.\n• Guide and persuade team members and communicate complex / sensitive information.\n• Act as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation.\n\nAll colleagues will be expected to demonstrate the Barclays Values of Respect, Integrity, Service, Excellence and Stewardship – our moral compass, helping us do what we believe is right. They will also be expected to demonstrate the Barclays Mindset – to Empower, Challenge and Drive – the operating manual for how we behave.",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=N7eHv3R2ndO_i6H7AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQoCMQyAYVzvERwkg5NoK4KLt4kiim7uR1pDe1Kb0gQ5Z1_cc_mnj7_5Tpr58X6FA70pcaEKK7iwAyGsPgJnODGHRNM2qhbZWSuSTBBF7b3x_LKcyfFgn-zkn04iVioJlbrNdj2YksNith9vCT8CfYYbjgIlasUlnPOjxx9aM1wnhgAAAA&shmds=v1_AdeF8KiNb57kZV1yOgH3Jqou_vAYWsQmpnSrtRn5N6coTcSuqw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=N7eHv3R2ndO_i6H7AAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "ARMPL",
         "Bhopal, Madhya Pradesh, India",
         "Job Description : - To analyse the user requirements (new / change requests) and develop solutions for ETL implementation.- To follow the Microsoft framework- To understand the existing data source setup and plan for ETL using SSIS and SQL server.- Implementing and maintaining ETL process in platform recommended by client using SSIS- Monitoring performance of the ETL jobs in Platform and provide recommendations (where necessary)- To perform batch and real time production support- Design, develop, and maintain SQL Server databases, stored procedures, functions, and triggers- Optimize complex T-SQL queries for performance and scalability- Perform data analysis, data migration, and transformation- Build and maintain ETL processes (SSIS preferred)- Collaborate with developers and analysts to meet project goals- Ensure data security, consistency, and best practices- SSIS experience ingesting and doing complex transformations.- Create and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organizations data assets- Ensure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis- Work closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organizations data architecture is integrated and aligned with other IT systems and applications- Stay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organizations data architecture- Generates application documentation.- Contributes to systems analysis and design.- Designs and develops moderately complex applications.- Contributes to integration builds.- Contributes to maintenance and support.Requirements :- Experience in ETL by analyzing source data and providing transformation logic- Experience in SSIS for ETL implementation. Min 1-2 years of SSIS experience and the candidate should be willing to work on SSIS (Mandatory).- Experience in DBs like SQL server- Experience in writing scripts for transformation if required- Experience in suggesting tools for clients new requirement- Provide support for production runs and improve performance if required (ref:hirist.tech)",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=v90CsYml292k3YcnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43iIPURAQXOymKKC0UcS_X5GgqMRdyQeo3-NPiG17xnRWL86OGE73Jc6QEa7hxD0KYjAMOcGEePM0rl3OUvdYiXg2SMY9GGX5pDtTzpJ_cy79OHCaKHjN1291mUjEMq-Xh3rQ1jAGOjiP6Ehq07oPQJrQkroRrsCP-AMyvzq6OAAAA&shmds=v1_AdeF8KhweSv_8aJrYESRw-930uLDUPvAZ7z71ssTjQ99ibLEbQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=v90CsYml292k3YcnAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "Streamsets ETL Developer, Associate",
         "E902 DWS India Private Limited, Maharashtra Branch",
         "India",
         "Job Description: Job Title - Streamsets ETL Developer, Associate Location - Pune, India Role Description Currently DWS sources technology infrastructure, corporate functions systems [Finance, Risk, HR, Legal, Compliance, AFC, Audit, Corporate Services etc] and other key services from DB. Project Proteus aims to strategically transform DWS to an Asset Management standalone operating platform; an ambitious and ground-breaking project that delivers separated DWS infrastructure and Corporate Functions in the cloud with essential new capabilities, further enhancing DWS’ highly competitive and agile Asset Management capability. This role offers a unique opportunity to be part of a high performing team implementing a strategic future state technology landscape for all DWS Corporate Functions globally. We are seeking a highly skilled and motivated ETL developer (individual contributor) to join our integration team. The ETL developer will be responsible for developing, testing and maintaining robust and scalable ETL processes to support our data integration initiatives. This role requires a strong understanding of database, Unix and ETL concepts, excellent SQL skills and experience with ETL tools and databases. What we’ll offer you As part of our flexible scheme, here are just some of the benefits that you’ll enjoy Best in class leave policy Gender neutral parental leaves 100% reimbursement under childcare assistance benefit (gender neutral) Sponsorship for Industry relevant certifications and education Employee Assistance Program for you and your family members Comprehensive Hospitalization Insurance for you and your dependents Accident and Term life Insurance Complementary Health screening for 35 yrs. and above Your key responsibilities This role will be primarily responsible for creating good quality software using the standard coding practices. Will get involved with hands-on code development. Thorough testing of developed ETL solutions/pipelines. Do code review of other team members. Take E2E Accountability and ownership of work/projects and work with the right and robust engineering practices. Converting business requirements into technical design Delivery, Deployment, Review, Business interaction and Maintaining environments. Additionally, the role will include other responsibilities, such as: Collaborating across teams Ability to share information, transfer knowledge and expertise to team members Work closely with Stakeholders and other teams like Functional Analysis and Quality Assurance teams. Work with BA and QA to troubleshoot and resolve the reported bugs / issues on applications. Your skills and experience Bachelor’s Degree from an accredited college or university with a concentration in Science or an IT-related discipline (or equivalent) Hands-on experience with StreamSets, SQL Server and Unix. Experience of developing and optimizing ETL Pipelines for data ingestion, manipulation and integration. Strong proficiency in SQL, including complex queries, stored procedures, functions. Solid understanding of relational database concepts. Familiarity with data modeling concepts (Conceptual, Logical, Physical) Familiarity with HDFS, Kafka, Microservices, Splunk. Familiarity with cloud-based platforms (e.g. GCP, AWS) Experience with scripting languages (e.g. Bash, Groovy). Excellent knowledge of SQL. Experience of delivering within an agile delivery framework Experience with distributed version control tool (Git, Github, BitBucket). Experience within Jenkins or pipelines based modern CI/CD systems How we’ll support you Training and development to help you excel in your career Coaching and support from experts in your team A culture of continuous learning to aid progression A range of flexible benefits that you can tailor to suit your needs About us and our teams Please visit our company website for further information: https://www.db.com/company/company.htm For over 150 years, our dedication to being the Global Hausbank for our clients has been driven by our people – in around 60 countries and across more than 150 nationalities. Their deep understanding, insights, expertise, and passion help our clients navigate an increasingly complex world – be it in our Corporate Bank, our Private Bank, our Investment Bank or our Asset Management (DWS) division. Together we can make a great impact for our clients at home and abroad, securing their lasting success and financial security. More information at: Deutsche Bank Careers (db.com)",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=bDLqsL0ZNf2MhCujAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNwQqCUBBFaesntJp1mIrQIlsVShQGgUFLGXXQF_pG3gzil_V9KW3O6p57vO_GSwp1hIOQCmSvHFKaqOeRnA9nEa4NKsEe7lyBELq6A7ZwZW572p461VGSMBTpg1YU1dRBzUPIliqeww9XsqKUDh2N_XJVxodoDkbb7k7ZMYohfRdws41BeDozra3cDEap8eGBi4XSqUO4OLRL2tj_-AfWzd90uQAAAA&shmds=v1_AdeF8Kgc1sPAPO3xWRGm3yrUZuLvaeWGSP4zv7CLx4O1bRJDkA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=bDLqsL0ZNf2MhCujAAAAAA%3D%3D",
         "ETL Developer"
        ],
        [
         "Spark Engineer",
         "Staffingine LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3NsQrCMBAA0L1fIE63uAgmIrjoKCJKt35AucRrkhrvQi5DJ79ddHnr6z7dZihYX3DlkJiowg4e4kAJq48gDDeRkGl9jq0VPVmrmk3Qhi154-VthcnJYmdx-mPUiJVKxkbj4bhfTOGwXQ0Npyn9C-j7CySGOz8TfgHTIbRdgAAAAA&shmds=v1_AdeF8KjUl41s9rD9xNd7RywARJoJQ3Nel1qruGSZXxbjxJAYWA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Data Engineer with Scala and Spark",
         "Albireo Tech System",
         "Anywhere",
         "Description:\n\nWe are Looking for candidate having 7+ Yrs of Experience in Database for Below Requirement\n\nTech skills :\nData Engineering, Spark, Hive, Scala, Airflow\nTime : Morning IST(Flexibility Required for Both morning an Evening for 2 Days\nDuration : 3 hrs /Day\n\nWork Type: Screen Sharing Via Meeting Platforms\n\nJob Types: Full-time, Contractual / Temporary, Freelance\nContract length: 4 months\n\nPay: ₹30,000.00 per month\n\nBenefits:\n• Work from home\n\nExperience:\n• Scala: 7 years (Required)\n• Pyspark: 7 years (Required)\n\nWork Location: Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=DRc3KS95GCWtyWMqAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CeJws2AjgoM6CYroWvdySY8kmuZC7sD6I36vurzxNZ9ZszuhIpyzj5mowitqgM5hQsA8QFewPmEFN7YghNUF4AwXZp9ofgiqRfbGiKTWi6JG1zoeDWeyPJkHW_nTS8BKJaFSv9mup7Zkv1wck42VGO70S7u3KI0QM1zzEPELwmnNeZkAAAA&shmds=v1_AdeF8KjvWDUQa9bfitKmr9kDc_mT56uQlftcSO1qwVaf86J8Cw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=DRc3KS95GCWtyWMqAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Senior Apache Spark Engineer (Deployment, Tuning, and Internals)",
         "AkashX",
         "Anywhere",
         "Job Title: Senior Apache Spark Engineer (Deployment, Tuning, and Internals)\n\nLocation: Remote /India\nEmployment Type: Full-Time\n\nAbout AkashX.ai\n\nAkashX.ai is building next-generation analytics infrastructure that combines advanced SQL execution with optimized storage-compute integration. We are looking for an engineer with deep expertise in Apache Spark internals to help us deploy, optimize, and scale high-performance clusters.\n\nRole Overview\n\nYou will be responsible for deploying, tuning, maintaining, and scaling large-scale Apache Spark clusters. This role requires a strong understanding of Spark internals, from the Catalyst optimizer to Tungsten execution engine. Experience with Velox and Gluten for query acceleration is a plus.\n\nKey Responsibilities\n• Deploy, configure, and maintain Apache Spark clusters in production (YARN, Kubernetes, or standalone).\n• Tune Spark jobs for performance, including shuffle optimization, memory management, and adaptive query execution.\n• Diagnose and resolve performance bottlenecks at the JVM, Spark, and cluster levels.\n• Scale workloads efficiently across large clusters while ensuring reliability and cost efficiency.\n• Work with Velox and Gluten for query acceleration where applicable.\n• Integrate Spark with data lakes, warehouses, and downstream systems.\n• Manage Spark upgrades and implement CI/CD pipelines for Spark workloads.\n• Monitor and maintain cluster health using observability tools.\n• Document deployment and tuning best practices.\n\nRequired Skills & Experience\n• 5+ years of production experience with Apache Spark at scale.\n• Deep knowledge of Spark internals, including:\n• Catalyst optimizer\n• Tungsten execution engine\n• RDD and DataFrame execution flows\n• Shuffle and stage execution planning\n• Strong experience in Spark performance tuning (shuffle, memory, GC tuning, partitioning).\n• Proficiency in Scala and/or Java (Python experience a plus).\n• Familiarity with Velox and Gluten.\n• Experience with distributed systems and JVM tuning.\n• Knowledge of Parquet, ORC, Arrow, and other columnar formats.\n• Hands-on experience with monitoring tools such as Spark UI, Prometheus, and Grafana.\n\nPreferred Qualifications\n• Contributions to Apache Spark or related open-source projects.\n• Familiarity with C++ for query engine optimization.\n• Experience with large-scale cloud deployments (AWS EMR, Databricks, GCP Dataproc, Azure Synapse).\n\nJob Type: Full-time\n\nPay: ₹1,243,420.91 - ₹4,538,447.54 per year\n\nBenefits:\n• Flexible schedule\n• Health insurance\n• Paid sick time\n• Paid time off\n• Provident Fund\n• Work from home\n\nWork Location: Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=LJ62fDUbwkOevjRkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFy7uDvdqFJbEVx0KiiiawXdyrU9ktj2LuQi1C_yN63LW1_ynSWPkthJgMJjYwlKj6GDMxvHRAGWJ_K9fAbimML9zY5NCsgtXDlSYOx1BRu4SQ1KGBoLwnARMT0tjjZGr4c8V-0zoxGja7JGhlyYahnzl9T6p1KLYVowUrXbb8fMs1nPiw7VPsHxNLUOf6KxurOqAAAA&shmds=v1_AdeF8KjbctUqTTSP6-SWypiOHgyAT7JGHj-kIuH8cWhoQVsqqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=LJ62fDUbwkOevjRkAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "Visa",
         "India",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSq1EcFFN1H8mYSCa7m2aRKNdyF3Q30LH1n8hq_4TopDrTgMcERFOJELZG2GeZ0wv0q4f9QzlXDBnjktYAU3bkEs5s4DE5yZXbSzvVdNsjNGJFZOFDV0Vcdvw2RbHs2TW_nXiMdsU0S1zWa7HqtEbjl9BEEIBFfqA_4AG-QyopMAAAA&shmds=v1_AdeF8KjOBn1yJUpUCbIoZfiL9XOirLPsVpNYluZLK1C-IlMUqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Big Data Engineer - Spark/Scala",
         "The IT Firm",
         "India",
         "Position : Big Data Engineer\n\nIndustry : Information Technology / Data Engineering\n\nDepartment : Data Engineering\n\nJob Overview\n\nWe are looking for a highly skilled and motivated Big Data Engineer with expertise in Apache Spark, Scala, and Java to join our dynamic team. The ideal candidate will have experience in designing, developing, and implementing scalable and high-performance data processing systems. The successful candidate will work on large-scale data sets and build robust data pipelines to extract, transform, and load (ETL) complex data from various sources to facilitate analytics and :\n• Design and develop scalable, high-performance data pipelines using Apache Spark.\n• Write efficient code in Scala and Java for distributed data processing tasks.\n• Collaborate with data scientists, analysts, and other engineers to integrate various data sources and ensure smooth data flow.\n• Create and maintain ETL processes that handle structured, semi-structured, and unstructured data.\n• Optimize performance of data processing tasks, identify bottlenecks, and implement solutions to improve efficiency.\n• Perform data ingestion from various sources, including relational databases, APIs, and file systems.\n• Develop and maintain real-time data streaming solutions using Apache Kafka and Spark Streaming.\n• Ensure data quality and integrity throughout the data pipeline.\n• Troubleshoot and debug issues across the data pipeline.\n• Work with cloud platforms (e.g., AWS, Azure, or GCP) to deploy and scale Big Data solutions.\n• Build and maintain data models and assist with data architecture decisions.\n• Conduct unit tests and ensure the deployment of high-quality code.\n• Stay up to date with industry trends and emerging technologies in the Big Data ecosystem.\n\nRequired Skills And Qualifications\n• Bachelor's or Master's degree in Computer Science, Engineering, or related field.\n• 5+ years of hands-on experience in Big Data technologies, specifically with Apache Spark.\n• Proficient in Scala and Java programming languages.\n• Strong knowledge of Hadoop ecosystem (Hive, HBase, Pig, etc.).\n• Expertise in Spark SQL and working with structured and unstructured data.\n• Experience with NoSQL databases (e.g., MongoDB, Cassandra).\n• Understanding of distributed computing concepts and parallel processing.\n• Hands-on experience with Apache Kafka, Flume, or similar messaging platforms.\n• Familiarity with cloud-based Big Data solutions (AWS, Google Cloud, or Microsoft Azure).\n• Experience in data integration and transformation using ETL tools.\n• Strong problem-solving skills with the ability to troubleshoot complex data-related issues.\n• Familiarity with data warehousing concepts and tools.\n• Experience with version control tools (e.g., Git).\n• Ability to work in an agile, fast-paced environment.\n\nPreferred Skills\n• Knowledge of Apache Hudi or Delta Lake for managing large datasets.\n• Experience with Kubernetes and Docker for containerized deployment.\n• Familiarity with Apache Airflow for orchestrating data workflows.\n• Understanding of Machine Learning concepts and integration with Big Data platforms.\n• Prior experience working in an Agile team environment.\n\nBenefits\n• Competitive salary and benefits package.\n• Work with cutting-edge technologies in a collaborative and fast-paced environment.\n• Opportunity for career growth and development in Big Data and cloud technologies.\n• Flexible working hours and remote work options.\n• Health and wellness programs.\n\n(ref:hirist.tech)",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=G4OalA2QwET-irlQAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHvQrCMBAAYFw7OzndLJiIoINu4g91bfdyjUcSTe9CLkPfwZdWlw--5rNoDufo4YIV4co-MlGBDXQZy9t2DhP-9pARlLC4AMJwF_GJVqdQa9ajtarJeK1YozNOJitMo8z2JaP-GTRgoZyw0rDbb2eT2a-XfSBoe7jFMkFkaPkZ8Qu5yf47jgAAAA&shmds=v1_AdeF8KhAh6hgrFZo3jIxPiI_hhf9k-NtPBR_cn2OP0edPnuUJw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=G4OalA2QwET-irlQAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Senior Lead Engineer(Python & Spark )",
         "IND201 Refinitiv India Shared Services Private Limited",
         "India",
         "Senior Lead Engineer (Python + Spark in AWS) Main Responsibilities / Accountabilities Looking for 10 t 14 years of proven experience Design, build and maintain robust, scalable and efficient ETL pipelines using Python and Spark, ensuring alignment with data lakehouse architecture on AWS Develop and optimize workflows leveraging AWS services such as Glue, Glue Data Catalog, Lambda and S3. Implement data quality and governance frameworks to ensure reliable and consistent data processing across the platform. Collaborate with cross-functional teams to gather requirements, provide technical insights and deliver high-quality data solutions. Drive the migration of existing data processing workflows to the lakehouse architecture, leveraging Iceberg capabilities Establish and enforce best practices for coding standards, design patterns, and system architecture. Monitor and improve system performance and data reliability through proactive analysis and optimization techniques. Lead technical discussions, mentor team members, and foster a culture of continuous learning and innovation. Ensure all solutions are secure, compliant, and meet company and industry standards. Key Relationships Senior Management and Architectural Group Development Managers and Team Leads Data Engineers and Analysts Agile team members Crucial Skills/Experience Extensive Expertise in Python and Spark: Consistent track record of designing and implementing complex data processing workflows. AWS Services: Strong experience with AWS Glue, Glue Data Catalog, Lambda, S3 and EMR with a focus on data lakehouse solutions. Data Quality and Governance: Deep understanding of data quality frameworks, data contracts and governance standard processes. Scalable Architecture: Ability to design and implement scalable, maintainable, and secure architectures using modern data technologies. Iceberg and Lakehouse: Hands-on experience with Apache Iceberg and its integration within data lakehouse environments Problem-Solving and Performance Optimization: Expertise in identifying bottlenecks and optimizing data workflows for performance and cost-efficiency Agile Methodologies: Strong experience in Agile development, including sprint planning, reviews, and retrospectives Interpersonal Skills: Excellent verbal and written communication, with the ability to articulate complex technical solutions to diverse audiences Desired Skills/Experience Familiarity with additional programming languages (eg. Java). Experience with serverless computing paradigms. Knowledge of data visualization or reporting tools for stakeholder communication. Certification in AWS or data engineering (eg. AWS Certified Data Analytics, Certified Spark Developer). Education/Certifications A bachelor's degree in Computer Science, Software Engineering or a related field is helpful. Equivalent professional experience or certifications will also be considered. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organization of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone’s race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it’s used for, and how it’s obtained, your rights and how to contact us as a data subject. If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice. Learn more about our graduate and internship programmes. If you want to apply for a job, please click the Apply button. You will then be redirected to our Careers sign-in page where you can enter your existing credentials or set up an account with us. If there is nothing that currently suits you, feel free to send us your Resume/CV LSEG (London Stock Exchange Group) is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our culture of connecting, creating opportunity and delivering excellence shapes how we think, how we do things and how we help our people fulfil their potential. Our Data & Analytics, Capital Markets and Post Trade divisions have a combined power that provides a comprehensive, integrated suite of trusted financial market infrastructure services to help our customers pursue their ambitions. Explore our divisions LSEG is headquartered in the United Kingdom, with significant operations in 70 countries across Europe, the Middle East, Africa, North America, Latin America and Asia Pacific. Find out more Get to know some of our people who are pushing the boundaries of technology, finance and more around the world.",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=M33odOQv5NcwSnbJAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWLwQqCQBBA6eondJpTVJCa0KWCLkUYEpIfIOs66ZTOyO4i9m99XEqXd3nved-Zd8yQSQwkqEq4cEWMaJbpx9XCsICsU-YNK9jATQqwqIyuYTRXkarB-aF2rrP7ILC28SvrlCPta2kDYSxkCF5S2Am5rZXBrlEO82gXDn7H1foU389RuIUHPonJUQ8xl6Qgm9oSMjQ9abSQGurHERJqyY2C-B_-AHo-x7G_AAAA&shmds=v1_AdeF8Kg_AQkR9dVinW1WCvWDkDu6ppjbCKxt9wGM47k7T8ExJA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=M33odOQv5NcwSnbJAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Senior Spark Data Engineer",
         "Nestor Technologies",
         "India",
         "Qualifications:Bachelors degree in computer science or a related field10+ years of professional experienceStrong SQL skills and knowledge in python requiredSeveral years experience with Spark and Databricks",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=fF8lrVvnb9SdZ6hNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNoQ4CMQwA0GDvEwiimoQNSDBgIQQE5vCX3mi2wWiXteK-gm8GzLOv-8y6TU-cpUFfsb3giIZw4piZqMEKrjKCEraQQBjOIrHQ_JDMqu69Vy0uqqHl4IK8vTCNMvmnjPpn0ISNakGjYbtbT65yXC5upPb77hQSS5GYSSEzXPiR8QtFFSKbkQAAAA&shmds=v1_AdeF8KhZkjeTrmpbUjnRnYvh_sF6rhv8NM8yxYj0YbYNPJ1Z_g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=fF8lrVvnb9SdZ6hNAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Sr. Software Engineer - Java & Python development, Kafka, Spark, SQL",
         "Visa",
         "India",
         "Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nVisa’s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world’s most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you’ll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms.\n\nThe Opportunity:\n\nWe are looking for Versatile, curious, and energetic Software Engineers who embrace solving complex challenges on a global scale. As a Visa Software Engineer, you will be an integral part of a multi-functional development team inventing, designing, building, and testing software products that reach a truly global customer base. While building components of powerful payment technology, you will get to see your efforts shaping the digital future of monetary transactions.\n\nThe Work itself:\n• Design code and systems that touch 40% of the world population while influencing Visa’s internal standards for scalability, security, and reusability\n• Collaborate multi-functionally to create design artifacts and develop best-in-class software solutions for multiple Visa technical offerings\n• Actively contribute to product quality improvements, valuable service technology, and new business flows in diverse agile squads\n• Develop robust and scalable products intended for a myriad of customers including end-user merchants, b2b, and business to government solutions.\n• Leverage innovative technologies to build the next generation of Payment Services, Transaction Platforms, Real-Time Payments, and Buy Now Pay Later Technology\n• Opportunities to make a difference on a global or local scale through mentorship and continued learning opportunities\n\nEssential Functions:\n• Managing OpenSearch and Apache Druid Clusters.\n• Write efficient and scalable code in Python and Java to support data engineering projects.\n• Utilize SQL for data analytics and reporting tasks.\n• Engineer and develop solutions using OpenSearch/Elastic including Kafka streaming.\n• Collaborate with cross-functional teams to understand requirements and deliver solutions.\n• Ensure data integrity, quality, and security throughout the data lifecycle.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\nQualifications\n\nBasic Qualifications:\n• Bachelors degree AND 4+ years of relevant work experience\n\nPreferred Qualifications:\n• Bachelor’s degree in computer science, Engineering, or any related field with minimum 4 years of experience\n• 3+ years of experience in Platform Engineering\n• 3+ years of experience in Python and Java programming, Ansible for automation.\n• 3+ years of experience working in a Linux or Unix environment shell scripting.\n• 3+ years of experience in data analytics, including proficiency in SQL.\n• 1+ years of experience in OpenSearch/Elastic Platform\n• 1+ years of experience in Kubernetes, Dockers good to have.\n• 1+ year of experience in managing Kafka, Druid Clusters.\n• Strong problem-solving skills and attention to detail.\n• Excellent communication and collaboration skills.\n• Experience with cloud-based data engineering solutions is good to have.\n• Experience with big data technologies such as Hadoop, Kafka and Spark is good to have.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=6Al4qOfxYxNIcPb9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNuwrCMBRAcS34A053cpCaiOCis4iPQSm4OJTb9jaNTXNDEmr9JP_SdjlnOXCS3yx5ZV5AxnX8oCc4WqUtkYc1XLBHWML9Gxu2UFFPhl1HNqZwxbrFFDKHvh31uE05FxAIfdnAmJ-YlaHFoYnRhb2UIRihQsSoS1FyJ9lSwYN8cxEm5KEZ785gpHy72wzCWbWaP3VA0BbOttL4B6Sze2qsAAAA&shmds=v1_AdeF8Kh8EcAMlDRS5BiephThdTDDA0nhPLsjXsJdb-cVl2V7hA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=6Al4qOfxYxNIcPb9AAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Big Data Lead/ Lead Data Engineer/Spark Tech Lead",
         "Tanisha Systems  Inc",
         "India",
         "Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWJuwrCQBBFsc0nCMLUglkRbLSTiA_skj5MNsNmNZlZdraIP-R3Gk1zLvec7LPIrifvoMCE8CBszZ_zP7PzTBRNGTC-oCLbzXUDd2lACeNkhOEi4npaHruUgh6MUe1zpwmTt7mVwQhTI6N5SqM_1NphpNBjonq33455YLdeVch-ClC-NdGgADe24Hma1uMXP5iqRqkAAAA&shmds=v1_AdeF8KgvLioLXzdCbGcM30mO5raEHL9pykXDnB4l0r3zd6_bmQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Staff Engineer, Data Engineer (.Net Core,Apache Spark)",
         "Nagarro",
         "India",
         "Nagarro is a digital product engineering company that focuses on creating products, services, and experiences using AI. The role involves developing and designing overall solutions while ensuring compliance with both functional and non-functional requirements. Responsibilities include writing high-quality code, translating client requirements into technical designs, and conducting proof of concepts (POCs) to validate solutions. The position emphasizes the importance of collaboration and problem-solving in technology integration scenarios.",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=9rwNNE5ztmyWTtyqAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z3HsQrCMBAAUFw7OTvdqFITEVx0EhVRsEs_oFzjNYnGXEhu6Nf4reLi8uBVn0l1bwWHAc7R-kiUazih4L8wVw0JHDlTfUhoHEGbML8WsIIb91AIs3HAES7MNtBs70RS2WldSlC2CIo3yvBbc6SeR_3kvvzoisNMKaBQt9muR5WiXU4btJgzg49wjQ-PX2afhXGhAAAA&shmds=v1_AdeF8KhuW93HTvnE5Y1bRxWajGLlkaF5gXE33-OZ4XtWSAfkBQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=9rwNNE5ztmyWTtyqAAAAAA%3D%3D",
         "Spark Engineer"
        ],
        [
         "Data Analyst with DBA",
         "ExamRoom.AI",
         "India",
         "Position Title: Data Analyst (with DBA Skills)\n\nDepartment: Data Science\n\nReports To: Team Lead / Tech Lead\n\nWork Location: On-site\n\nPosition Summary\n\nThe Data Analyst with DBA skills will be responsible for supporting both data analytics and database performance needs across the organization. This role includes maintaining data quality, creating actionable reports, administering SQL Server databases, and optimizing query performance. The ideal candidate brings a blend of analytical thinking, database administration expertise, and business intelligence experience.\n\nEssential Duties and Responsibilities\n\nKey responsibilities include, but are not limited to:\n• Develop and maintain complex SQL queries for reporting, data analysis, and performance tuning.\n• Administer SQL Server databases, ensuring uptime, performance, and security of database systems.\n• Perform database performance analysis, capacity planning, and query optimization.\n• Monitor and maintain internal database architecture and configuration best practices.\n• Create insightful dashboards and reports using standard BI tools such as Apache Superset, Power BI, or similar.\n• Analyze large volumes of data to identify trends, anomalies, and insights that support business decisions.\n• Collaborate with engineering and business teams to understand reporting needs and deliver accurate and timely insights.\n• Perform backup and recovery tasks, patch management, and security auditing as part of DBA responsibilities.\n• Document database configurations, data models, and reporting structures for ongoing support and compliance.\n• Ensure data accuracy, consistency, and availability through proper monitoring and maintenance practices.\n\nSupervisory Responsibilities\n• None\n\nQualifications\n\nTo perform this role successfully, candidates must meet the following requirements:\n\nEducation & Experience\n• Bachelor’s degree in Computer Science, Information Systems, Data Analytics, or a related field (preferred).\n• Minimum of 4 years of professional experience in data analysis and database administration.\n• Strong hands-on experience with SQL Server, including query tuning and performance optimization.\n• Deep understanding of database administration tasks, including architecture, configuration, backup/recovery, and security.\n• Experience working with BI/reporting tools such as Apache Superset, Power BI, Tableau, or similar.\n• Strong data analysis, data visualization, and data profiling skills.\n• Familiarity with database indexing, partitioning, and high availability strategies.\n• Strong communication and collaboration skills to work effectively with technical and non-technical stakeholders.\n\nLanguage Skills\n• Ability to understand and create technical documentation.\n• Capable of writing professional correspondence and conveying technical information clearly.\n• Strong verbal communication skills for explaining complex data issues to diverse audiences.\n\nMathematical Skills\n• Proficient in statistics, percentages, and data-driven calculations used in reporting and analysis.\n\nReasoning Ability\n• Strong problem-solving and analytical thinking skills.\n• Ability to diagnose and troubleshoot performance issues independently.\n• Capable of prioritizing tasks and managing time in a fast-paced, data-driven environment.\n\nComputer Skills\n• Proficient in SQL Server database management and performance tuning tools.\n• Familiar with BI/reporting platforms like Apache Superset, Power BI, or Tableau.\n• Knowledge of scripting or automation using SQL or other tools is a plus.\n\nCertificates and Licenses\n• None required (Relevant SQL Server or BI tool certifications are a plus)\n\nTravel Requirements\n• Minimal travel (up to 5%) may be required for training, meetings, or conferences.\n\nWork Environment\n• Work is performed in a standard office environment with climate control.\n• Reasonable accommodations may be made for individuals with disabilities.\n• No substantial exposure to adverse environmental conditions.\n\nInformation Security & Privacy Compliance\n• Follow secure practices for managing data access and system permissions.\n• Ensure sensitive data is encrypted and protected according to internal policies.\n• Comply with data governance and regulatory standards (e.g., GDPR).\n• Support audits and maintain documentation for system changes and configurations.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=cp90yU9CIij6zP4zAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFw7OzndLJqI4KJTpCJ19AfKJYYmJcmF3oFx9M_FN7zuu-p2PQqCKZg-LPCOEqC_GtjDgyywx8UFoAJ3oin5zSWIVD5rzZzUxIISnXKUNRVvqemZLP8bOeDia0Lx4_F0aKqWabu-NcxPoqzMALHAUF4Rf0mCcPqEAAAA&shmds=v1_AdeF8KhiYGMLKnNOEpXvbzTVzdW1N4DhSRHRC-J7zYzenWw4mg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=cp90yU9CIij6zP4zAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Procurement Data Analyst",
         "IN10 (FCRS = IN010) Novartis Healthcare Private Limited",
         "India",
         "We are seeking a skilled ESO Data and Digital Analyst specializing in manufacturing procurement to lead the transformation and optimization of procurement processes through digital solutions. This role involves handling large datasets, preparing presentations for senior management, and supporting operations and strategy with digitalization. The ideal candidate will collaborate with various stakeholders to enhance efficiency, compliance, and performance across the procurement function within manufacturing settings.Key Responsibilities:Digital Transformation Strategy:Develop and implement a digital strategy for manufacturing procurement.Identify opportunities for automation, data analytics, and integrated solutions to drive efficiencies.Presentation and Reporting:Prepare and present data-driven insights and reports to senior management.Create intuitive data visualizations and dashboards to support strategic decisions.Support Procurement Operations and Strategy:Assist in the digitalization of procurement processes to enhance operational efficiency.Support strategic initiatives with data analysis and digital solutions.Stakeholder Management:Collaborate with cross-functional teams including operations, finance, and IT to understand procurement needs and requirements.Engage with suppliers, vendors, and internal stakeholders to ensure alignment on objectives and requirements.Facilitate workshops and meetings to gather feedback and foster strong relationships.Requirements Analysis:Conduct detailed assessments of existing procurement processes and systems.Document functional requirements and develop comprehensive specifications for digital solutions.System Implementation:Lead the implementation of procurement software and tools, ensuring proper integration with existing systems.Oversee user training and change management initiatives to encourage adoption of new technologies.Performance Metrics:Establish KPIs and benchmarking standards to measure the effectiveness of procurement processes.Analyze procurement data to support decision-making and continuous improvement initiatives.Compliance and Risk Management:Ensure procurement practices comply with organizational policies and legal regulations.Identify potential risks in procurement processes and propose mitigation strategies.Continuous Improvement:Stay updated on market trends, technologies, and best practices in procurement and supply chain management.Propose innovative solutions to enhance procurement efficiencies and reduce costs.Essential Requirement:Education: Bachelor’s degree in Supply Chain Management, Business Administration, Information Technology, or a related field; Master’s degree preferred. Experience: Minimum of 5 years of experience in procurement or supply chain management, with a strong focus on digital transformation initiatives. Proficiency in data analysis tools and procurement software (e.g., SAP Ariba, Coupa). Strong quantitative skills to analyze large datasets and identify trends. Skills in creating intuitive data reports and dashboards. Understanding of procurement processes and financial principles. Precision in running complex calculations. Soft Skills: Excellent communication skills; ability to work collaboratively with diverse teams.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=f5MHJHbIKWjhHrvfAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLwQ7BQBAA0Lj2E5zmiES3JC5EQggq0ggfINM1aVe2O83OaPgrn4jLu73k00vMObJ9RmooKGxREdYB_VsUxnDkEoQw2ho4wJ658tRf1KqtzI0R8WkliupsarkxHKjkl3lwKX9uUmOk1qPSbTrLXmkbqtEqLyYZDHabyxWWkBfZJBtCwR1GdQIHQq-1_TU4R9f9Jpxc45Tu4ALk4e7wC_eltiyzAAAA&shmds=v1_AdeF8Khr63cRSOBQEoIZmJjIyZ_b4e1GUP05ezczOmMe293jGw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=f5MHJHbIKWjhHrvfAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Senior Data Analyst with SQL Development",
         "Hitachi Careers",
         "Hyderabad, Telangana, India",
         "Location: Hyderabad\n\nFunction: HV Services\nRequisition ID: 1031134\n\nOur Company\n\nWe're Hitachi Vantara, the data foundation trusted by the world's innovators. Our resilient, high-performance data infrastructure means that customers - from banks to theme parks - can focus on achieving the incredible with data.\n\nIf you've seen the Las Vegas Sphere, you've seen just one example of how we empower businesses to automate, optimize, innovate - and wow their customers. Right now, we're laying the foundation for our next wave of growth. We're looking for people who love being part of a diverse, global team - and who get excited about making a real-world impact with data.\n\nThe Role\n\nOffice Location: Hyderabad (Work from Office)\n\nKey Responsibilities:\n• Lead SQL development: Write and maintain complex queries, stored procedures, triggers, functions, and manage views for high-performance data operations.\n• Advanced Python Programming: Use Python to build ETL pipelines, automate data workflows, and conduct data wrangling and analysis.\n• Data Modeling & Architecture: Develop scalable data models and work closely with engineering to ensure alignment with database design best practices.\n• Database Optimization: Design and implement efficient indexing and partitioning strategies to improve query speed and system performance.\n• Business Insights: Translate large and complex datasets into clear, actionable insights to support strategic business decisions.\n• Cross-functional Collaboration: Partner with product, tech, and business stakeholders to gather requirements and deliver analytics solutions.\n• Mentorship & Review: Guide junior analysts and ensure adherence to coding standards, data quality, and best practices.\n• Visualization & Reporting: Create dashboards and reports using tools like Power BI, Tableau, or Python-based libraries (e.g., Plotly, Matplotlib).\n• Agile & Version Control: Operate in Agile environments with proficiency in Git, JIRA, and continuous integration for data workflows.\nAbout us\n\nWe're a global team of innovators. Together, we harness engineering excellence and passion for insight to co-create meaningful solutions to complex challenges. We turn organizations into data-driven leaders that can a make positive impact on their industries and society. If you believe that innovation can inspire the future, this is the place to fulfil your purpose and achieve your potential.\n\n#LI-RS1\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=yRk2C3CS1DpQuMrGAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCQAwAUFz7CU6ZpbYiuOggYsEqLqJ7SdvQO7km5RK0_Sm_UV3e8pLPLNnfib1EKNAQDoxhUoO3Nwf32xUKelGQoSc2WMJFalDC2DgQhpNIF2i-c2aDbvNcNWSdGppvskb6XJhqGfOn1PqnUoeRhoBG1XqzGrOBu0VWesPGeTj-jqKCZyinliLW2KbwoIDcIWMKZ249fgGCF9fasQAAAA&shmds=v1_AdeF8Kg4_Wgfnakmou8H1JgV1Afbodlsx0K9mPd3fuYY8TpRQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=yRk2C3CS1DpQuMrGAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Senior Data Management Analyst",
         "Experian",
         "Hyderabad, Telangana, India",
         "The Data Management analyst will play an instrumental role in supporting ongoing projects related to data integrity, consumer advocacy, related analytics, and accuracy. The analyst will work with stakeholders to identify opportunities for data accuracy, business process re-engineering, and provide insights to improve data management.You will be reporting to a Senior Manager.You are required to work from Hyderabad as its a Hybrid working (2 days WFO)\n\nKey Responsibilities\n• Identify, analyze, and interpret trends and patterns in core Consumer, Clarity and Rent bureau, and Ops processing data to help make business decisions.\n• Design new analytical workflows, processes, and/or optimize existing workflows with the goal to streamline processes and enable other analysts to self-service analytics.\n• Convert high level business requirements into clear technical specifications, process flow diagrams, and queries.\n• Effectively summarize, present actional insights and recommendations to the management team. Be a great story teller!\n• Consult with internal clients on data quality issues and partner with them to set up remediation and monitoring programs.\n• Engage with internal teams like data operation, data governance, compliance, audit, product development, consumer assistance center and gather requirements for business process re-engineering and improving data accuracy.\n\nAbout Experian\n\nExperian is a global data and technology company, powering opportunities for people and businesses around the world. We help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. We also assist millions of people to realize their financial goals and help them save time and money.\n\nWe operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments.\n\nWe invest in people and new advanced technologies to unlock the power of data. As a FTSE 100 Index company listed on the London Stock Exchange (EXPN), we have a team of 22,500 people across 32 countries. Our corporate headquarters are in Dublin, Ireland. Learn more at experianplc.com.\n\nExperience and Skills\n• Bachelor's degree in Data science, Engineering, Computer Science, Information Management, Statistics, related field, or equivalent experience is required.\n• 5+ years of experience in Data Analytics roles.\n• Expertise in SQL and one of the databases like SQL server, MySQL, or Aurora is required.\n• Experience analyzing large datasets and familiarity with one of analytical tools like Alteryx, Python, SAS, R, or equivalent tool is required.\n• Experience working with BI tools like Tableau, Qlik, and MS Office tools.\n• Experience with Metro2 data quality, public records, credit inquiries and consumer disputes\n• Experience with data modeling, GenAI, machine learning, and tools like Python, Spark, Athena, Hive is desirable.\n• Navigate a rather complex business environment and willingness to learn new business processes, tools and techniques is needed.\n\nAdditional Information\n\nOur uniqueness is that we truly celebrate yours. Experian's culture and people are important differentiators. We take our people agenda very seriously and focus on what truly matters; DEI, work/life balance, development, authenticity, engagement, collaboration, wellness, reward & recognition, volunteering... the list goes on. Experian's strong people first approach is award winning; Great Place To Work™ in 24 countries, FORTUNE Best Companies to work and Glassdoor Best Places to Work (globally 4.4 Stars) to name a few. Check out Experian Life on social or our Careers Site to understand why.\n\nExperian is proud to be an Equal Opportunity and Affirmative Action employer. Innovation is a critical part of Experian's DNA and practices, and our diverse workforce drives our success. Everyone can succeed at Experian and bring their whole self to work, irrespective of their gender, ethnicity, religion, color, sexuality, physical ability or age. If you have a disability or special need that requires accommodation, please let us know at the earliest opportunity.\n\nExperian Careers - Creating a better tomorrow together\n\nBenefits\n\nExperian care for employee's work life balance, health, safety and wellbeing. In support of this endeavor, we offer best-in-class family well-being benefits, enhanced medical benefits and paid time off.\n\n#LI-Hybrid\n\nThis is a hybrid /in-office role.\n\nExperian Careers - Creating a better tomorrow together\n\nFind out what its like to work for Experian by clicking here",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE7ZBKk9EbroJCj-gJPuJW3D9eSaHJcM7cP4ruryzV_xWRT1kzhIhhMawgMZPY3EBkfGOKvBBu7SghLmbgBhuIj4SMvDYJZ075xqrLwaWuiqTkYnTK1M7i2t_ml0wEwpolGzq7dTldivV-cpUQ7IEBiuc08ZW-xLeFFE9r9DCTfuA34BLmjMQaAAAAA&shmds=v1_AdeF8KgmPumRSh5zjXnBPATcQ4TT0jmaqf8aqTIMkmWe6402Ow&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Associate/Analyst - Data Analytics",
         "D. E. Shaw India",
         "Hyderabad, Telangana, India",
         "The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNzQrCMBCE8dpH8LRnqYkIHtRToeLPVe9lk4YkJe6WbsD2kXxLq14G5psPpngvin0lwjZidroiTJNkWEONGeFXc7QygxsbEIeDDcAEZ2af3PIYcu7loLVIUl4yzrKy_NRMzvCoOzbyjUYCDq5P80ez3W1G1ZNf6VrBScE94Auu1EaESHCZWjegwbaEh0tIHgnL__wBfMXaI6wAAAA&shmds=v1_AdeF8KiGYFSm74ttc37WRtV6BAmPdDCIXraWSL4lzvQKbqdoJQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Manager - Data Analyst",
         "BNP Paribas India Solutions",
         "India",
         "About BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function:\n\nThe jobholder will be a part of the ISPL KPI Factory with a global mission to monitor and foster performance at group & division level.\n\nJob Title:\n\nData Analyst\n\nDate:\n\n7-Jan-2025\n\nDepartment:\n\nCAO\n\nLocation:\n\nChennai\n\nBusiness Line / Function:\n\nTransitions and Project Office\n\nReports to:\n\n(Direct)\n\nDivya Sundar\n\nGrade:\n\n(if applicable)\n\nNA\n\n(Functional)\n\nNumber of Direct Reports:\n\nNA\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nKPI and Reporting team is in charge to produce dashboards and reports to the regional/local Business, Management, Operations and Functions across the Group. The Candidate will be responsible for managing the entire lifecycle, from requirements gathering to deployment, and will work closely with cross-functional teams to ensure successful project delivery.\n\n· Monitor data inputs from stakeholders’ teams to make sure there is an alignment with the global strategy and targets.\n\n· Contribute to the implementation, maintenance and optimization of the tool dedicated to reports creation.\n\n· Business partnering with Business managers on regular basis and contribute to Adhoc requests.\n\n· Develop analyses to support strategic initiatives and decision support.\n\n· Assist in identifying process improvements in production, control procedures, and workflow organization to increase the team’s efficiency and effectiveness.\n\n· Participate in the upgrades / version releases of respective reporting tools, as well as system amendments / enhancements required to facilitate new / modified reporting requirements. In connection with this effort, assist in user acceptance testing and troubleshooting after deployment.\n\n· Help document operational process improvements in production, control procedures, and workflow organization to support team’s efficiency and effectiveness initiatives. Also, assist in integrating and streamlining the reporting under the team’s responsibility.\n\nResponsibilities\n\nDirect Responsibilities:\n\n· PowerBI Dashboard Development and Maintenance\n\n· Produce key reports by various Business Units and Entities\n\n· Address any ad-hoc requests and be the backup for performance dashboards.\n\n· Contribute to projects whenever necessary, involving Power BI dashboards creation/maintenance and the feeding of the dashboards developed in Python language.\n\n· Maintenance/ Enhancements of the ETL tool used to pull, transform and load the data from source applications to our database.\n\n· Should be focused on automation initiatives to reduce manual activities.\n\n· Propose improvements related to the support activity.\n\n· Ensure requests from business users including technical and functional queries are answered immediately with qualified inputs, End user functional support and ensure Business Continuity objectives are met.\n\n· Maintains effective relationships with core and extended program team members, peers, senior stakeholders and business managers.\n\n· Ensure full compliance to organization and project specific policies, procedure and guidelines.\nTechnical & Behavioral Competencies\n\n· 4-5 years of experience in reporting, business intelligence\n\n· Strong analytical skills\n\n· Expertise in PowerBI and SQL\n\n· Knowledge of Python and ETL is preferred.\n\n· Good knowledge of our environment (products, markets, practices, processes, systems, organization and people).\n\n· Ability to deliver the various phases (framing, study / design, execution, completion/closure) of large / complex projects.\n\n· Ability to develop a culture of the planning, be able to keep a direction to meet the deadlines in an autonomous manner.\n\n· Ability to take decisions linked to the project when necessary.\n\nSoft Skills\n\n· Strong analytical skills\n\n· Strong problem-solving\n\n· Comfortable operating with a strong level of autonomy, self-driven\n\n· Organized and delivery-focused, with attention to detail.\n\n· Good communication and reporting skills.\n\n· Working capacity and efficiency.\n\n· Team Player/ Networking Skills\nSpecific Qualifications (if required)\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to deliver / Results driven\n\nAbility to collaborate / Teamwork\n\nResilience\n\nCritical thinking\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to manage a project\n\nAbility to anticipate business / strategic evolution\n\nAbility to understand, explain and support change\n\nAbility to develop and adapt a process\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 8 years\n\nOther/Specific Qualifications (if required)",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=hRqS-JTYRygERb1SAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXIMQoCMRBAUWz3CFZTWAmbiGCjlSKIgrLgAZbJGpJInAmZEdZreGIVmw_vN-9J056RMPgKLexREbaE-SX65YkdiMc6RGCCA3PIfrqJqkXW1opkE0RR02AGflgm73i0d3bySy8Rqy8Z1ffL1WI0hcJ8trt00GFNDgWOdEsIV85PTUwCif7rA_P3q7SVAAAA&shmds=v1_AdeF8Kj5RFpRe3R0U3v3dda-0pNtvGyyGb3WjomNtLQczBqGeQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=hRqS-JTYRygERb1SAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Data Analyst, WW Returns & Recommerce",
         "Amazon",
         "Hyderabad, Telangana, India",
         "Description\n\nWe are looking for an experienced Data Analyst to join the Reverse Logistics Services team to help unlock insights which take our team to the next level. The ideal candidate will be excited about understanding and implementing new and repeatable processes, while providing data to improve the Returns transportation business. The successful candidate will have strong data mining and modeling skills and is comfortable facilitating ideas and working from concept through to execution. They will partner with various key stakeholders to deep dive into the business challenges and data to identify insights, providing recommendations for process improvement on a globally scalable level. This role requires an individual with excellent analytical abilities as well as strong business acumen.\n\nKey job responsibilities\n• Retrieving and analyzing data using Excel, SQL, and other data management systems\n• Monitoring existing metrics, analyzing data and partnering with internal/external teams to identify process and system improvement opportunities\n• Design, develop and maintain scalable, automated, user-friendly systems, reports, or dashboards to enable stakeholders to manage the business and drive effective decisions\n• Prepare and deliver business requirements reviews to leadership teams\n• Excellent writing skills, to create artifacts easily digestible by business and tech partners.\n• Be self-driven, and show ability to deliver on ambiguous projects with incomplete data\n\nAbout The Team\n\nAt Amazon Worldwide Returns & ReCommerce (WW R&R), we aspire to zero: zero cost of returns, zero waste, and zero defects.\n\nWe are an agile, innovative organization dedicated to ‘making zero happen’ to benefit our customers, company, and environment. We are constantly innovating to create long-term value at Amazon by investing in the future and focusing on the planet, not just on the bottom line. WW R&R includes business, product, program, operational, data, and software engineering teams, who together manage the lifecycle of returned and damaged products.\n\nBasic Qualifications\n• Bachelor’s degree in Business, STEM, Operations, Finance, or related field\n• Experience in a business analyst, data analyst or statistical analysis role\n• Experience managing and influencing key metrics\n• Ability to analyze associate and customer inputs to influence internal and external partners\n• Experience with MS Outlook, Excel, and Word\n• Strong written communications skills\n• Experience with data visualization tools like Tableau and/or experience with SQL & QuickSight\n\nPreferred Qualifications\n• Experience in Retail, Transportation or Operations\n• Advanced SQL skills\n• Experience with Python, R or other scripting languages\n• Experience communicating across all levels of management, peers, and partners\n\nOur inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.\n\nCompany - Amazon Dev Center India - Hyderabad\n\nJob ID: A2920316",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=OwQO9R-Bmx5ieTRKAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMuwrCQBAAsc0nWG2jhcScCDZqExB8lCKkDHuX5RK57IbbE4yf45cam2Gamew7y44nTAglYxg15VBVcKf0iqywnMxJ31N0BGu4iQUljK4FYTiL-EDzQ5vSoHtjVEPhNWHqXDFFRpisvM1TrP5Ra4uRhoCJ6u1u8y4G9qtF2eNnenUMl7GhiBabHB4UkD0y5nDlpsMfKUcboKUAAAA&shmds=v1_AdeF8Kj0MQM5XQhNHaUCBdrqmD3v_Ee4wDVD7upBPZI5IJKRFQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=OwQO9R-Bmx5ieTRKAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Sr. Data Analyst",
         "iCIMS Talent Acquisition",
         "Rai Durg, Telangana, India",
         "Job Overview\n\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n\nAbout Us\n\nWhen you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n\nResponsibilities\n• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n\nAdditional Job Responsibilities: \n• Produce and adapt data visualizations in response to business requests for internal and external use\n• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n\nQualifications\n• 5-10 years professional experience working in an analytics capacity\n• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n• Strong data analytics and visualization skills\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n\nPreferred\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n\nEEO Statement\n\niCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n\nWe are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n\nCompensation and Benefits\n\nCompetitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU63CVITEXTQqViQCi62e7nWkJ7ES82dUH_Gb1WXN77sM8sWdTJQoiIUjOEtCis4xw7EYeoHiAynGH1w88OgOsreWpFgvCgq9aaPDxvZdXGy99jJn1YGTG4MqK7dbNeTGdkvd3SsLjU0GBwrFP3zRUJKv50YrkhQvpLPoXEB2SNjDhXfCL--pAUHoQAAAA&shmds=v1_AdeF8KitaSipmTLybCgiXZCrA1aS7QWwHWuaNBynowT2b-63_w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Data Science Analyst",
         "IN10 (FCRS = IN010) Novartis Healthcare Private Limited",
         "India",
         "About the Role:Job Description Summary: The Insights and Decision Science (IDS) team is dedicated to enabling improved decision-making at Novartis by leveraging data and advanced analytics capabilities to generate actionable insights that drive business growth. We collaborate closely with the US business, bringing in-sights and challenging ideas to empower smarter, data-driven decision-making. Launch and BD&L Insights & Analytics, is crucial in orchestrating the strategic launch of new products within the therapeutic area, ensuring that each launch is informed by comprehensive market research and analytics. This role will be responsible for creating and maintaining various reports and deliverables. They will be involved in delivering analytical solutions for business problems related to Launch and BD&L products. They will also help in using analytical, statistical and Data Science skills to answer ad-hoc business questions and drive data-driven decisions. They will also be collaborating with other teams to facilitate various downstream processes.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:· Deliver analytical solutions as an individual contributor for various projects related to quantification of patient journey, informing & validating forecast assumptions and other ad-hoc business questions related to launch products.· Support exploratory research to identify new areas of application of patient / provider analytics in providing enhanced decision-making support.· Development and automation of project codes and deliverables.· Participate in various knowledge sharing sessions that enable growth and improves quality of deliverables across the function.· Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes.Minimum Requirements:· Minimum 1 year of experience in the pharmaceutical or healthcare industry.· Strong analytical and problem-solving skills to extract insights from complex APLD data sets and identify patterns and trends.· Excellent communication and interpersonal skills, with the ability to influence and collaborate with cross-functional teams.· Proficiency in data analytics tools, platforms and languages like SQL and Python, with the ability to translate data into actionable insights.· Strong problem-solving skills and a strategic mindset, with the ability to anticipate challenges and develop innovative solutions.Education:Bachelor's degree in related field is required; Master of Science and/or MBA strongly preferred.Preferred Skills:· Ability to balance operational execution with high-level strategic thinking, supporting both day-to-day performance and long-term business goals· Ability to work collaboratively with cross-functional teams, including sales, Data Operations, and product development and drive strategic initiatives· Proficiency in insight and hypothesis generation, data science, and primary and secondary research methodologies· Ability to thrive in a fast-paced, dynamic environment and adapt to changing business needs and priorities.Why Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=E8qUCATb85C0rqR8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLzQqCQBAAYLr6CJ3m2A-pBV2KoCgqIyTyAWRcB91Yd8QZxN6ph6wu3-0LPqNgfkJFyIwlbwgOHt1bFBZw4wKEsDM1sIcLc-VovK1VW9lEkYgLK1FUa0LDTcSeCh6iFxfyJ5caO2odKuWrdTyEra9m-yRdxjA5H58Z7CBJ42U8hZR77NQKXAmd1ubX4NHZ_jfhbhurVIL1kPjS4hdRPmF7rwAAAA&shmds=v1_AdeF8KitnZKbCILU3LI82mL07zTe7Dv_v83dUFqGICYpHkWx8w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=E8qUCATb85C0rqR8AAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)",
         "Dupont",
         "Hyderabad, Telangana, India",
         "At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n\nJob Summary:\n\nThe Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n\nKey Areas of Expertise and Responsibilities:\n\n1. Visual Basic for Applications (VBA)\n• Responsibilities:\n• Develop and maintain complex VBA applications to automate repetitive tasks.\n• Incorporate SAP Scripting within VBA to optimize business processes.\n• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n• Criteria:\n• Advanced proficiency in VBA programming.\n• Demonstrated experience with SAP interfaces and scripting.\n• Ability to write modular, efficient, and maintainable code.\n• Knowledge of Excel object model and its functionalities.\n\n2. Power Query\n• Responsibilities:\n• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n• Develop and maintain data models in Excel to streamline data preparation.\n• Create and optimize Power Query scripts for efficient data processing.\n• Criteria:\n• Intermediate experience with Power Query including M language for data transformation.\n• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n• Ability to perform data cleansing and manipulation through Power Query.\n\n3. Power BI\n• Responsibilities:\n• Create interactive, user-friendly dashboards and reports using Power BI.\n• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n• Optimize Power BI reports for performance and usability.\n• Criteria:\n• Intermediate knowledge of Power BI Desktop and Power BI Service.\n• Ability to create DAX measures and calculated columns for enhanced analytics.\n• Familiarity with data visualization best practices and techniques.\n\n4. Python\n• Responsibilities:\n• Develop Python scripts to automate data manipulation and Excel-related tasks.\n• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n• Collaborate with the data team to integrate Python solutions with existing tools.\n• Criteria:\n• Intermediate proficiency in Python, especially in data manipulation and automation.\n• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n• Understanding of APIs and ability to retrieve data programmatically.\n\nQualifications:\n• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills and the ability to work collaboratively with diverse teams.\n\nPreferred Skills:\n• Experience with SQL and relational databases for data querying and data management.\n• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n• Knowledge of machine learning principles is an advantage.\n• Understanding of data warehousing concepts and methodologies.\n\nJoin our Talent Community to stay connected with us!\n\nOn May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n\n(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n\nDuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n\nDuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWLMU7DQBBFRZsjUP0GiQRjI5Q0oUoUAUkFEqKNxvbEu2iZWe1OhPeSOROmoPl6T09_drmauR0ZYSMUSjY8eyHpGLef202FN_3hhPczp_Iv2_1ExZzKHMs7FKaUoSfwGDl5nr5z3OOgLfKUOgcVvKgOga-fnFnM66bJOdRDNjLf1Z1-Nyrc6th8aZv_5pgdJY6BjI-Pq4exjjIsbnbnqGLwgtfSc6KW-gofHEgGEqqwl97TL1wZQbrRAAAA&shmds=v1_AdeF8KjE_yEZOuigr5UIe33mQKphFiyPOSsPa5mAJd61oGdJsA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D",
         "Data Analyst"
        ],
        [
         "AI/ML Engineer – XR Software (All levels))",
         "Qualcomm",
         "Bengaluru, Karnataka, India",
         "Company:\nQualcomm India Private Limited\n\nJob Area:\nEngineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nWe are seeking a passionate and skilled AI/ML Engineer to join our cutting-edge Extended Reality (XR) Software team. In this role, you will work on next-generation XR products that blend the physical and digital worlds, leveraging artificial intelligence and machine learning to create immersive, intelligent, and responsive experiences.\n\nYou will collaborate with cross-functional teams of researchers, engineers, and designers to build real-time AI/ML software optimized for XR platforms. A strong background in C++ or embedded firmware development is essential, as you will be working close to hardware and performance-critical systems.\n\nKey Responsibilities\n• Design, develop, and optimize AI/ML models for XR applications such as computer vision, sensor fusion, gesture recognition, and spatial understanding.\n• Implement real-time inference pipelines on embedded or edge devices.\n• Collaborate with firmware and hardware teams to integrate ML models into XR systems.\n• Analyze system performance and optimize for latency, power, and memory.\n• Stay up to date with the latest research and trends in AI/ML and XR technologies.\n• Contribute to the full lifecycle of product development—from prototyping to production.\n\nRequired Qualifications\n• Bachelor’s or Master’s degree in Computer Science, Electrical Engineering, or a related field.\n• 1–10 years of industry experience in AI/ML engineering or embedded systems.\n• Proficiency in C++ and/or embedded firmware development.\n• Solid understanding of machine learning fundamentals and experience with frameworks like TensorFlow, PyTorch, or ONNX.\n• Experience with deploying ML models on edge devices\n• Familiarity with XR technologies (AR/VR/MR), sensor data processing, or 3D spatial computing.\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n\nApplicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\nIf you would like more information about this role, please contact Qualcomm Careers.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuw4BURCA4Wj3EVTTuYQ9ItFQISLrUqDRyewaZ5cxsznnLErv4DG8lSdB81df_uhdi6bjxKxXMBNbCJGDz_MF-y3s9BTu6AiaY2ZguhH7Vgu6sNAUPKHLclCBuaplqo_yEEo_NMZ7jq0PGIoszvRqVCjVhzlr6v85-Pz3LBkDHfqD3iMuxbYbmwr5h69QCExILHLlqg4s0QkGvGAHEjkW-AWsJyrJrgAAAA&shmds=v1_AdeF8Kjy55dYa7U3WoErm860Lebff2UCLLCE78OUgv0RP_zcbw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "Lead AI Engineer",
         "Keywords Studios",
         "Pune, Maharashtra, India",
         "About the team:You will be a part of the AI team, which is responsible for developing AI features at Helpshift.The AI team has built product features like User Intent Detection, AI-Powered Answers,Agent CoPilot features, etc by building our own ML models and leveraging generative AI.The team consists of Backend and Frontend Developers, Full-stack Developers, MLEngineers and Data Scientists.About the role:We are looking for a seasoned engineer with a passion for Artificial Intelligence and a knackfor leadership. In this role, you'll be instrumental in shaping our AI products, leading a groupof engineers within the team, and driving the successful delivery of innovative solutions.You'll blend deep technical expertise in AI with strong leadership skills, guiding a team offull-stack, backend, and frontend developers. You'll be responsible for the end-to-enddelivery of AI-powered features and products, from conceptualization and design todeployment and optimization.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKOwrCQBAAUGxzBKvpBIlZEWy0shCJHxA8QJhkh92VOBN2Jqj38MBq86pXfCbF7EzoYVfDnkNiogwLOEoLSpi7CMJwEAk9TbfRbNCNc6p9FdTQUld18nDC1MrL3aXVP41GzDT0aNSs1stXNXCYlyd6PyV7hZuNPolCYriOTCVc8PdRo2UsoWaf8AtFRB7ClwAAAA&shmds=v1_AdeF8KiNR20bIkNkNAivhghX5MFAQu0T9NqGzMXN5rQ3M58NCQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "Senior Application Engineer for Artificial Intelligence (AI)",
         "MathWorks",
         "New Delhi, Delhi, India",
         "Job Summary\n\nMathWorks has a hybrid work model that enables staff members to split their time between office and home. The hybrid model provides the advantage of having both in-person time with colleagues and flexible at-home life optimizations. Learn More: https://www.mathworks.com/company/jobs/resources/applying-and-interviewing.html#onboarding.\n\nAs a Senior Application Engineer for Artificial Intelligence (AI) you will partner with our most innovative customers across many industries to establish MATLAB as a platform for these application areas by\n• Developing machine learning, and deep learning algorithms in MATLAB\n• Integrating algorithms developed in MATLAB into full-fledged applications\n• Deploying applications developed in MATLAB to enterprise-wide systems\n\nYou will work in a pre-sales manner with our customers to first identify and understand their technical and business challenges and then develop compelling demonstrations of solutions that help them appreciate how the adoption of MathWorks products add value towards enhancing their workflows.\n\nUsing your technical expertise as well as your excellent interpersonal, communication, and presentation skills, you will engage customers and prospects in seminars, conference calls, meetings, and through product evaluations to develop a shared vision for success.\n\nMathWorks nurtures growth, appreciates diversity, encourages initiative, values teamwork, shares success, and rewards excellence.\n\nResponsibilities\n\nProvide technical pre-sales support and guidance to the India sales organization, prospects, and customers in the domains of statistics, machine learning, deep learning and application deployment by:\n• Preparing and delivering presentations, demonstrations, and application examples in customer meetings, seminars, and other public events\n• Managing product evaluations and developing adoption plans that assist customers in adopting MathWorks products\n• Partnering with sales representatives to help develop account and territory level selling strategies\n• Identify new trends and application areas and provide feedback to development and marketing teams. Collaborate with the worldwide team on developing compelling messages and demonstrations.\n• Advocate for the future direction of MathWorks products based on customer interactions.\n• Establish rapport and credibility with our customers across multiple hierarchy levels to build champion users and supporters of our solutions.\n\nQualifications\n• A bachelor's degree and 6 years of professional work experience (or a master's degree and 3 years of professional work experience, or a PhD degree, or equivalent experience) is required.\n\nAdditional Qualifications\n• Strong knowledge of mathematical modelling, statistics, optimization, machine learning, and deep learning algorithms & frameworks\n• Experience programming in MATLAB\n• Experience programming in Python and C/C++ is a plus\n• Working knowledge of cloud (AWS / Azure) is a plus\n• Working knowledge of CI/CD workflows is a plus\n• Exposure to Medical and healthcare applications is a plus\n• Excellent verbal and written communication and presentation skills\n• Highly motivated toward working directly with customers.\n• This position is based in New Delhi with travel generally throughout India specific to various customer visits and seminars. Travel time can be expected to amount to approximately 25-40% with trips generally no longer than five days. Though far less frequent, some international travel is expected.\n• Willingness to learn new tools and skills as required",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=lUSHTrfCo_VquIa6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWMsQrCMBRFce0nOL3BQUVbEVx0KihSQZcOjiWNr8nT-F5IAvbT_DzbweWe4XBu9p1kdY1MEqD03pFWiYThxIYYMUA3ipCoI03KQcUJnSODrBHmZbWANVykhYgqaAtDeRYxDqcHm5KP-6KI0eUmpuFW51rehTC20hdPaeM4TbQqoHcqYbPdbfrcs1nOrirZu4RXBGK44QeO6Cyt_qj4QeoHPSLVD7sAAAA&shmds=v1_AdeF8Kj9VxHI4jOIUQMZn1bZZhFKP9HNe6unw0mM_MEcLoasnA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=lUSHTrfCo_VquIa6AAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "AI Engineer for software development",
         "EcoSmart Energy System LLC",
         "Karnataka, India",
         "Develops cloud / on prem applications and systems that utilize AI tools , cloud AI services. The candidate will be able to apply Gen AI models as part of the solution. Could also include deep learning , neural networks and chatbots , image processing. Mu\n\nAs an AI / ML engineer , you will develop applications and systems utilizing AI tools , cloud AI services and Gen AI models. Your role involves creating cloud or on prem application pipelines with production ready quality, incorporating deep learning, neural networks , chatbots, and image processing.\n\nExperience – Minimum 5 years post qualification experience in a similar role.\n\nQualification – Engineering degree in computer science or equivalent.\n\nJob Types: Full-time, Permanent\n\nPay: ₹800,000.00 - ₹1,000,000.00 per year",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=TdqlU35Qrt4MqZQAAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNMQ6CUAyA4bhyBKfORsGYuMhkDDEoGwcgBcoDfbTktVG8kqcUl3_8_ui7itJzDhm7gYkCdBJApbM3BoKWXuRlGokNdnCTGpQwND0Iw1XEeVqnvdmkpyRR9bFTQxuauJExEaZa5uQhtf5Tab-Ik0ej6nDcz_HEbhNnjZQjBlv-FNwHyo8ajVAUFxgY7hgYDZ-4hZzbAX-qs52ErQAAAA&shmds=v1_AdeF8Kjv2AX1CL20Z8T1f8eLB-FGavuJuST1yq4vZ26TPsMPDg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=TdqlU35Qrt4MqZQAAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "AI Application Engineer",
         "Avaya",
         "Pune, Maharashtra, India",
         "About Avaya\n\nAvaya is an enterprise software leader that helps the world’s largest organizations and government agencies forge unbreakable connections.\n\nThe Avaya Infinity™ platform unifies fragmented customer experiences, connecting the channels, insights, technologies, and workflows that together create enduring customer and employee relationships.\n\nWe believe success is built through strong connections – with each other, with our work, and with our mission. At Avaya, you'll find a community that values your contributions and supports your growth every step of the way.\n\nLearn more at https://www.avaya.com.\n\nJob Information\n\nJob Description\n\nWe are seeking a highly experienced and visionary Lead AI Application Engineer to join our core AI team. This pivotal role will be instrumental in designing, developing, and deploying advanced Generative AI and Agentic AI applications. You will be a key contributor to our strategic initiatives, working on complex problems that require deep technical expertise and a forward-thinking approach. This is an exceptional opportunity for an individual who thrives on building sophisticated, autonomous AI systems and wants to make a significant impact.\n\nResponsibilities\n\n● Provide thought leadership around applying AI and ML technology and help define technical direction and architecture with engineering team members.\n\n● Leading the exploration and application of Large Language Models, Agentic AI and Generative AI, venturing into new areas within these fields.\n\n● Architect and implement robust, scalable, and efficient AI systems leveraging a variety of AI/ML, data science, deep learning, and agentic AI frameworks.\n\n● Drive the integration of complex Model Context Protocol (MCP) and agent-to-agent (A2A) communication protocols, ensuring seamless interaction and optimal performance.\n\n● Design, build, and deploy autonomous agents capable of complex decision-making and task execution in dynamic environments.\n\n● Design and implement scalable and efficient AI/ML systems, architectures, and pipelines that can handle large volumes of data and deliver accurate and timely results.\n\n● Develop and maintain real-world NLP features using the latest techniques for call routing, intent identification, named entity recognition, information retrieval, summarization etc.\n\n● Conduct extensive research and experimentation with emerging AI technologies, specifically in Generative AI and Agentic AI, to identify and implement innovative solutions.\n\n● Collaborate closely with cross-functional teams including data scientists, software engineers and product managers to translate business requirements into technical solutions.\n\n● Regularly self-educate while mentoring and provide technical leadership to other engineers, fostering a culture of continuous learning and excellence.\n\nRequirements\n\nQualifications\n\n● 10+ years of progressive hands-on technical experience in Artificial Intelligence, Machine Learning, Data Science, and Deep Learning.\n\n● Demonstrated experience (3+ years) actively working on and deploying Generative AI and Agentic AI applications.\n\n● Extensive hands-on experience with multiple AI and Agentic AI frameworks, such as (but not limited to):\n\n○ Generative AI: TensorFlow, PyTorch, Hugging Face Transformers, OpenAI API, LangChain, LlamaIndex, etc.\n\n○ Agentic AI: LangChain Agents, Auto-GPT, CrewAI, multi-agent simulation frameworks, etc.\n\n● Experienced with leading LLM models such as OpenAI's GPT series, Google's Gemini, Meta's Llama, and Anthropic's Claude, including fine-tuning and deployment for specific applications.\n\n● Deep understanding and practical experience with Multi-Component Platforms (MCP) and Agent-to-Agent (A2A) communication patterns.\n\n● Experience with cloud platforms (Azure, GCP, AWS) and MLOps practices.\n\n● 5 + years of experience writing and deploying production quality models in languages such as Python\n\n● Master's or Ph.D. in Computer Science, Artificial Intelligence, Machine Learning, or a related quantitative field.\n\nKnowledge, Skills, And Abilities\n\n● Expertise in developing on a Azure cloud platform is a big plus\n\n● Contributions to open-source AI projects, particularly in Generative AI or Agentic AI\n\n● Familiarity with reinforcement learning and multi-agent reinforcement learning.\n\n● Outstanding written and verbal communication skills\n\nEducation\n\nBachelor degree or equivalent experience\nAdvance Degree preferred\n\nFooter\n\nAvaya is an Equal Opportunity employer and a U.S. Federal Contractor. Our commitment to equality is a core value of Avaya. All qualified applicants and employees receive equal treatment without consideration for race, religion, sex, age, sexual orientation, gender identity, national origin, disability, status as a protected veteran or any other protected characteristic. In general, positions at Avaya require the ability to communicate and use office technology effectively. Physical requirements may vary by assigned work location. This job brief/description is subject to change. Nothing in this job description restricts Avaya right to alter the duties and responsibilities of this position at any time for any reason. You may also review the Avaya Global Privacy Policy (accessible at https://www.avaya.com/en/privacy/policy/) and applicable Privacy Statement relevant to this job posting (accessible at https://www.avaya.com/en/documents/info-applicants.pdf).",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=u5m3655fcuIxzgcnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWLQQrCMBAA8doneFqvUhMRvOipB5EKgj8o27gkkbgbslHqL3yy9TKXmWm-i8Z0PXQ5p-iwRmE4sY9MVGADFxlBCYsLMIuziE-0PIZasx6sVU3Ga50vZ5w8rTCNMtmHjPrHoAEL5YSVht1-O5nMfr3q3vhBiAy3F1MLV5wj1FALttDzPeIPThguDZMAAAA&shmds=v1_AdeF8KiZGolIScAKLYhEZ62BinbpGuk1TEcwUrX1P70a1mJ8PA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=u5m3655fcuIxzgcnAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "AI Architect",
         "Adobe",
         "Bengaluru, Karnataka, India",
         "JOB LEVEL\nP50\n\nEMPLOYEE ROLE\nIndividual Contributor\n\nExperience: 12-16 years\n\nThe candidate should have proven expertise in building scalable platforms that are customer facing and have expertise in evangelizing the platform with customers and with internal stakeholders Expert level knowledge of Cloud Computing including aspects of VPC Network Design, Shared Responsibility Matrix, Cloud databases, No SQL Databases, Data Pipelines on the cloud, VM and VM orchestration, Serverless frameworks. This should be across all 3 major cloud providers (AWS, Azure, GCP), preferably at least in 2 of the 3 Public Clouds Expert level knowledge in Data Ingestion paradigms & use of different types of databases like OLTP, OLAP for specific purposes Hands-on experience with Apache Spark, Apache Flink, Kafka, Kinesis, Pub/Sub, Databricks, Apache Airflow, Apache Iceberg, and Presto. Expertise in designing ML Pipelines for experiment management, model management, feature management, model retraining, A/B testing of models and design of APIs for model inferencing at scale. Proven expertise with Kube Flow, SageMaker/Vertex AI/Azure AI. SME in LLM Serving paradigms, deep knowledge of GPU architectures, distributed training and serving of large language models. Expertise in Model and Data parallel training, expertise with frameworks like DeepSpeed and service frameworks like vLLM etc. Proven expertise in Model finetuning and model optimization techniques to achieve better latencies, better accuracies in results. Be an expert in reducing training and resource requirements of finetuning of LLM and LVM models. Have a wide knowledge of different LLM models and have an opinion on aspects of applicability of each model based the usecases. Proven expertise of having worked on specific customer usecases and having seen delivery of a solution end to end from engineering to production. Proven expertise in DevOps and LLMOps, knowledge of Kubernetes, Docker and container orchestration, and deep knowledge of LLM Orchestration frameworks like Flowise, Langflow, Langgraph.\n\nSkill Matrix\n\nLLM: Hugging Face OSS LLMs, GPT, Gemini, Claude, Mixtral, Llama\n\nLLM Ops: ML Flow, Langchain, Langraph, LangFlow, Flowise, LLamaIndex, SageMaker, AWS Bedrock, Vertex AI, Azure AI\n\nDev Ops: Kubernetes, Docker, FluentD, Kibana, Grafana, Prometheus\n\nDatabases/Datawarehouse: DynamoDB, Cosmos, MongoDB, RDS, MySQL, PostGreSQL, Aurora, Spanner, Google BigQuery.\n\nCloud Expertise: AWS/Azure/GCP\n\nCloud Certifications: AWS Professional Solution Architect, AWS Machine Learning Specialty, Azure Solutions Architect Expert\n\nProficient in Python, SQL, Javascript\n\nInternal Opportunities\n\nCreativity, curiosity, and constant learning are celebrated aspects of your career growth journey. We’re glad that you’re pursuing a new opportunity at Adobe!\n\nPut your best foot forward:\n\n1. Update your Resume/CV and Workday profile – don’t forget to include your uniquely ‘Adobe’ experiences and volunteer work.\n\n2. Visit the Internal Mobility page on Inside Adobe to learn more about the process and set up a job alert for roles you’re interested in.\n\n3. Check out these tips to help you prep for interviews.\n\n4. If you are applying for a role outside of your current country, ensure you review the International Resources for Relocating Employees on Inside Adobe, including the impacts to your Benefits, AIP, Equity & Payroll.\n\nOnce you apply for a role via Workday, the Talent Team will reach out to you within 2 weeks. If you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth.\n\nAt Adobe, you will be immersed in an exceptional work environment that is recognized around the world. You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely. If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.\n\nAdobe is proud to be an Equal Employment Opportunity employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.\n\nAdobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=0ETwn0QL83xo_61WAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIMQ7CMAwAQLH2CUyWYEKlQUgsMJUFFR5ROamVBoJdxa7UJ_BsYLnhqs-q2rQdtCWMySgY7OEuHpTwNyAMN5GYaX0ZzSY9O6eam6iGlkIT5O2EycvinuL1T68jFpoyGvXH02FpJo67bTuIJ0gMV-KIeS5zDQ8sjIYvrKHjIeEXtmlRP4sAAAA&shmds=v1_AdeF8KhyrvoqNhvJ0pdjtjlp2_BOOAjQdNQdK4v6U8P1yoScUQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=0ETwn0QL83xo_61WAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "Sr Specialist Data/AI Engineering",
         "AT&T",
         "Bengaluru, Karnataka, India",
         "Job Description:\n\nJob Title: Data/AI Engineer – GenAI & Agentic AI Integration (Azure)\nLocation: Bangalore, India\nJob Type: Full-Time\nAbout the Role\nWe are seeking a highly skilled Data/AI Engineer to join our dynamic team, specializing in integrating cutting-edge Generative AI (GenAI) and Agentic AI solutions within the Azure cloud environment. The ideal candidate will have a strong background in Python, data engineering, and AI model integration, with hands-on experience working on Databricks, Snowflake, Azure Storage, and Palantir platforms. You will play a crucial role in designing, developing, and deploying scalable data and AI pipelines that power next-generation intelligent applications.\nKey Responsibilities\nDesign, develop, and maintain robust data pipelines and AI integration solutions using Python on Azure Databricks.\nIntegrate Generative AI and Agentic AI models into existing and new workflows to drive business innovation and automation.\nCollaborate with data scientists, AI researchers, software engineers, and product teams to deliver scalable and efficient AI-powered solutions.\nOrchestrate data movement and transformation across Azure-native services including Azure Databricks, Azure Storage (Blob, Data Lake), and Snowflake, ensuring data quality, security, and compliance.\nIntegrate enterprise data using Palantir Foundry and leverage Azure services for end-to-end solutions.\nDevelop and implement APIs and services to facilitate seamless AI model deployment and integration.\nOptimize data workflows for performance and scalability within Azure.\nMonitor, troubleshoot, and resolve issues related to data and AI pipeline performance.\nDocument architecture, designs, and processes for knowledge sharing and operational excellence.\nStay current with advances in GenAI, Agentic AI, Azure data engineering best practices, and cloud technologies.\nRequired Qualifications\nBachelor’s or Master’s degree in Computer Science, Engineering, Data Science, or a related field (or equivalent practical experience).\n5+ years of professional experience in data engineering or AI engineering roles.\nStrong proficiency in Python for data processing, automation, and AI model integration.\nHands-on experience with Azure Databricks and Spark for large-scale data engineering.\nProficiency in working with Snowflake for cloud data warehousing.\nIn-depth experience with Azure Storage solutions (Blob, Data Lake) for data ingestion and management.\nFamiliarity with Palantir Foundry or similar enterprise data integration platforms.\nDemonstrated experience integrating and deploying GenAI or Agentic AI models in production environments.\nKnowledge of API development and integration for AI and data services.\nStrong problem-solving skills and ability to work in a fast-paced, collaborative environment.\nExcellent communication and documentation skills.\nPreferred Qualifications\nExperience with Azure Machine Learning, Azure Synapse Analytics, and other Azure AI/data services.\nExperience with MLOps, model monitoring, and automated deployment pipelines in Azure.\nExposure to data governance, privacy, and security best practices.\nExperience with visualization tools and dashboard development.\nKnowledge of advanced AI model architectures, including LLMs and agent-based systems.\n\n#DataEngineer\n\nWeekly Hours:\n40\n\nTime Type:\nRegular\n\nLocation:\nBangalore, Karnataka, India\n\nIt is the policy of AT&T to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, AT&T will provide reasonable accommodations for qualified individuals with disabilities. AT&T is a fair chance employer and does not initiate a background check until an offer is made.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=VBue8OZ4hjMI9hO6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU4HgoPURgRBdKooUh3bvVzrkUbjXcil0I_xY9XlrS_7zLJ9HaEO1Dv0ThOcMaEpK7iwdUwUHVtYw006UMLYDyAMVxHraX4cUgp6MEbVF1YTJtcXvbyNMHUymad0-qfVASMFj4na7W4zFYHtalE2ywYcw4nYoh_jmMMdI__6F-ZQ8cPhFyuv7eufAAAA&shmds=v1_AdeF8KiVbtMP89pgbN5skTi7M9LBmafXkZhUjBMaB6ltsKhUhA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=VBue8OZ4hjMI9hO6AAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "Senior AI Engineer I",
         "MicroStrategy",
         "Mumbai, Maharashtra, India",
         "Company Description\n\nStrategy (Nasdaq: MSTR) is at the forefront of transforming organizations into intelligent enterprises through data-driven innovation. We don't just follow trends—we set them and drive change. As a market leader in enterprise analytics and mobility software, we've pioneered BI and analytics space, empowering people to make better decisions and revolutionizing how businesses operate.\n\nBut that's not all. Strategy is also leading to a groundbreaking shift in how companies approach their treasury reserve strategy, boldly adopting Bitcoin as a key asset. This visionary move is reshaping the financial landscape and solidifying our position as a forward-thinking, innovative force in the market. Four years after adopting the Bitcoin Standard, Strategy's stock has outperformed every company in S&P 500.\n\nOur people are the core of our success. At Strategy, you'll join a team of smart, creative minds working on dynamic projects with cutting-edge technologies. We thrive on curiosity, innovation, and a relentless pursuit of excellence.\n\nOur corporate values—bold, agile, engaged, impactful, and united—are the foundation of our culture. As we lead the charge into the new era of AI and financial innovation, we foster an environment where every employee's contributions are recognized and valued.\n\nJoin us and be part of an organization that lives and breathes innovation every day. At Strategy, you're not just another employee; you're a crucial part of a mission to push the boundaries of analytics and redefine financial investment.\n\nJob Description\n\nJob Location: Pune. Full time 5 days a week from Strategy Pune office.\n\nSenior AI Engineer I\n\nWe are looking for a highly skilled AI Engineer to join our team and contribute to the development of AI-powered solutions. The ideal candidate will have expertise in Python programming, REST API development, cloud infrastructure, and AI model integration. This role requires strong problem-solving skills, a deep understanding of AI technologies, and the ability to optimize and deploy AI applications efficiently.\n\nKey Responsibilities:\n• Develop, maintain, and optimize AI-driven applications using Python.\n• Implement object-oriented programming, module creation, and unit testing to ensure code quality and maintainability.\n• Design and build RESTful APIs with FastAPI for seamless integration of AI services.\n• Work with various Large Language Models (LLMs), including OpenAI and Azure OpenAI, to enhance AI-driven functionalities.\n• Interface with relational and vector databases for efficient AI data retrieval and storage.\n• Implement text embedding techniques to improve AI-powered natural language processing tasks.\n• Deploy and manage AI applications on cloud platforms such as AWS, Azure, and GCP.\n• Utilize Docker for containerization, ensuring scalable and efficient deployment.\n• Collaborate with cross-functional teams using Git for version control and code management.\n\nRequirements:\n• Bachelor’s/Master’s degree in Computer Science or equivalent.\n• A minimum of 4 years of hands-on experience in Python software development with a focus on modular, scalable, and efficient coding for AI/LLM-based applications.\n• Experience developing and maintaining RESTful APIs using FastAPI and interfacing with relational databases.\n• Experience in implementing LLMs (OpenAI, Azure) using vector databases, text embedding, and RAG.\n• Hands-on experience with cloud platforms (AWS, Azure, GCP) and docker for AI deployment and scaling.\n• Strong Git skills for version control and collaborative development.\n• Ability to adjust and thrive in a fast-paced environment with changing priorities.\n• Strong analytical and problem-solving skills.\n• Familiarity with data visualization tools such as MicroStrategy preferred.\n\nAdditional Information\n\nThe recruitment process includes online assessments as a first step. We send them via e-mail, please check also your SPAM folder.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=HchZv1aDiP5RZdWOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWKMQ7CMAwAxdonMHmmJUFILDAxIFSkTn1A5QQrCWrtKg5SeQnfpSw33F313VR1T5wkw7WFG4fERBla2MNDHChh9hGE4S4SRtpeYimznq1VHU3QgiV542WywuRksS9x-segETPNIxYajqfDYmYOu7pLPktf8mrDBxJD954cpgY6XHfUuKYGWn4m_AHJnWG9mgAAAA&shmds=v1_AdeF8Kg7hAEdri5RQpm-kTSnlODzqo61RM6-_hvQb-2-gVRN4w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=HchZv1aDiP5RZdWOAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "Snr AI Security Engineer (Detection)",
         "Zoom",
         "India",
         "What you can expect\n\nZoom are seeking to hire an experienced, hands on AI Security Engineer (Detection)with proven experience in AI and LLM Security, to join the Threat Detection & Analytics Engineering team, which is a part of the Detection and Response (D&R) org in Information Security.\n\nAbout the Team\n\nThe AI security engineer will play a crucial role in ensuring the security of Zoom’s AI products. This team identify, assess, and detect application-level threats and vulnerabilities.\n\nResponsibilities:\n• Collaborate with AI, engineering and DevOps teams to model AI threats and assess risks.\n• Design and develop threat detection solutions tailored to Zoom’s AI applications and services.\n• Proactively identify AI application log sources with detection values and facilitate onboarding and tuning in order to have better efficacy.\n• Build and maintain automations to streamline detection processes.\n\nWhat we are looking for:\n\nHave a B.S. in Computer Science, Information Security, or related field.\n\n5 - 10 years experience as a Security Engineer with a focus on AI Security, especially LLM security.\n• Solid understanding of application security concepts, including OWASP Top Ten for LLM applications.\n• Deep knowledge of security architecture of applications and services deployed to on-premise and cloud environments.\n• Experience with building security defenses against attacks such as prompt injection, data poisoning and leakage for AI/ML systems.\n• Proficiency in programming (Python, Java, Scala, etc.).\n• Proficiency in statistical analysis and machine learning methodologies.\n• Knowledge of MITRE ATLAS framework preferrable.\n• Be experienced with cloud environments and cloud security principles. Also have the technical know-how and understanding of APIs, vulnerability management (CVE, CVSS, OWASP Top 10), container, network, and systems security.\n• Be experienced in collaborating and working effectively with global and remote teams.\n\n#India\n\n#Remote\n\n#RemoteIndia\n\nWays of Working\nOur structured hybrid approach is centered around our offices and remote work environments. The work style of each role, Hybrid, Remote, or In-Person is indicated in the job description/posting.\n\nBenefits\nAs part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways. Click Learn for more information.\n\nAbout Us\nZoomies help people stay connected so they can get more done together. We set out to build the best collaboration platform for the enterprise, and today help people communicate better with products like Zoom Contact Center, Zoom Phone, Zoom Events, Zoom Apps, Zoom Rooms, and Zoom Webinars.\nWe’re problem-solvers, working at a fast pace to design solutions with our customers and users in mind. Find room to grow with opportunities to stretch your skills and advance your career in a collaborative, growth-focused environment.\n\nOur Commitment​\n\nAt Zoom, we believe great work happens when people feel supported and empowered. We’re committed to fair hiring practices that ensure every candidate is evaluated based on skills, experience, and potential. If you require an accommodation during the hiring process, let us know—we’re here to support you at every step.\n\nIf you need assistance navigating the interview process due to a medical disability, please submit an Accommodations Request Form and someone from our team will reach out soon. This form is solely for applicants who require an accommodation due to a qualifying medical disability. Non-accommodation-related requests, such as application follow-ups or technical issues, will not be addressed.\n\n#LI-Remote",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=aWSfnf_ubnOAGDwvAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFwL_oDTjSqYiOBiJ0GRunZzKWk8kkh6F3In1E_wr8U3vOa7aNqeKpw76NG_a9IPXCkkQqywvqCi18S0gR3ceQRBV30EJrgxh4yrNqoWOVkrkk0QdZq88TxZJhx5ti8e5d8g0VUs2SkOh-N-NoXCdvlgniARdPRM7gc43eVojAAAAA&shmds=v1_AdeF8KhsqS5HiIfYqXXjqnMGVgJBnreqyc2xm6dUXOwh-SVzFQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=aWSfnf_ubnOAGDwvAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "Senior AI Engineer (5+ Years Exp) – Immediate Joiners Only",
         "Perimattic",
         "Mumbai, Maharashtra, India",
         "Experience: 5+ years\n\nLocation: Mumbai, Maharashtra\n\nJoining: Immediate only\n\nAbout the Role\n\nWe are hiring an experienced Senior AI Engineer who has successfully built and scaled AI systems in production. This role requires deep expertise in LLMs, ML pipelines, model optimization, and real-time deployment. You should also be comfortable leading junior developers and contributing to architectural decisions.\n\nKey Responsibilities\n• Lead design, development, and deployment of advanced AI systems\n• Fine-tune large language models and optimize training pipelines\n• Own the end-to-end lifecycle of AI projects (data to deployment)\n• Evaluate third-party models and tools, and recommend best-fit solutions\n• Collaborate with cross-functional teams to align AI efforts with product goals\n\nMust-Have Skills\n• 5+ years of hands-on experience in AI/ML roles\n• Strong understanding of NLP, deep learning, and large-scale model architectures\n• Proficiency in Python, PyTorch/TensorFlow, and data engineering tools\n• Experience with LLMs, vector databases, and MLOps tools\n• Ability to design and optimize ML pipelines for performance and scalability\n\nPreferred Skills\n• Experience managing AI teams or mentoring juniors\n• Working knowledge of Retrieval-Augmented Generation (RAG), embeddings\n• Familiarity with CI/CD, Docker, and cloud deployment pipelines",
         "https://www.google.com/search?ibp=htl;jobs&q=AI+Engineer&htidocid=OR6W3HincLy0sLuwAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOvQrCQBCEsc0jWG0Z_xIR0mhlIRIhKKSyCpu4JCe53XB7Qux8B1_Hp_FJPJuBYb4ZJvpMorIkNuJgn8OBW8NEDuJsAVdCp3AYhxl8X2_IraWbQU9wkgCF6Mz9E1bB1qCBbToQhqNI29N013k_6DZNVfukVY_eNEkjNhWmWsb0LrX-pdIOHQ192K022XpMBm7n8YWcsehDBwxD8bA1miUUGFjUzjtcQs7hzA-lfAaKvwAAAA&shmds=v1_AdeF8KjJuZWUSEQ-suqfddJlmbJASMO65LeEZJXEeTp06MDW-g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI+Engineer&htidocid=OR6W3HincLy0sLuwAAAAAA%3D%3D",
         "AI Engineer"
        ],
        [
         "AI/ML Engineer",
         "HPE",
         "Hyderabad, Telangana, India",
         "AI/ML Engineer\n\nThis role has been designed as ‘’Onsite’ with an expectation that you will primarily work from an HPE office.\n\nWho We Are:\n\nHewlett Packard Enterprise is the global edge-to-cloud company advancing the way people live and work. We help companies connect, protect, analyze, and act on their data and applications wherever they live, from edge to cloud, so they can turn insights into outcomes at the speed required to thrive in today’s complex world. Our culture thrives on finding new and better ways to accelerate what’s next. We know varied backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good. If you are looking to stretch and grow your career our culture will embrace you. Open up opportunities with HPE.\n\nJob Description:\n\nJob Family Definition:\n\nThe Cloud Developer builds from the ground up to meet the needs of mission-critical applications, and is always looking for innovative approaches to deliver end-to-end technical solutions to solve customer problems.\n\nBrings technical thinking to break down complex data and to engineer new ideas and methods for solving, prototyping, designing, and implementing cloud-based solutions.\nCollaborates with project managers and development partners to ensure effective and efficient delivery, deployment, operation, monitoring, and support of Cloud engagements.\n\nThe Cloud Developer provides business value expertise to drive the development of innovative service offerings that enrich HPE's Cloud Services portfolio across multiple systems, platforms, and applications.\n\nManagement Level Definition:\n\nContributions impact technical components of HPE products, solutions, or services regularly and sustainable. Applies advanced subject matter knowledge to solve complex business issues and is regarded as a subject matter expert. Provides expertise and partnership to functional and technical project teams and may participate in cross-functional initiatives. Exercises significant independent judgment to determine best method for achieving objectives. May provide team leadership and mentoring to others.\n\nWhat you will do:\n• Experiment, design, develop and maintain machine learning models and pipelines with a high potential for value and scale.\n• Collaborate with other ML engineers, data scientists, product managers, and other engineers to ensure successful implementation of machine learning solutions.\n• Perform research and testing to develop or customize machine learning algorithms; conduct model training and evaluation as needed; integrate, test, tune and monitor the solutions developed.\n• Research and evaluate new technologies and tools for machine learning.\n• Maintain and update existing machine learning systems.\n• Hands-on develop, productionize, and operate Machine Learning models and pipelines at scale, including both batch and real-time use cases.\n• Work with large scale structured and unstructured data, build and continuously improve cutting edge Machine Learning models.\n• Leads the project team for design and development of complex products and platforms, including solution design, analysis, coding, testing, and integration for building efficient, scalable and robust cloud subsystems.\n• Reviews and evaluates designs, test plans, and develops code for compliance with cloud design and development guidelines and standards.\n• Provides tangible feedback to improve product quality and mitigate risks.\n• Represents the engineering team in various technical forums and provides guidance and mentoring to less-experienced team members.\n• Drives innovation and integration of new technologies into projects and activities in the software systems design organization.\n• Analyzes science, engineering, business, and other data processing problems to develop and implement solutions to complex application problems, system administration issues, or network concerns.\n\nWhat you will need:\n• Bachelor's or master's degree in computer science, engineering, information systems, or closely related quantitative discipline.\n• Typically, 7-10 years’ experience.\n\nKnowledge and Skills:\n• Strong programming skills in Python & preferrable familiar with Golang\n• Understanding microservice architecture and how they can be built in a containerized, Kubernetes-managed environment is central to all modern cloud-native applications.\n• Designing and integrating software systems running on multiple platform types into the overall architecture.\n• Evaluating forms and processes for software systems testing and methodology, including writing and executing test plans, debugging, and testing scripts and tools.\n• Excellent written and verbal communication skills. Ability to effectively communicate product architectures and design proposals at senior management levels.\n\nAdditional Skills:\n\nCloud Architectures, Cross Domain Knowledge, Design Thinking, Development Fundamentals, DevOps, Distributed Computing, Microservices Fluency, Full Stack Development, Release Management, Security-First Mindset, User Experience (UX)\n\nWhat We Can Offer You:\n\nHealth & Wellbeing\n\nWe strive to provide our team members and their loved ones with a comprehensive suite of benefits that supports their physical, financial and emotional wellbeing.\n\nPersonal & Professional Development\n\nWe also invest in your career because the better you are, the better we all are. We have specific programs catered to helping you reach any career goals you have — whether you want to become a knowledge expert in your field or apply your skills to another division.\n\nUnconditional Inclusion\n\nWe are unconditionally inclusive in the way we work and celebrate individual uniqueness. We know varied backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good.\n\nLet's Stay Connected:\n\nFollow @HPECareers on Instagram to see the latest on people, culture and tech at HPE.\n\n#india\n\n#hybridcloud\n\nJob:\nEngineering\n\nJob Level:\nTCP_04\n\nHPE is an Equal Employment Opportunity/ Veterans/Disabled/LGBT employer. We do not discriminate on the basis of race, gender, or any other protected category, and all decisions we make are made on the basis of qualifications, merit, and business need. Our goal is to be one global team that is representative of our customers, in an inclusive environment where we can continue to innovate and grow together. Please click here: Equal Employment Opportunity.\n\nHewlett Packard Enterprise is EEO Protected Veteran/ Individual with Disabilities.\n\nHPE will comply with all applicable laws related to employer use of arrest and conviction records, including laws requiring employers to consider for employment qualified applicants with criminal histories.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=K2w_9m4znMEZnhJ5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU6H4CI1EcFFJ4diKwoO7uWSHmkk3pVchvoHfrb4hld9F9X63Nn7DRoOkYkybOEqDpQw-xGE4SISEi1PYymTHq1VTSZowRK98fK2wuRkti9x-q_XETNNCQv1-8NuNhOHzap9NBAZ2s9AGR0ONTwpIQdkrKHjIeIP5hSTu4sAAAA&shmds=v1_AdeF8KgzhY__i-5O2lHxWD-q9EllLuxaHTxtOzj1nFa4uBGxNQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=K2w_9m4znMEZnhJ5AAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "AI/ML Engineer – XR Software (All levels))",
         "Qualcomm",
         "Bengaluru, Karnataka, India",
         "Company:\nQualcomm India Private Limited\n\nJob Area:\nEngineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nWe are seeking a passionate and skilled AI/ML Engineer to join our cutting-edge Extended Reality (XR) Software team. In this role, you will work on next-generation XR products that blend the physical and digital worlds, leveraging artificial intelligence and machine learning to create immersive, intelligent, and responsive experiences.\n\nYou will collaborate with cross-functional teams of researchers, engineers, and designers to build real-time AI/ML software optimized for XR platforms. A strong background in C++ or embedded firmware development is essential, as you will be working close to hardware and performance-critical systems.\n\nKey Responsibilities\n• Design, develop, and optimize AI/ML models for XR applications such as computer vision, sensor fusion, gesture recognition, and spatial understanding.\n• Implement real-time inference pipelines on embedded or edge devices.\n• Collaborate with firmware and hardware teams to integrate ML models into XR systems.\n• Analyze system performance and optimize for latency, power, and memory.\n• Stay up to date with the latest research and trends in AI/ML and XR technologies.\n• Contribute to the full lifecycle of product development—from prototyping to production.\n\nRequired Qualifications\n• Bachelor’s or Master’s degree in Computer Science, Electrical Engineering, or a related field.\n• 1–10 years of industry experience in AI/ML engineering or embedded systems.\n• Proficiency in C++ and/or embedded firmware development.\n• Solid understanding of machine learning fundamentals and experience with frameworks like TensorFlow, PyTorch, or ONNX.\n• Experience with deploying ML models on edge devices\n• Familiarity with XR technologies (AR/VR/MR), sensor data processing, or 3D spatial computing.\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Engineering or related work experience.\n\nApplicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\nIf you would like more information about this role, please contact Qualcomm Careers.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuw4BURCA4Wj3EVTTuYQ9ItFQISLrUqDRyewaZ5cxsznnLErv4DG8lSdB81df_uhdi6bjxKxXMBNbCJGDz_MF-y3s9BTu6AiaY2ZguhH7Vgu6sNAUPKHLclCBuaplqo_yEEo_NMZ7jq0PGIoszvRqVCjVhzlr6v85-Pz3LBkDHfqD3iMuxbYbmwr5h69QCExILHLlqg4s0QkGvGAHEjkW-AWsJyrJrgAAAA&shmds=v1_AdeF8Kjkwu3emhikRMcwxsbJzM0PcFN4Cmp6p0XCuEOoHNe2gw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=A_grkjk9xtOJ_mNiAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "ML Engineer II - Applied AI",
         "Uber",
         "Bengaluru, Karnataka, India",
         "ABOUT THE ROLE\n\nApplied AI is a horizontal AI team at Uber collaborating with business units across the company to deliver cutting-edge AI solutions for core business problems. We work closely with engineering, product and data science teams to understand key business problems and the potential for AI solutions, then deliver those AI solutions end-to-end. Key areas of expertise include Generative AI, Computer Vision, and Personalization.\n\nWe are looking for a strong Senior ML engineer to be a part of a high-impact team at the intersection of classical machine learning, generative AI, and ML infrastructure. In this role, you’ll be responsible for delivering Uber’s next wave of intelligent experiences by building ML solutions that power core user and business-facing products.\n\nWHAT YOU’LL DO\n• Solve business-critical problems using a mix of classical ML, deep learning, and generative AI.\n• Collaborate with product, science, and engineering teams to execute on the technical vision and roadmap for Applied AI initiatives.\n• Deliver high-quality, production-ready ML systems and infrastructure, from experimentation through deployment and monitoring.\n• Adopt best practices in ML development lifecycle (e.g., data versioning, model training, evaluation, monitoring, responsible AI).\n• Deliver enduring value in the form of software and model artifacts.\n\nBASIC QUALIFICATIONS\n• Master or PhD or equivalent experience in Computer Science, Engineering, Mathematics or a related field and 2 years of Software Engineering work experience, or 5 years Software Engineering work experience.\n• Experience in programming with a language such as Python, C, C++, Java, or Go.\n• Experience with ML packages such as Tensorflow, PyTorch, JAX, and Scikit-Learn.\n• Experience with SQL and database systems such as Hive, Kafka, and Cassandra.\n• Experience in the development, training, productionization and monitoring of ML solutions at scale.\n\nPREFERRED QUALIFICATIONS\n• Prior experience working with generative AI (e.g., LLMs, diffusion models) and integrating such technologies into end-user products.\n• Experience in modern deep learning architectures and probabilistic models.\n• Machine Learning, Computer Science, Statistics, or a related field with research or applied focus on large-scale ML systems.\n\nWe welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.\n\nOffices continue to be central to collaboration and Uber’s cultural identity. Unless formally approved to work fully remotely, Uber expects employees to spend at least half of their work time in their assigned office. For certain roles, such as those based at green-light hubs, employees are expected to be in-office for 100% of their time. Please speak with your recruiter to better understand in-office expectations for this role.\n• Accommodations may be available based on religious and/or medical conditions, or as required by applicable law. To request an accommodation, please reach out to accommodations@uber.com.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=75zem_tk1v_fxmEhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNvwrCMBCAcVz7CE4HblIbKbjoVEGk_hmdy6U90mi8C7kUfBKfV7t88Ju-4rso6vsNTuw8EyVoW9hAE2PwNEAz4yIWlDD1IwjDWcQFWh7GnKPujVENldOM2fdVL28jTFY-5ilW53Q6YqIYMFNX77afKrJbrx72f_IMR2KHYUpTCVdMjBlfWELLg8cfNrKN3ZkAAAA&shmds=v1_AdeF8Kip_hUrnQpLhkvbK1ixwB1cbNY3BqXJxBMbZjlUuzpZyw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=75zem_tk1v_fxmEhAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "AI/ML Engineer",
         "ASSA ABLOY",
         "Chennai, Tamil Nadu, India",
         "An Amazing Career Opportunity for AI/ML Engineer Location: Chennai, India (Hybrid) Job ID: 39582 Position Summary A rewarding career at HID Global beckons you! We are looking for an AI/ML Engineer , who is responsible for designing, developing, and deploying advanced AI/ML solutions to solve complex business challenges. This role requires expertise in machine learning, deep learning, MLOps, and AI model optimization, with a focus on building scalable, high-performance AI systems. As an AI/ML Engineer, you will work closely with data engineers, software developers, and business stakeholders to integrate AI-driven insights into real-world applications. You will be responsible for model development, system architecture, cloud deployment, and ensuring responsible AI adoption. We are a leading company in the trusted source for innovative HID Global Human Resources products, solutions and services that help millions of customers around the globe create, manage and use secure identities. Who are we? HID powers the trusted identities of the world’s people, places, and things, allowing people to transact safely, work productively and travel freely. We are a high-tech software company headquartered in Austin, TX, with over 4,000 worldwide employees. Check us out: www.hidglobal.com and https://youtu.be/23km5H4K9Eo About HID Global, Chennai HID Global powers the trusted identities of the world’s people, places and things. We make it possible for people to transact safely, work productively and travel freely. Our trusted identity solutions give people secure and convenient access to physical and digital places and connect things that can be accurately identified, verified and tracked digitally. Millions of people around the world use HID products and services to navigate their everyday lives, and over 2 billion things are connected through HID technology. We work with governments, educational institutions, hospitals, financial institutions, industrial businesses and some of the most innovative companies on the planet. Headquartered in Austin, Texas, HID Global has over 3,000 employees worldwide and operates international offices that support more than 100 countries. HID Global® is an ASSA ABLOY Group brand. For more information, visit www.hidglobal.com. HID Global has is the trusted source for secure identity solutions for millions of customers and users around the world. In India, we have two Engineering Centre (Bangalore and Chennai) over 200+ Engineering Staff. Global Engineering Team is based in Chennai and one of the Business Unit Engineering team is based in Bangalore. Physical Access Control Solutions (PACS) HID's Physical Access Control Solutions Business Area: HID PAC’s Business Unit focuses on the growth of new clients and existing clients where we leverage the latest card and reader technologies to solve the security challenges of our clients. Other areas of focus will include authentication, card sub systems, card encoding, Biometrics, location services and all other aspects of a physical access control infrastructure. Qualifications:- To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Roles & Responsibilities: Design, develop, and deploy robust & scalable AI/ML models in Production environments. Collaborate with business stakeholders to identify AI/ML opportunities and define measurable success metrics. Design and build Retrieval-Augmented Generation (RAG) pipelines integrating vector stores, semantic search, and document parsing for domain-specific knowledge retrieval. Integrate Multimodal Conversational AI platforms (MCP) including voice, vision, and text to deliver rich user interactions. Drive innovation through PoCs, benchmarking, and experiments with emerging models and architectures. Optimize models for performance, latency and scalability. Build data pipelines and workflows to support model training and evaluation. Conduct research & experimentation on the state-of-the-art techniques (DL, NLP, Time series, CV) Partner with MLOps and DevOps teams to implement best practices in model monitoring, version and re-training. Lead code reviews, architecture discussions and mentor junior & peer engineers. Architect and implement end-to-end AI/ML pipelines, ensuring scalability and efficiency. Deploy models in cloud-based (AWS, Azure, GCP) or on-premises environments using tools like Docker, Kubernetes, TensorFlow Serving, or ONNX Ensure data integrity, quality, and preprocessing best practices for AI/ML model development. Ensure compliance with AI ethics guidelines, data privacy laws (GDPR, CCPA), and corporate AI governance. Work closely with data engineers, software developers, and domain experts to integrate AI into existing systems. Conduct AI/ML training sessions for internal teams to improve AI literacy within the organization. Strong analytical and problem solving mindset. Technical Requirements: Strong expertise in AI/ML engineering and software development. Strong experience with RAG architecture, vector databases Proficiency in Python and hands-on experience in using ML frameworks (tensorflow, pytorch, scikit-learn, xgboost etc) Familiarity with MCPs like Google Dialogflow, Rasa, Amazon Lex, or custom-built agents using LLM orchestration. Cloud-based AI/ML experience (AWS Sagemaker, Azure ML, GCP Vertex AI, etc.). Solid understanding of AI/ML life cycle – Data preprocessing, feature engineering, model selection, training, validation and deployment. Experience in production grade ML systems (Model serving, APIs, Pipelines) Familiarity with Data engineering tools (SPARK, Kafka, Airflow etc) Strong knowledge of statistical modeling, NLP, CV, Recommendation systems, Anomaly detection and time series forecasting. Hands-on in Software engineering with knowledge of version control, testing & CI/CD Hands-on experience in deploying ML models in production using Docker, Kubernetes, TensorFlow Serving, ONNX, and MLflow. Experience in MLOps & CI/CD for ML pipelines, including monitoring, retraining, and model drift detection. Proficiency in scaling AI solutions in cloud environments (AWS, Azure & GCP). Experience in data preprocessing, feature engineering, and dimensionality reduction. Exposure to Data privacy, Compliance and Secure ML practices Education and/or Experience: Graduation or master’s in computer science or information technology or AI/ML/Data science 3+ years of hands-on experience in AI/ML development/deployment and optimization Experience in leading AI/ML teams and mentoring junior engineers. Why apply? Empowerment: You’ll work as part of a global team in a flexible work environment, learning and enhancing your expertise. We welcome an opportunity to meet you and learn about your unique talents, skills, and experiences. You don’t need to check all the boxes. If you have most of the skills and experience, we want you to apply. Innovation: You embrace challenges and want to drive change. We are open to ideas, including flexible work arrangements, job sharing or part-time job seekers. Integrity: You are results-orientated, reliable, and straightforward and value being treated accordingly. We want all our employees to be themselves, to feel appreciated and accepted. This opportunity may be open to flexible working arrangements. HID is an Equal Opportunity/Affirmative Action Employer – Minority/Female/Disability/Veteran/Gender Identity/Sexual Orientation. We make it easier for people to get where they want to go! On an average day, think of how many times you tap, twist, tag, push or swipe to get access, find information, connect with others or track something. HID technology is behind billions of interactions, in more than 100 countries. We help you create a verified, trusted identity that can get you where you need to go – without having to think about it. When you join our HID team, you’ll also be part of the ASSA ABLOY Group, the global leader in access solutions. You’ll have 63,000 colleagues in more than 70 different countries. We empower our people to build their career around their aspirations and our ambitions – supporting them with regular feedback, training, and development opportunities. Our colleagues think broadly about where they can make the most impact, and we encourage them to grow their role locally, regionally, or even internationally. As we welcome new people on board, it’s important to us to have diverse, inclusive teams, and we value different perspectives and experiences. #LI-HIDGlobal We make it easier for people to get where they want to go! On an average day, think of how many times you tap, twist, tag, push or swipe to get access, find information, connect with others or track something. HID technology is behind billions of interactions, in more than 100 countries. We help you create a verified, trusted identity that can get you where you need to go – without having to think about it. When you join our HID team, you’ll also be part of the ASSA ABLOY Group, the global leader in access solutions. You’ll have 63,000 colleagues in more than 70 different countries. We empower our people to build their career around their aspirations and our ambitions – supporting them with regular feedback, training, and development opportunities. Our colleagues think broadly about where they can make the most impact, and we encourage them to grow their role locally, regionally, or even internationally. As we welcome new people on board, it’s important to us to have diverse, inclusive teams, and we value different perspectives and experiences. #LI-HIDGlobal",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=bbr4TKvbz_KjqmOOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU63CCK1EcFFpygilapDXZzKpT2SSHpXmgj9Cr9ZfMPLvrNsoUt1q-DM1jPRCGu4ioFIOLYOhOEiYgPNDy6lIe6VijEUNiZMvi1a6ZUwGZnUW0z810SHIw0BEzXb3WYqBrarpa5rDfpYPV7gGU6OmNHn8MTeB7hj98mh5M7jD830oIWRAAAA&shmds=v1_AdeF8Kj2Mp6NCO33Hp2mrtyNB-o4RHb2REYzOhPXCCC1Z8-GMw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=bbr4TKvbz_KjqmOOAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "Machine Learning Engineer – ML Ops & Support",
         "Aera Technology",
         "Pune, Maharashtra, India",
         "Aera Technology is the Decision Intelligence company. Our platform, Aera Decision Cloud™, integrates with enterprise systems to digitize, augment, and automate decisions in real time. We deliver millions of AI-powered recommendations each year, generating significant value for some of the world’s largest brands.\n\nWe are seeking a Machine Learning Engineer (Support & Ops focus) to ensure our AI-powered decision systems run reliably at scale. This role is less about building models from scratch, and more about keeping production AI systems healthy, observable, and performant, while enabling Data Science teams to deliver faster.\n\nThis position is also a strong career pathway into ML feature development — you will work closely with Product, Data Science, and Engineering teams, gain exposure to LLMs, Agentic AI, and advanced ML tooling, and progressively take on more responsibilities in building new ML-powered product features.\n\nResponsibilities\n• Monitor, troubleshoot, and maintain ML pipelines and services in production, ensuring high availability and minimal downtime.\n• Work closely with Data Scientists and Engineers to operationalize ML/LLM models, from development through deployment.\n• Build and maintain observability tools for tracking data quality, model performance, drift detection, and inference metrics.\n• Support LLM and Agentic AI features in production, focusing on stability, optimization, and seamless integration into the platform.\n• Develop and enhance internal ML tooling for faster experimentation, deployment, and feature integration.\n• Collaborate with Product teams to roll out new ML-driven features and improve existing ones.\n• Work with DevOps to improve CI/CD workflows for ML code, data pipelines, and models.\n• Optimize resource usage and costs for large-scale model hosting and inference.\n• Document workflows, troubleshooting guides, and best practices for ML systems support.\n\nAbout You\n• B.E./ B.Tech in Computer Science, Engineering, or related field.3–5 years of experience in software engineering, ML Ops, or ML platform support.\n• Strong Python skills, with experience in production-grade code and automation. Experience with ML pipeline orchestration tools (Airflow, Prefect, Kubeflow, or similar).\n• Familiarity with containerized microservices (Docker, Kubernetes) and CI/CD pipelines.\n• Experience monitoring ML systems using tools like Prometheus, Grafana, ELK, Sentry, or equivalent.\n• Understanding of model packaging and serving frameworks (FastAPI, TorchServe, Triton Inference Server, Hugging Face Inference API).\n• Strong collaboration skills with cross-functional teams.\n\nGood to Have\n• Exposure to LLM operations (prompt engineering, fine-tuning, inference optimization).Familiarity with Agentic AI workflows and multi-step orchestration (LangChain, LlamaIndex).\n• Experience with data versioning (DVC, Delta Lake) and experiment tracking (MLflow, Weights & Biases).\n• Knowledge of vector databases (Pinecone, Weaviate, FAISS).\n• Experience with streaming data (Kafka) and caching (Redis).\n• Skills in cost optimization for GPU workloads.\n• Basic understanding of system design for large-scale AI infrastructure.\n\nIf you share our passion for building a sustainable, intelligent, and efficient world, you’re in the right place. Established in 2017 and headquartered in Mountain View, California, we're a series D start-up, with teams in Mountain View, San Francisco (California), Bucharest and Cluj-Napoca (Romania), Paris (France), Munich (Germany), London (UK), Pune (India), and Sydney (Australia). So join us, and let’s build this!\n\nAera Technology is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status.\n\nBenefits Summary\n\nAt Aera Technology, we strive to support our Aeranauts and their loved ones through different stages of life with a variety of attractive benefits, and great perks. In addition to offering a competitive salary and company stock options, we have other great benefits available. You’ll find comprehensive medical, Group Medical Insurance, Term Insurance, Accidental Insurance, paid time off, Maternity leave, and much more. We offer unlimited access to online professional courses for both professional and personal development, coupled with people manager development programs. We believe in a flexible working environment, to allow our Aeranauts to perform at their best, ensuring a healthy work-life balance. When you’re working from the office, you’ll also have access to a fully-stocked kitchen with a selection of snacks and beverages.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=pDU2hLyxJBfqhe6DAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPYoCQRCGYTb1CEZfZODPzLJgopGBiOKwC2suNW3R3TJWNV0taOYd9iReyZM4m7zwwjN4fgzWDbkQhbFnyhLFYy2-f854Pf7Q7PGdDCP8XlPSXDDDTltYj12ACjaqvuPhMpSSbFHXZl3lrVCJrnJ6qVW41Vt91tb-c7RAmVNHhY9f889blcSPJyvOhAO7INqpvyMKfq7CUzTUc7JQMk2xlVOkN6KHg3e0AAAA&shmds=v1_AdeF8KjDKCLrvGPgND_vK-DY1qQu4OMxZP9uLc_ZBdRA5BzEAA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=pDU2hLyxJBfqhe6DAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "AI/ML Engineer",
         "CodeCraft Technologies",
         "Bengaluru, Karnataka, India (+1 other)",
         "Position Overview\nWe are looking for an experienced AI/ML Engineer to join our team. The ideal candidate will bring a deep understanding of machine learning, artificial intelligence, and big data technologies, with proven expertise in developing scalable AI/ML solutions. This role shall lead technical efforts, mentor team members, and collaborate with cross-functional teams to design, develop, and deploy cutting edge AI/ML applications.\n\nJob Details\n• Job Category: AI/ML Engineer.\n• Job Type: Full-Time\n• Job Location: Bangalore/Mangalore\n• Experience Required: 4-8 Years\n\nKey Responsibilities\n• Design, develop, and deploy deep learning models for object classification, detection, and segmentation using CNNs and Transfer Learning.\n• Implement image preprocessing and advanced computer vision pipelines.\n• Optimize deep learning models using pruning, quantization, and ONNX for deployment on edge devices.\n• Work with PyTorch, TensorFlow, and ONNX frameworks to develop and convert models.\n• Accelerate model inference using GPU programming with CUDA and cuDNN.\n• Port and test models on embedded and edge hardware platforms. (Orin, Jetson, Hailo)\n• Conduct research and experiments to evaluate and integrate GenAI technologies in computer vision tasks.\n• Explore and implement cloud-based AI workflows, particularly using AWS/Azure AI/ML services.\n• Collaborate with cross-functional teams for data analytics, data processing, and large-scale model training.\n\nDesired Profile:\n• Strong programming experience in Python.\n• Solid background in deep learning, CNNs, and transfer learning and Machine learning basics.\n• Expertise in object detection, classification, segmentation.\n• Proficiency with PyTorch, TensorFlow, and ONNX.\n• Experience with GPU acceleration (CUDA, cuDNN).\n• Hands-on knowledge of model optimization (pruning, quantization).\n• Experience deploying models to edge devices (e.g., Jetson, mobile, Orin, Hailo )\n• Understanding of image processing techniques.\n• Familiarity with data pipelines, data preprocessing, and data analytics.\n• Willingness to explore and contribute to Generative AI and cloud-based AI solutions.\n• Good problem-solving and communication skills.\n\nGood to have\n• Experience with C/C++.\n• Familiarity with AWS Cloud AI/ML tools (e.g., SageMaker, Rekognition).\n• Exposure to GenAI frameworks like OpenAI, Stable Diffusion, etc.\n• Knowledge of real-time deployment systems and streaming analytics.\n\nQualifications\n\nGraduation/Post-graduation in Computers, Engineering, or Statistics from a reputed institute\n\nIf you are passionate to work in a collaborative and challenging environment, apply now!",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=6NG-cOAKE8257UNnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE63uIg2InTRSYtI_dncyzU9k2i8K7kU-i6-rPgNX_GdFYtDY-43OLELTJRgDRfpQAmT9SAMZxEXab73OQ-6M0Y1lk4z5mBLKx8jTJ1M5iWd_mvVY6IhYqZ2W22mcmC3rGrpqU74zPAg61miuEAKgeFI7DCOaVzBFRNjxjeuoOE-4A_ILc0zngAAAA&shmds=v1_AdeF8KirjKT_jo2XCCNYa5AY1aXL_c1eKOzfI2Xad0tGwJ5Olg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=6NG-cOAKE8257UNnAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "Senior AI/ML Engineer- Python, Genai, NLP, ML,LLM",
         "UnitedHealth Group",
         "Hyderabad, Telangana, India",
         "Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.\n\nOptum AI is UnitedHealth Group’s enterprise AI team. We are AI/ML scientists and engineers with deep expertise in AI/ML engineering for health care. We develop AI/ML solutions for the highest impact opportunities across UnitedHealth Group businesses including UnitedHealthcare, Optum Financial, Optum Health, Optum Insight, and Optum Rx. In addition to transforming the health care journey through responsible AI/ML innovation, our charter also includes developing and supporting an enterprise AI/ML development platform.\n\nOptum AI team members:\n• Have impact at scale: We have the data and resources to make an impact at scale. When our solutions are deployed, they have the potential to make health care system work better for everyone\n• Do ground-breaking work: Many of our current projects involve cutting edge ML, NLP and LLM techniques. Generative AI methods for working with structured and unstructured health care data are continuously being developed and improved. We are working in one of the most important frontiers of AI/ML research and development\n• Partner with world-class experts on innovative solutions: Our team members are developing novel AI/ML solutions to business challenges. In some cases, this includes the opportunity to file patents and publish papers about the methods we develop. We also collaborate with AI/ML researchers at some of the world’s top universities\n\nPrimary Responsibilities:\n• Develop and deploy innovative AI solutions to various business problems. You will have the opportunity to learn and apply some of the most cutting-edge AI technologies\n• Research and evaluate new data sources, AI technology trends and methodologies to enhance Organization level data science capabilities and solutions\n• Perform data analysis, pre-processing and feature engineering required to ensure quality and reliability of the AI models and optimize them for performance and robustness\n• Develop and deploy big data pipelines and frameworks for data ingestion, processing and analysis\n• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\n• Bachelor's degree or higher in computer science or engineering with a focus on language processing.\n• 7+ years of experience in AI, ML and/or NLP R&D (with demonstrable impact preferred). This requirement maybe relaxed for candidates with a PhD\n• 6+ years of overall experience\n• Experience with both traditional machine learning algorithms and modern deep learning and generative AI models\n• Proven good knowledge of and hands-on skills with repository management and GPU use, and the ability to rapidly set up pipelines for testing new ideas\n• Proficiency in Python, R, SQL and other programming languages for data analysis and modeling\n• Proficiency with cloud development environments such as Azure, AWS, and GCP\n• Proven excellent analytical and problem-solving skills, including the ability to disaggregate issues, identify root causes and recommend solutions\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=cF7V2ieHA0fldXQ3AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43S2xUcNHJQdpKKoI6y7U9kki8K0mE-mt-nfiGV3xnRX0l9hLh0OjWwJGtZ6K4hMsnO2EFFTF6BWdzUdAaZUwLSzhJB4kw9g6EoRKxgeZ7l_OYdlqnFEqbMmbfl728tDB1MumndOnfIzmMNAbM9NhsV1M5sl2s7-wzDTVhyA6qKO8RPEP9GShih4OCGwVki4wKGh48_gAuCopqvQAAAA&shmds=v1_AdeF8Ki94rxPTvuG1T6wimaYNTmJ9St7I61OGcvGJ1jJgZbgDA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=cF7V2ieHA0fldXQ3AAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "Associate AI/ML Engineer",
         "Maersk",
         "India",
         "Assoicate AIML Engineer– Global Data Analytics, Technology (Maersk)\n\nThis position will be based in India – Bangalore/Pune\n\nA.P. Moller - Maersk\n\nA.P. Moller – Maersk is the global leader in container shipping services. The business operates in 130 countries and employs 80,000 staff. An integrated container logistics company, Maersk aims to connect and simplify its customers’ supply chains.\n\nToday, we have more than 180 nationalities represented in our workforce across 131 Countries and this mean, we have elevated level of responsibility to continue to build inclusive workforce that is truly representative of our customers and their customers and our vendor partners too.\n\nWe are responsible for moving 20 % of global trade & is on a mission to become the Global Integrator of Container Logistics. To achieve this, we are transforming into an industrial digital giant by combining our assets across air, land, ocean, and ports with our growing portfolio of digital assets to connect and simplify our customer’s supply chain through global end-to-end solutions, all the while rethinking the way we engage with customers and partners.\n\nThe Brief\n\nIn this role as an Associate AIML Engineer on the Global Data and Analytics (GDA) team, you will support the development of strategic, visibility-driven recommendation systems that serve both internal stakeholders and external customers. This initiative aims to deliver actionable insights that enhance supply chain execution, support strategic decision-making, and enable innovative service offerings.\n\nData AI/ML (Artificial Intelligence and Machine Learning) Engineering involves the use of algorithms and statistical models to enable systems to analyse data, learn patterns, and make data-driven predictions or decisions without explicit human programming. AI/ML applications leverage vast amounts of data to identify insights, automate processes, and solve complex problems across a wide range of fields, including healthcare, finance, e-commerce, and more. AI/ML processes transform raw data into actionable intelligence, enabling automation, predictive analytics, and intelligent solutions. Data AI/ML combines advanced statistical modelling, computational power, and data engineering to build intelligent systems that can learn, adapt, and automate decisions.\n\nWhat I'll be doing – your accountabilities?\n• Build and maintain machine learning models for various applications, such as natural language processing, computer vision, and recommendation systems\n• Perform exploratory data analysis (EDA) to identify patterns and trends in data\n• Clean, preprocess, perform hyperparameter tuning and analyze large datasets to prepare them for AI/ML model training\n• Build, test, and optimize machine learning models and experiment with algorithms and frameworks to improve model performance\n• Use programming languages, machine learning frameworks and libraries, algorithms, data structures, statistics and databases to optimize and fine-tune machine learning models to ensure scalability and efficiency\n• Learn to define user requirements and align solutions with business needs\n• Work on AI/ML engineering projects, perform feature engineering and collaborate with teams to understand business problems\n• Learn best practices in data / AI/ML engineering and performance optimization\n• Contribute to research papers and technical documentation\n• Contribute to project documentation and maintain data quality standards\n\nFoundational Skills\n• Understands Programming skills beyond the fundamentals and can demonstrate this skill in most situations without guidance.\n• Understands the below skills beyond the fundamentals and can demonstrate in most situations without guidance\n• AI & Machine Learning\n• Data Analysis\n• Machine Learning Pipelines\n• Model Deployment\n\nSpecialized Skills\n• To be able to understand beyond the fundamentals and can demonstrate in most situations without guidance for the following skills:\n• Deep Learning\n• Statistical Analysis\n• Data Engineering\n• Big Data Technologies\n• Natural Language Processing (NPL)\n• Data Architecture\n• Data Processing Frameworks\n• Proficiency in Python programming.\n• Proficiency in Python-based statistical analysis and data visualization tool\n• While having limited understanding of Technical Documentation but are focused on growing this skill\n\nQualifications & Requirements\n• BSc/MSc/PhD in computer science, data science or related discipline with 1+ years of industry experience building cloud-based ML solutions for production at scale, including solution architecture and solution design experience\n• Good problem solving skills, for both technical and non-technical domains\n• Good broad understanding of ML and statistics covering standard ML for regression and classification, forecasting and time-series modeling, deep learning\n• 3+ years of hands-on experience building ML solutions in Python, incl knowledge of common python data science libraries (e.g. scikit-learn, PyTorch, etc)\n• Hands-on experience building end-to-end data products based on AI/ML technologies\n• Some experience with scenario simulations.\n• Experience with collaborative development workflow: version control (we use github), code reviews, DevOps (incl automated testing), CI/CD\n• Team player, eager to collaborate and good collaborator\n\nPreferred Experiences\n\nIn addition to basic qualifications, would be great if you have…\n• Hands-on experience with common OR solvers such as Gurobi\n• Experience with a common dashboarding technology (we use PowerBI) or web-based frontend such as Dash, Streamlit, etc.\n• Experience working in cross-functional product engineering teams following agile development methodologies (scrum/Kanban/…)\n• Experience with Spark and distributed computing\n• Strong hands-on experience with MLOps solutions, including open-source solutions.\n• Experience with cloud-based orchestration technologies, e.g. Airflow, KubeFlow, etc\n• Experience with containerization (Kubernetes & Docker)\n\nAs a performance-oriented company, we strive to always recruit the best person for the job – regardless of gender, age, nationality, sexual orientation or religious beliefs. We are proud of our diversity and see it as a genuine source of strength for building high-performing teams.\n\nMaersk is committed to a diverse and inclusive workplace, and we embrace different styles of thinking. Maersk is an equal opportunities employer and welcomes applicants without regard to race, colour, gender, sex, age, religion, creed, national origin, ancestry, citizenship, marital status, sexual orientation, physical or mental disability, medical condition, pregnancy or parental leave, veteran status, gender identity, genetic information, or any other characteristic protected by applicable law. We will consider qualified applicants with criminal histories in a manner consistent with all legal requirements.\n\nWe are happy to support your need for any adjustments during the application and hiring process. If you need special assistance or an accommodation to use our website, apply for a position, or to perform a job, please contact us by emailing accommodationrequests@maersk.com.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=O_zgrdd16QeeSZLFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMoQ7CMBAAUD-DR50mYUdIMKAmCBlh37Bcy6UtdHdNr2KOXwfMk6_7dDiYqU_UGIYRpwdcJSRhrrCHuzowpuojqMBNNWTeXmJrxc6IZrkP1qgl33tdUIWdrvhSZ39mi1S55N88H0-HtS8SdpuJuNobksAoz0RfVctudYEAAAA&shmds=v1_AdeF8Kh84bXdOYpezjdJ6xmvg0QMtdmv7CctA2z-vQTJqAke3Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=O_zgrdd16QeeSZLFAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "Lead AI Engineer",
         "Keywords Studios",
         "Pune, Maharashtra, India",
         "About the team:You will be a part of the AI team, which is responsible for developing AI features at Helpshift.The AI team has built product features like User Intent Detection, AI-Powered Answers,Agent CoPilot features, etc by building our own ML models and leveraging generative AI.The team consists of Backend and Frontend Developers, Full-stack Developers, MLEngineers and Data Scientists.About the role:We are looking for a seasoned engineer with a passion for Artificial Intelligence and a knackfor leadership. In this role, you'll be instrumental in shaping our AI products, leading a groupof engineers within the team, and driving the successful delivery of innovative solutions.You'll blend deep technical expertise in AI with strong leadership skills, guiding a team offull-stack, backend, and frontend developers. You'll be responsible for the end-to-enddelivery of AI-powered features and products, from conceptualization and design todeployment and optimization.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKOwrCQBAAUGxzBKvpBIlZEWy0shCJHxA8QJhkh92VOBN2Jqj38MBq86pXfCbF7EzoYVfDnkNiogwLOEoLSpi7CMJwEAk9TbfRbNCNc6p9FdTQUld18nDC1MrL3aXVP41GzDT0aNSs1stXNXCYlyd6PyV7hZuNPolCYriOTCVc8PdRo2UsoWaf8AtFRB7ClwAAAA&shmds=v1_AdeF8Kj6B1cePoeK-rZ__bHkWYIkydErgHmkalRQ7Y1AXJ3GZw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=EArXB1jbcPlcRgXLAAAAAA%3D%3D",
         "AI/ML Engineer"
        ],
        [
         "Machine Learning Engineer",
         "ExxonMobil",
         "Bengaluru, Karnataka, India",
         "About us\n\nAt ExxonMobil, our vision is to lead in energy innovations that advance modern living and a net-zero future. As one of the world’s largest publicly traded energy and chemical companies, we are powered by a unique and diverse workforce fueled by the pride in what we do and what we stand for.\n\nThe success of our Upstream, Product Solutions and Low Carbon Solutions businesses is the result of the talent, curiosity and drive of our people. They bring solutions every day to optimize our strategy in energy, chemicals, lubricants and lower-emissions technologies.\n\nWe invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society’s evolving needs. Learn more about our What and our Why and how we can work together.\n\nWhat role you will play in our team\n\nWe are looking to hire candidates to work on challenging technology and engineering problems that span oil and gas exploration & production, chemicals/fuels/lubricants products and low carbon solutions. A successful candidate would understand a business problem (both commercially and technically), translate it into a computational, data science or machine learning problem and apply engineering, numerical, data science and programming skills to tackle it. The Machine Learning Engineer will work as part of a team to design, develop, deploy and sustain data science solutions that are scalable, reproducible and with commercial-grade quality.\n\nWhat you will do\n• Applies software development practices, DevOps skills and Machine Learning (ML) techniques to orchestrate an end-to-end machine learning workflow that effectively brings ML models to production.\n• Participates in scoping of deployment of new data science solutions and implements the appropriate solution design.\n• Sustain data science solutions by enabling continuous ML model and/or service performance monitoring, training, and re-training of models, including the implementation of proactive alerting methods.\n• Works effectively with computational scientists, data scientists, engineers, software developers, and domain experts across the globe to develop and apply computational and data science solutions in support of our business.\n\nAbout You\n\nSkills and Qualifications\n• Bachelor’s degree from a recognized university in Computer Science, IT, Applied Mathematics, Engineering or related disciplines with minimum 7.0 CGPA or equivalent.\n• Minimum 3 years of experience in Data Science and Machine Learning or related computational domain.\n• Competent to expert level programming experience in C/C++/Python.\n• Strong foundation in application design.\n• Experience with refactoring legacy code and leveraging third-party libraries/APIs during software development.\n• Experience with Source code version control (Git), Azure Cloud platform and containers, Databricks and MLflow.\n• Continuous Integration and Continuous Deployment.\n• Familiarity with statistical analysis, regression and classification.\n\nPreferred Qualifications / Experience\n• Experience with time series analysis, computer vision, natural language processing.\n• Knowledge or hands on experience on Matlab & SQL.\n• Strong written and verbal communication skills.\n• Prior knowledge of commercial software development and/or experience in commercial software teams.\n• Familiarity with Oil and Gas Industry.\n\nYour benefits\n\nAn ExxonMobil career is one designed to last. Our commitment to you runs deep our employees grow personally and professionally, with benefits built on our core categories of health, security, finance and life. We offer you:\n• Competitive compensation\n• Medical plans, maternity leave and benefits, life, accidental death and dismemberment benefits\n• Retirement benefits\n• Global networking & cross-functional opportunities\n• Annual vacations & holidays\n• Day care assistance program\n• Training and development program\n• Tuition assistance program\n• Workplace flexibility policy\n• Relocation program\n• Transportation facility\n\nPlease note benefits may change from time to time without notice, subject to applicable laws. The benefits programs are based on the Company’s eligibility guidelines.\n\nStay connected with us\n• Learn more about ExxonMobil in India, visit ExxonMobil India and Energy Factor India.\n• Follow us on LinkedIn and ExxonMobil (@exxonmobil) • Instagram photos and videos\n• Like us on Facebook\n• Subscribe our channel at YouTube\n\nEEO Statement\n\nExxonMobil is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin or disability status.\n\nBusiness solicitation and recruiting scams\n\nExxonMobil does not use recruiting or placement agencies that charge candidates an advance fee of any kind (e.g., placement fees, immigration processing fees, etc.). Follow the LINK to understand more about recruitment scams in the name of ExxonMobil.\n\nNothing herein is intended to override the corporate separateness of local entities. Working relationships discussed herein do not necessarily represent a reporting connection, but may reflect a functional guidance, stewardship, or service relationship.\n\nExxon Mobil Corporation has numerous affiliates, many with names that include ExxonMobil, Exxon, Esso and Mobil. For convenience and simplicity, those terms and terms like corporation, company, our, we and its are sometimes used as abbreviated references to specific affiliates or affiliate groups. Abbreviated references describing global or regional operational organizations and global or regional business lines are also sometimes used for convenience and simplicity. Similarly, ExxonMobil has business relationships with thousands of customers, suppliers, governments, and others. For convenience and simplicity, words like venture, joint venture, partnership, co-venturer, and partner are used to indicate business relationships involving common activities and interests, and those words may not indicate precise legal relationships.",
         "https://www.google.com/search?ibp=htl;jobs&q=AI/ML+Engineer&htidocid=Gf14xxeb1PMFW9slAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNSwrCMBBAcdsjuJqtUpMiuNGdUMRPz1AmcUiicaYkKeQsnta6efDe5jXfVdMNaH1gggdh4sAOenaLU4Id3MRAXrr1IAwXERdpffKlTPmodc5RuVywBKusfLQwGan6JSb_MWaPiaaIhcb9oatqYrfd9LUKD2JChMBwJnYY5zS3cF_2WPCNLVz5GfAHD4WFHp0AAAA&shmds=v1_AdeF8KiVtfTQyhVeWOhG-JmTM6JDpzh-E2JeatkyNt7NWSzqDQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=AI/ML+Engineer&htidocid=Gf14xxeb1PMFW9slAAAAAA%3D%3D",
         "AI/ML Engineer"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "share_link",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "search_role",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018a8e7c-0bbd-4cd8-9650-15cd7a6829de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##SILVER LAYER ---> Cleaning and Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dff4d43-e944-44b5-9131-a6b1b5a721fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Reading from Bronze Layer -------> Silver Layer\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"JobETL_Silver\").getOrCreate()\n",
    "\n",
    "#reading Bronze Layer\n",
    "print(\"✅Reading from Bronze Layer -------> Silver Layer\")\n",
    "df=spark.read.format(\"delta\").table(\"main.bronze.jobs_raw\")\n",
    "silver_table_path = \"main.silver.jobs_cleaned\"\n",
    "#cleaning and Structuring\n",
    "silver_df=df.selectExpr(\n",
    "    \"title\",\n",
    "    \"company_name\",\n",
    "    \"location\",\n",
    "    \"description\",\n",
    "    \"share_link as job_link\",  \n",
    "    \"search_role\"\n",
    ").dropna(subset=[\"title\", \"company_name\", \"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dffaf134-b3d9-4905-b05b-1eb3b13e2562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Written to Silver Layer!\n"
     ]
    }
   ],
   "source": [
    "silver_df=silver_df.withColumn(\"company_name\",trim(upper(col(\"company_name\"))))\n",
    "#silver_df=silver_df.dropna(subset=[\"posted_at\"])\n",
    "# Create temporary view for merge operation\n",
    "#silver_updates.createOrReplaceTempView(\"updates\")\n",
    "# Writing to Silver Layer - Corrected approach\n",
    "silver_df.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"append\").saveAsTable(\"main.silver.jobs_cleaned\")  # Changed from .save() to .saveAsTable()\n",
    "# Changed from append to overwrite for idempotency\n",
    "print(\"✅ Data Written to Silver Layer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a311bde1-eb7c-434b-a9ee-90d96a3ccdf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Golden LAYER ---> Generate KPI Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86744ff5-b80f-4b54-86fd-3e7c2a111128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Reading from Silver Layer -------> Golden Layer\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>company_name</th><th>count</th></tr></thead><tbody><tr><td>IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED</td><td>6</td></tr><tr><td>BNP PARIBAS INDIA SOLUTIONS</td><td>6</td></tr><tr><td>VISA</td><td>6</td></tr><tr><td>QUALCOMM</td><td>6</td></tr><tr><td>EPAM SYSTEMS</td><td>6</td></tr><tr><td>KEYWORDS STUDIOS</td><td>6</td></tr><tr><td>THE IT FIRM</td><td>3</td></tr><tr><td>AKASHX</td><td>3</td></tr><tr><td>AERA TECHNOLOGY</td><td>3</td></tr><tr><td>ECOSMART ENERGY SYSTEM LLC</td><td>3</td></tr><tr><td>INFOSYS</td><td>3</td></tr><tr><td>ARMPL</td><td>3</td></tr><tr><td>EXPERIAN</td><td>3</td></tr><tr><td>BRAINIUM INFORMATION TECHNOLOGIES PVT. LTD.</td><td>3</td></tr><tr><td>STAFFINGINE LLC</td><td>3</td></tr><tr><td>BNP PARIBAS</td><td>3</td></tr><tr><td>NAGARRO</td><td>3</td></tr><tr><td>MATHWORKS</td><td>3</td></tr><tr><td>AVAYA</td><td>3</td></tr><tr><td>HITACHI CAREERS</td><td>3</td></tr><tr><td>SYNECHRON TECHNOLOGIES PVT. LTD._INDIA COMPANY</td><td>3</td></tr><tr><td>ALBIREO TECH SYSTEM</td><td>3</td></tr><tr><td>AMAZON</td><td>3</td></tr><tr><td>ICIMS TALENT ACQUISITION</td><td>3</td></tr><tr><td>EXXONMOBIL</td><td>3</td></tr><tr><td>MAERSK</td><td>3</td></tr><tr><td>BARCLAYS</td><td>3</td></tr><tr><td>A CLIENT OF ANALYTICS VIDHYA</td><td>3</td></tr><tr><td>NESTOR TECHNOLOGIES</td><td>3</td></tr><tr><td>TANISHA SYSTEMS  INC</td><td>3</td></tr><tr><td>MCKINSEY & COMPANY</td><td>3</td></tr><tr><td>MINFY</td><td>3</td></tr><tr><td>ASTELLAS PHARMA</td><td>3</td></tr><tr><td>UNITEDHEALTH GROUP</td><td>3</td></tr><tr><td>FLUOR</td><td>3</td></tr><tr><td>IITJOBS INC</td><td>3</td></tr><tr><td>GIANTMIND SOLUTIONS</td><td>3</td></tr><tr><td>ALL EUROPEAN CAREERS</td><td>3</td></tr><tr><td>ASSA ABLOY</td><td>3</td></tr><tr><td>TIETOEVRY</td><td>3</td></tr><tr><td>HPE</td><td>3</td></tr><tr><td>D. E. SHAW INDIA</td><td>3</td></tr><tr><td>MOTIVE</td><td>3</td></tr><tr><td>ZOOM</td><td>3</td></tr><tr><td>DUPONT</td><td>3</td></tr><tr><td>FEDERAL EXPRESS CORPORATION AMEA</td><td>3</td></tr><tr><td>INSIGHT GLOBAL</td><td>3</td></tr><tr><td>ZENSAR TECHNOLOGIES</td><td>3</td></tr><tr><td>PERIMATTIC</td><td>3</td></tr><tr><td>E902 DWS INDIA PRIVATE LIMITED, MAHARASHTRA BRANCH</td><td>3</td></tr><tr><td>CODECRAFT TECHNOLOGIES</td><td>3</td></tr><tr><td>5100 KYNDRYL SOLUTIONS PRIVATE LIMITED</td><td>3</td></tr><tr><td>THE VALUE MAXIMIZER</td><td>3</td></tr><tr><td>MASTERCARD</td><td>3</td></tr><tr><td>MICROSTRATEGY</td><td>3</td></tr><tr><td>ADOBE</td><td>3</td></tr><tr><td>HIREGINIE</td><td>3</td></tr><tr><td>IND201 REFINITIV INDIA SHARED SERVICES PRIVATE LIMITED</td><td>3</td></tr><tr><td>EXAMROOM.AI</td><td>3</td></tr><tr><td>SAPLING INFOSYSTEMS</td><td>3</td></tr><tr><td>MSCI</td><td>3</td></tr><tr><td>AT&T</td><td>3</td></tr><tr><td>UBER</td><td>3</td></tr><tr><td>BRILLIANTECH SOFTWARE</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED",
         6
        ],
        [
         "BNP PARIBAS INDIA SOLUTIONS",
         6
        ],
        [
         "VISA",
         6
        ],
        [
         "QUALCOMM",
         6
        ],
        [
         "EPAM SYSTEMS",
         6
        ],
        [
         "KEYWORDS STUDIOS",
         6
        ],
        [
         "THE IT FIRM",
         3
        ],
        [
         "AKASHX",
         3
        ],
        [
         "AERA TECHNOLOGY",
         3
        ],
        [
         "ECOSMART ENERGY SYSTEM LLC",
         3
        ],
        [
         "INFOSYS",
         3
        ],
        [
         "ARMPL",
         3
        ],
        [
         "EXPERIAN",
         3
        ],
        [
         "BRAINIUM INFORMATION TECHNOLOGIES PVT. LTD.",
         3
        ],
        [
         "STAFFINGINE LLC",
         3
        ],
        [
         "BNP PARIBAS",
         3
        ],
        [
         "NAGARRO",
         3
        ],
        [
         "MATHWORKS",
         3
        ],
        [
         "AVAYA",
         3
        ],
        [
         "HITACHI CAREERS",
         3
        ],
        [
         "SYNECHRON TECHNOLOGIES PVT. LTD._INDIA COMPANY",
         3
        ],
        [
         "ALBIREO TECH SYSTEM",
         3
        ],
        [
         "AMAZON",
         3
        ],
        [
         "ICIMS TALENT ACQUISITION",
         3
        ],
        [
         "EXXONMOBIL",
         3
        ],
        [
         "MAERSK",
         3
        ],
        [
         "BARCLAYS",
         3
        ],
        [
         "A CLIENT OF ANALYTICS VIDHYA",
         3
        ],
        [
         "NESTOR TECHNOLOGIES",
         3
        ],
        [
         "TANISHA SYSTEMS  INC",
         3
        ],
        [
         "MCKINSEY & COMPANY",
         3
        ],
        [
         "MINFY",
         3
        ],
        [
         "ASTELLAS PHARMA",
         3
        ],
        [
         "UNITEDHEALTH GROUP",
         3
        ],
        [
         "FLUOR",
         3
        ],
        [
         "IITJOBS INC",
         3
        ],
        [
         "GIANTMIND SOLUTIONS",
         3
        ],
        [
         "ALL EUROPEAN CAREERS",
         3
        ],
        [
         "ASSA ABLOY",
         3
        ],
        [
         "TIETOEVRY",
         3
        ],
        [
         "HPE",
         3
        ],
        [
         "D. E. SHAW INDIA",
         3
        ],
        [
         "MOTIVE",
         3
        ],
        [
         "ZOOM",
         3
        ],
        [
         "DUPONT",
         3
        ],
        [
         "FEDERAL EXPRESS CORPORATION AMEA",
         3
        ],
        [
         "INSIGHT GLOBAL",
         3
        ],
        [
         "ZENSAR TECHNOLOGIES",
         3
        ],
        [
         "PERIMATTIC",
         3
        ],
        [
         "E902 DWS INDIA PRIVATE LIMITED, MAHARASHTRA BRANCH",
         3
        ],
        [
         "CODECRAFT TECHNOLOGIES",
         3
        ],
        [
         "5100 KYNDRYL SOLUTIONS PRIVATE LIMITED",
         3
        ],
        [
         "THE VALUE MAXIMIZER",
         3
        ],
        [
         "MASTERCARD",
         3
        ],
        [
         "MICROSTRATEGY",
         3
        ],
        [
         "ADOBE",
         3
        ],
        [
         "HIREGINIE",
         3
        ],
        [
         "IND201 REFINITIV INDIA SHARED SERVICES PRIVATE LIMITED",
         3
        ],
        [
         "EXAMROOM.AI",
         3
        ],
        [
         "SAPLING INFOSYSTEMS",
         3
        ],
        [
         "MSCI",
         3
        ],
        [
         "AT&T",
         3
        ],
        [
         "UBER",
         3
        ],
        [
         "BRILLIANTECH SOFTWARE",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>company_name</th><th>count</th></tr></thead><tbody><tr><td>IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED</td><td>6</td></tr><tr><td>BNP PARIBAS INDIA SOLUTIONS</td><td>6</td></tr><tr><td>VISA</td><td>6</td></tr><tr><td>QUALCOMM</td><td>6</td></tr><tr><td>EPAM SYSTEMS</td><td>6</td></tr><tr><td>KEYWORDS STUDIOS</td><td>6</td></tr><tr><td>THE IT FIRM</td><td>3</td></tr><tr><td>AKASHX</td><td>3</td></tr><tr><td>AERA TECHNOLOGY</td><td>3</td></tr><tr><td>ECOSMART ENERGY SYSTEM LLC</td><td>3</td></tr><tr><td>INFOSYS</td><td>3</td></tr><tr><td>ARMPL</td><td>3</td></tr><tr><td>EXPERIAN</td><td>3</td></tr><tr><td>BRAINIUM INFORMATION TECHNOLOGIES PVT. LTD.</td><td>3</td></tr><tr><td>STAFFINGINE LLC</td><td>3</td></tr><tr><td>BNP PARIBAS</td><td>3</td></tr><tr><td>NAGARRO</td><td>3</td></tr><tr><td>MATHWORKS</td><td>3</td></tr><tr><td>AVAYA</td><td>3</td></tr><tr><td>HITACHI CAREERS</td><td>3</td></tr><tr><td>SYNECHRON TECHNOLOGIES PVT. LTD._INDIA COMPANY</td><td>3</td></tr><tr><td>ALBIREO TECH SYSTEM</td><td>3</td></tr><tr><td>AMAZON</td><td>3</td></tr><tr><td>ICIMS TALENT ACQUISITION</td><td>3</td></tr><tr><td>EXXONMOBIL</td><td>3</td></tr><tr><td>MAERSK</td><td>3</td></tr><tr><td>BARCLAYS</td><td>3</td></tr><tr><td>A CLIENT OF ANALYTICS VIDHYA</td><td>3</td></tr><tr><td>NESTOR TECHNOLOGIES</td><td>3</td></tr><tr><td>TANISHA SYSTEMS  INC</td><td>3</td></tr><tr><td>MCKINSEY & COMPANY</td><td>3</td></tr><tr><td>MINFY</td><td>3</td></tr><tr><td>ASTELLAS PHARMA</td><td>3</td></tr><tr><td>UNITEDHEALTH GROUP</td><td>3</td></tr><tr><td>FLUOR</td><td>3</td></tr><tr><td>IITJOBS INC</td><td>3</td></tr><tr><td>GIANTMIND SOLUTIONS</td><td>3</td></tr><tr><td>ALL EUROPEAN CAREERS</td><td>3</td></tr><tr><td>ASSA ABLOY</td><td>3</td></tr><tr><td>TIETOEVRY</td><td>3</td></tr><tr><td>HPE</td><td>3</td></tr><tr><td>D. E. SHAW INDIA</td><td>3</td></tr><tr><td>MOTIVE</td><td>3</td></tr><tr><td>ZOOM</td><td>3</td></tr><tr><td>DUPONT</td><td>3</td></tr><tr><td>FEDERAL EXPRESS CORPORATION AMEA</td><td>3</td></tr><tr><td>INSIGHT GLOBAL</td><td>3</td></tr><tr><td>ZENSAR TECHNOLOGIES</td><td>3</td></tr><tr><td>PERIMATTIC</td><td>3</td></tr><tr><td>E902 DWS INDIA PRIVATE LIMITED, MAHARASHTRA BRANCH</td><td>3</td></tr><tr><td>CODECRAFT TECHNOLOGIES</td><td>3</td></tr><tr><td>5100 KYNDRYL SOLUTIONS PRIVATE LIMITED</td><td>3</td></tr><tr><td>THE VALUE MAXIMIZER</td><td>3</td></tr><tr><td>MASTERCARD</td><td>3</td></tr><tr><td>MICROSTRATEGY</td><td>3</td></tr><tr><td>ADOBE</td><td>3</td></tr><tr><td>HIREGINIE</td><td>3</td></tr><tr><td>IND201 REFINITIV INDIA SHARED SERVICES PRIVATE LIMITED</td><td>3</td></tr><tr><td>EXAMROOM.AI</td><td>3</td></tr><tr><td>SAPLING INFOSYSTEMS</td><td>3</td></tr><tr><td>MSCI</td><td>3</td></tr><tr><td>AT&T</td><td>3</td></tr><tr><td>UBER</td><td>3</td></tr><tr><td>BRILLIANTECH SOFTWARE</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED",
         6
        ],
        [
         "BNP PARIBAS INDIA SOLUTIONS",
         6
        ],
        [
         "VISA",
         6
        ],
        [
         "QUALCOMM",
         6
        ],
        [
         "EPAM SYSTEMS",
         6
        ],
        [
         "KEYWORDS STUDIOS",
         6
        ],
        [
         "THE IT FIRM",
         3
        ],
        [
         "AKASHX",
         3
        ],
        [
         "AERA TECHNOLOGY",
         3
        ],
        [
         "ECOSMART ENERGY SYSTEM LLC",
         3
        ],
        [
         "INFOSYS",
         3
        ],
        [
         "ARMPL",
         3
        ],
        [
         "EXPERIAN",
         3
        ],
        [
         "BRAINIUM INFORMATION TECHNOLOGIES PVT. LTD.",
         3
        ],
        [
         "STAFFINGINE LLC",
         3
        ],
        [
         "BNP PARIBAS",
         3
        ],
        [
         "NAGARRO",
         3
        ],
        [
         "MATHWORKS",
         3
        ],
        [
         "AVAYA",
         3
        ],
        [
         "HITACHI CAREERS",
         3
        ],
        [
         "SYNECHRON TECHNOLOGIES PVT. LTD._INDIA COMPANY",
         3
        ],
        [
         "ALBIREO TECH SYSTEM",
         3
        ],
        [
         "AMAZON",
         3
        ],
        [
         "ICIMS TALENT ACQUISITION",
         3
        ],
        [
         "EXXONMOBIL",
         3
        ],
        [
         "MAERSK",
         3
        ],
        [
         "BARCLAYS",
         3
        ],
        [
         "A CLIENT OF ANALYTICS VIDHYA",
         3
        ],
        [
         "NESTOR TECHNOLOGIES",
         3
        ],
        [
         "TANISHA SYSTEMS  INC",
         3
        ],
        [
         "MCKINSEY & COMPANY",
         3
        ],
        [
         "MINFY",
         3
        ],
        [
         "ASTELLAS PHARMA",
         3
        ],
        [
         "UNITEDHEALTH GROUP",
         3
        ],
        [
         "FLUOR",
         3
        ],
        [
         "IITJOBS INC",
         3
        ],
        [
         "GIANTMIND SOLUTIONS",
         3
        ],
        [
         "ALL EUROPEAN CAREERS",
         3
        ],
        [
         "ASSA ABLOY",
         3
        ],
        [
         "TIETOEVRY",
         3
        ],
        [
         "HPE",
         3
        ],
        [
         "D. E. SHAW INDIA",
         3
        ],
        [
         "MOTIVE",
         3
        ],
        [
         "ZOOM",
         3
        ],
        [
         "DUPONT",
         3
        ],
        [
         "FEDERAL EXPRESS CORPORATION AMEA",
         3
        ],
        [
         "INSIGHT GLOBAL",
         3
        ],
        [
         "ZENSAR TECHNOLOGIES",
         3
        ],
        [
         "PERIMATTIC",
         3
        ],
        [
         "E902 DWS INDIA PRIVATE LIMITED, MAHARASHTRA BRANCH",
         3
        ],
        [
         "CODECRAFT TECHNOLOGIES",
         3
        ],
        [
         "5100 KYNDRYL SOLUTIONS PRIVATE LIMITED",
         3
        ],
        [
         "THE VALUE MAXIMIZER",
         3
        ],
        [
         "MASTERCARD",
         3
        ],
        [
         "MICROSTRATEGY",
         3
        ],
        [
         "ADOBE",
         3
        ],
        [
         "HIREGINIE",
         3
        ],
        [
         "IND201 REFINITIV INDIA SHARED SERVICES PRIVATE LIMITED",
         3
        ],
        [
         "EXAMROOM.AI",
         3
        ],
        [
         "SAPLING INFOSYSTEMS",
         3
        ],
        [
         "MSCI",
         3
        ],
        [
         "AT&T",
         3
        ],
        [
         "UBER",
         3
        ],
        [
         "BRILLIANTECH SOFTWARE",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"JobETL_Gold\").getOrCreate()\n",
    "\n",
    "print(\"✅Reading from Silver Layer -------> Golden Layer\")\n",
    "\n",
    "df=spark.read.format(\"delta\").table(\"main.silver.jobs_cleaned\")\n",
    "\n",
    "#writing the golden layer to a table \n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"main.gold.jobs_final\")\n",
    "#KPI 1 - Top Companies \n",
    "top_companies = df.groupBy(\"company_name\").count().orderBy(col(\"count\").desc())\n",
    "display(top_companies)\n",
    "\n",
    "#KPI - Jobs by City \n",
    "df.groupBy('Location').agg(count('*').alias('job_count')).orderBy(col('job_count').desc())\n",
    "display(top_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaecc2e2-4643-4f2d-aa02-295226ba6c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1.Tutorial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}